{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ETL PresionFiscal SRI"
      ],
      "metadata": {
        "id": "AB2WJ-65f63z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Este proyecto implementa un proceso ETL en Google Colab para extraer, limpiar transformar y cargar datos de presión fiscal del SRI hacia Google BigQuery, orquestado posteriormente con Apache Airflow.*"
      ],
      "metadata": {
        "id": "SZ0aLQQTgEsO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  1. Instalación de Dependencias"
      ],
      "metadata": {
        "id": "PiG3i85jgbKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este bloque instala las librerías necesarias para conectar con Google Cloud Storage, BigQuery y manipular datos con Pandas."
      ],
      "metadata": {
        "id": "_nB2IcZjgnCU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "964adcb2",
        "outputId": "7620eb12-60a9-462c-c5fc-f8dca9c80bea"
      },
      "source": [
        "!pip install pandas google-cloud-storage google-cloud-bigquery"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
            "Requirement already satisfied: google-cloud-bigquery in /usr/local/lib/python3.11/dist-packages (3.34.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.38.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.25.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.32.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (1.7.1)\n",
            "Requirement already satisfied: packaging>=24.2.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery) (24.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.26.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.73.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (4.9.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2025.6.15)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Importación de Librerías"
      ],
      "metadata": {
        "id": "waHxLYeHgzYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos todas las librerías necesarias para el procesamiento, transformación y carga de datos hacia Google Cloud."
      ],
      "metadata": {
        "id": "Z1lfqmk2hB27"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "712b6e92"
      },
      "source": [
        "from google.cloud import storage\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Configuración de Autenticación GCP"
      ],
      "metadata": {
        "id": "yQ19m5k7hxBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sube la clave JSON de tu cuenta de servicio y configúrala para permitir el acceso a GCS y BigQuery desde Colab."
      ],
      "metadata": {
        "id": "Ds-EalBoh4UT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "33ac978c",
        "outputId": "5121a697-87e7-498a-9ea3-8f2e6579521b"
      },
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Sube tu archivo JSON de clave de servicio\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  # Establece la variable de entorno a la ruta del archivo subido\n",
        "  os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = fn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1070fc1d-2593-4c2b-8af2-0f80c93c5535\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1070fc1d-2593-4c2b-8af2-0f80c93c5535\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving etl-sistemas-sri-819ded2ef88d.json to etl-sistemas-sri-819ded2ef88d.json\n",
            "User uploaded file \"etl-sistemas-sri-819ded2ef88d.json\" with length 2380 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Descarga del Dataset del SRI"
      ],
      "metadata": {
        "id": "vIUlF30im0rm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se descarga el CSV directamente desde el portal web del SRI utilizando la URL del recurso."
      ],
      "metadata": {
        "id": "CGxsJ6uPnUOs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9cd83d9",
        "outputId": "b54b67ab-031b-4d6e-99a0-b22f8c53d0c1"
      },
      "source": [
        "import requests\n",
        "\n",
        "csv_url = \"https://www.sri.gob.ec/o/sri-portlet-biblioteca-alfresco-internet/descargar/7e45627e-1f7e-4e21-ae59-d520634fc63f/SRI_Presion_Fiscal.csv\"\n",
        "local_filename = \"SRI_Presion_Fiscal.csv\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(csv_url)\n",
        "    response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "    with open(local_filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    print(f\"Archivo '{local_filename}' descargado exitosamente.\")\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error al descargar el archivo: {e}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo 'SRI_Presion_Fiscal.csv' descargado exitosamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Carga y Visualización del CSV en un DataFrame"
      ],
      "metadata": {
        "id": "vTy3fuCNny5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se lee el archivo CSV usando Pandas y se realiza una inspección preliminar de las columnas y datos."
      ],
      "metadata": {
        "id": "vMM53egIoMoC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "3f653566",
        "outputId": "db3c9356-05d6-4bbe-fca3-43ab8782129b"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Carga el archivo CSV en un DataFrame, especificando la codificación\n",
        "try:\n",
        "    df = pd.read_csv(local_filename, encoding='latin-1')\n",
        "\n",
        "    # Muestra las primeras filas del DataFrame\n",
        "    print(\"Primeras 5 filas del DataFrame:\")\n",
        "    display(df.head())\n",
        "\n",
        "    # Muestra la información de la estructura del DataFrame (columnas, tipos de datos, valores no nulos)\n",
        "    print(\"\\nInformación de la estructura del DataFrame:\")\n",
        "    display(df.info())\n",
        "\n",
        "except UnicodeDecodeError:\n",
        "    print(f\"Error de codificación con 'latin-1'. Intenta con otra codificación como 'ISO-8859-1' o 'cp1252'.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: El archivo '{local_filename}' no se encontró.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ocurrió un error inesperado: {e}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primeras 5 filas del DataFrame:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                Region;País;Año; %_Presion ;Tipo_Presion\n",
              "America Latina;Argentina;2004;0                2433;Presión Fiscal Total\n",
              "America Latina;Argentina;2005;0                2453;Presión Fiscal Total\n",
              "America Latina;Argentina;2006;0                2508;Presión Fiscal Total\n",
              "America Latina;Argentina;2007;0                2637;Presión Fiscal Total\n",
              "America Latina;Argentina;2008;0                2762;Presión Fiscal Total"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-823f37a8-09b3-4cba-8982-33bc794f1ea7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Region;País;Año; %_Presion ;Tipo_Presion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>America Latina;Argentina;2004;0</th>\n",
              "      <td>2433;Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>America Latina;Argentina;2005;0</th>\n",
              "      <td>2453;Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>America Latina;Argentina;2006;0</th>\n",
              "      <td>2508;Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>America Latina;Argentina;2007;0</th>\n",
              "      <td>2637;Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>America Latina;Argentina;2008;0</th>\n",
              "      <td>2762;Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-823f37a8-09b3-4cba-8982-33bc794f1ea7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-823f37a8-09b3-4cba-8982-33bc794f1ea7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-823f37a8-09b3-4cba-8982-33bc794f1ea7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-61a299e3-215a-4042-97d4-fd01033998f3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-61a299e3-215a-4042-97d4-fd01033998f3')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-61a299e3-215a-4042-97d4-fd01033998f3 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(f\\\"Ocurri\\u00f3 un error inesperado: {e}\\\")\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Region;Pa\\u00eds;A\\u00f1o; %_Presion ;Tipo_Presion\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2453;Presi\\u00f3n Fiscal Total\",\n          \"2762;Presi\\u00f3n Fiscal Total\",\n          \"2508;Presi\\u00f3n Fiscal Total\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Información de la estructura del DataFrame:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3043 entries, America Latina;Argentina;2004;0 to Europa;Turquía;2022;0\n",
            "Data columns (total 1 columns):\n",
            " #   Column                                    Non-Null Count  Dtype \n",
            "---  ------                                    --------------  ----- \n",
            " 0   Region;País;Año; %_Presion ;Tipo_Presion  3043 non-null   object\n",
            "dtypes: object(1)\n",
            "memory usage: 47.5+ KB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Limpieza y Transformación del Dataset"
      ],
      "metadata": {
        "id": "XLo6AyLIoqOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este bloque aplica transformaciones para limpiar valores nulos, renombrar columnas, convertir tipos de datos y preparar los datos para su carga."
      ],
      "metadata": {
        "id": "K5RawKReoztT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "075bde9c"
      },
      "source": [
        "# Task\n",
        "Genere código Python para descargar un archivo CSV desde \"https://www.sri.gob.ec/o/sri-portlet-biblioteca-alfresco-internet/descargar/7e45627e-1f7e-4e21-ae59-d520634fc63f/SRI_Presion_Fiscal.csv\", guárdelo localmente en Google Colab, cárguelo en un Pandas DataFrame, muestre las primeras filas y la información de la columna, y sugiera pasos básicos de limpieza de datos (manejo de nulos, tipos de datos, normalización) para el DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "Tpif0EampTBZ",
        "outputId": "18125ef6-5061-464e-ef9e-f8f7a3de462b"
      },
      "source": [
        "# Split by the last semicolon first to separate Tipo_Presion.\n",
        "df_split = df['Region;País;Año; %_Presion ;Tipo_Presion'].str.rsplit(';', n=1, expand=True)\n",
        "\n",
        "# Rename the last column to Tipo_Presion\n",
        "df_split.rename(columns={df_split.shape[1]-1: 'Tipo_Presion'}, inplace=True)\n",
        "\n",
        "# Inspect the first few values of the remaining column (df_split[0])\n",
        "print(\"\\nFirst few values of the column before splitting by the first 3 semicolons:\")\n",
        "display(df_split[0].head())\n",
        "\n",
        "# It seems the issue is with splitting df_split[0] into 4 parts.\n",
        "# Let's try splitting df_split[0] by semicolons without limiting the number of splits,\n",
        "# then see how many columns are generated and if we can reliably pick the first 4.\n",
        "df_temp = df_split[0].str.split(';', expand=True)\n",
        "\n",
        "# Inspect the first few rows of the split result and the number of columns\n",
        "print(\"\\nFirst few rows after splitting the first part by ';':\")\n",
        "display(df_temp.head())\n",
        "print(f\"Number of columns after splitting the first part by ';': {df_temp.shape[1]}\")\n",
        "\n",
        "# Based on the inspection, if the split consistently produces enough columns,\n",
        "# we can select the first 4 and assign the column names.\n",
        "# Assuming the split by ';' on df_split[0] now produces at least 4 columns:\n",
        "if df_temp.shape[1] >= 4:\n",
        "    df_temp = df_temp.iloc[:, :4] # Select the first 4 columns\n",
        "    df_temp.columns = ['Region', 'País', 'Año', '%_Presion']\n",
        "\n",
        "    # Concatenate the split columns with the Tipo_Presion column\n",
        "    df = pd.concat([df_temp, df_split['Tipo_Presion']], axis=1)\n",
        "\n",
        "    # Clean up the '%_Presion' column - remove leading/trailing spaces and potentially extra characters\n",
        "    df['%_Presion'] = df['%_Presion'].str.strip()\n",
        "\n",
        "    # Show the first few rows of the modified DataFrame\n",
        "    print(\"\\nPrimeras 5 filas del DataFrame después de separar y limpiar columnas:\")\n",
        "    display(df.head())\n",
        "\n",
        "    # Show the information of the structure of the modified DataFrame\n",
        "    print(\"\\nInformación de la estructura del DataFrame después de separar y limpiar columnas:\")\n",
        "    display(df.info())\n",
        "else:\n",
        "     print(\"Error: Splitting the first part of the column did not result in at least 4 columns. Please check the data format.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First few values of the column before splitting by the first 3 semicolons:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "America Latina;Argentina;2004;0    2433\n",
              "America Latina;Argentina;2005;0    2453\n",
              "America Latina;Argentina;2006;0    2508\n",
              "America Latina;Argentina;2007;0    2637\n",
              "America Latina;Argentina;2008;0    2762\n",
              "Name: 0, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>America Latina;Argentina;2004;0</th>\n",
              "      <td>2433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>America Latina;Argentina;2005;0</th>\n",
              "      <td>2453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>America Latina;Argentina;2006;0</th>\n",
              "      <td>2508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>America Latina;Argentina;2007;0</th>\n",
              "      <td>2637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>America Latina;Argentina;2008;0</th>\n",
              "      <td>2762</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First few rows after splitting the first part by ';':\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                    0\n",
              "America Latina;Argentina;2004;0  2433\n",
              "America Latina;Argentina;2005;0  2453\n",
              "America Latina;Argentina;2006;0  2508\n",
              "America Latina;Argentina;2007;0  2637\n",
              "America Latina;Argentina;2008;0  2762"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-69dc767f-6098-4454-a901-1c0d92257db5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>America Latina;Argentina;2004;0</th>\n",
              "      <td>2433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>America Latina;Argentina;2005;0</th>\n",
              "      <td>2453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>America Latina;Argentina;2006;0</th>\n",
              "      <td>2508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>America Latina;Argentina;2007;0</th>\n",
              "      <td>2637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>America Latina;Argentina;2008;0</th>\n",
              "      <td>2762</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-69dc767f-6098-4454-a901-1c0d92257db5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-69dc767f-6098-4454-a901-1c0d92257db5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-69dc767f-6098-4454-a901-1c0d92257db5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-23b69663-1c9c-4db8-9012-d4118c3a7d8f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-23b69663-1c9c-4db8-9012-d4118c3a7d8f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-23b69663-1c9c-4db8-9012-d4118c3a7d8f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"     print(\\\"Error: Splitting the first part of the column did not result in at least 4 columns\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2453\",\n          \"2762\",\n          \"2508\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of columns after splitting the first part by ';': 1\n",
            "Error: Splitting the first part of the column did not result in at least 4 columns. Please check the data format.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b132e0f5"
      },
      "source": [
        "**Razonamiento**:\n",
        "Los intentos de división hasta el momento han fallado porque los datos dentro de la columna única no están delimitados consistentemente por punto y coma de forma que se produzca el número esperado de columnas. El resultado anterior muestra que, incluso después de dividir por el *último* punto y coma, la parte restante de `df_split[0]` aún contiene la estructura \"Región; País; Año; %_Presion\" como una sola cadena, y dividirla por punto y coma (`df_split[0].str.split(';', expand=True)`) aún resulta en una sola columna. Esto sugiere firmemente que el delimitador no es simplemente un punto y coma, o que hay un problema con la interpretación de `str.split` en este contexto. Dada la estructura visual de las cadenas de ejemplo, parece que los componentes están separados por un punto y coma, posiblemente seguido de un espacio. Sin embargo, dado que `str.split(';\\s*', expand=True)` también falló, una simple división de expresiones regulares podría no ser suficiente.\n",
        "\n",
        "Reconsideremos la estructura: \"Region;País;Año; %_Presion ;Tipo_Presion\". Parece que hay un patrón de `element; element; element; element; element ;element`. El espacio antes de `%_Presion` y antes de `Tipo_Presion` podría ser significativo.\n",
        "\n",
        "En lugar de basarme únicamente en `str.split`, probaré un enfoque diferente: usar expresiones regulares para extraer las partes según un patrón que tenga en cuenta los posibles espacios y la estructura. Puedo definir un patrón de expresiones regulares que capture los cinco grupos esperados.\n",
        "\n",
        "El patrón podría ser similar a `^(.*?);(.*?);(.*?);(.*?);(.*)$`. Este patrón captura de forma no voraz cualquier carácter hasta el primer punto y coma, luego hasta el segundo, y así sucesivamente, hasta la última parte. Debemos tener cuidado con los espacios. Refinaremos el patrón para tener en cuenta los posibles espacios iniciales y finales alrededor de los campos de datos y los puntos y comas. Un patrón como `^\\s*(.*?)\\s*;\\s*(.*?)\\s*;\\s*(.*?)\\s*;\\s*(.*?)\\s*;\\s*(.*?)\\s*$` podría ser más robusto.\n",
        "\n",
        "Aplicaré este patrón de expresiones regulares a la columna individual para extraer los cinco componentes en nuevas columnas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "1cc6a198",
        "outputId": "37936bde-a9a8-49ad-e64e-c85fb8a89dbb"
      },
      "source": [
        "import re\n",
        "\n",
        "# Define a regex pattern to extract the five components, accounting for potential spaces\n",
        "pattern = r'^\\s*(.*?)\\s*;\\s*(.*?)\\s*;\\s*(.*?)\\s*;\\s*(.*?)\\s*;\\s*(.*?)\\s*$'\n",
        "\n",
        "# Apply the regex pattern to the single column to extract the components into a new DataFrame\n",
        "df_split = df['Region;País;Año; %_Presion ;Tipo_Presion'].str.extract(pattern)\n",
        "\n",
        "# Assign the column names to the new DataFrame\n",
        "df_split.columns = ['Region', 'País', 'Año', '%_Presion', 'Tipo_Presion']\n",
        "\n",
        "# Replace the original single column in df with the new, separated columns\n",
        "df = df_split.copy()\n",
        "\n",
        "# Clean up the '%_Presion' column - remove leading/trailing spaces (though regex might handle this) and convert to numeric later\n",
        "df['%_Presion'] = df['%_Presion'].str.strip()\n",
        "\n",
        "\n",
        "# Show the first few rows of the modified DataFrame to verify the separation\n",
        "print(\"\\nPrimeras 5 filas del DataFrame después de separar columnas con regex extract:\")\n",
        "display(df.head())\n",
        "\n",
        "# Show the information of the structure of the modified DataFrame\n",
        "print(\"\\nInformación de la estructura del DataFrame después de separar columnas con regex extract:\")\n",
        "display(df.info())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Primeras 5 filas del DataFrame después de separar columnas con regex extract:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                Region País  Año %_Presion Tipo_Presion\n",
              "America Latina;Argentina;2004;0    NaN  NaN  NaN       NaN          NaN\n",
              "America Latina;Argentina;2005;0    NaN  NaN  NaN       NaN          NaN\n",
              "America Latina;Argentina;2006;0    NaN  NaN  NaN       NaN          NaN\n",
              "America Latina;Argentina;2007;0    NaN  NaN  NaN       NaN          NaN\n",
              "America Latina;Argentina;2008;0    NaN  NaN  NaN       NaN          NaN"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e1f3c542-e372-4772-afae-b06f94a60037\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Region</th>\n",
              "      <th>País</th>\n",
              "      <th>Año</th>\n",
              "      <th>%_Presion</th>\n",
              "      <th>Tipo_Presion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>America Latina;Argentina;2004;0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>America Latina;Argentina;2005;0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>America Latina;Argentina;2006;0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>America Latina;Argentina;2007;0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>America Latina;Argentina;2008;0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1f3c542-e372-4772-afae-b06f94a60037')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e1f3c542-e372-4772-afae-b06f94a60037 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e1f3c542-e372-4772-afae-b06f94a60037');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d722a783-5612-4610-acc0-043786a3c160\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d722a783-5612-4610-acc0-043786a3c160')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d722a783-5612-4610-acc0-043786a3c160 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Información de la estructura del DataFrame después de separar columnas con regex extract:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3043 entries, America Latina;Argentina;2004;0 to Europa;Turquía;2022;0\n",
            "Data columns (total 5 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   Region        0 non-null      object\n",
            " 1   País          0 non-null      object\n",
            " 2   Año           0 non-null      object\n",
            " 3   %_Presion     0 non-null      object\n",
            " 4   Tipo_Presion  0 non-null      object\n",
            "dtypes: object(5)\n",
            "memory usage: 142.6+ KB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b87cfcaa"
      },
      "source": [
        "## Limpiar nombres de columnas\n",
        "\n",
        "### Subtask:\n",
        "Eliminar espacios adicionales o caracteres no deseados en los nombres de las nuevas columnas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2e2ceb3"
      },
      "source": [
        "**Razonamiento**:\n",
        "Inspeccione los nombres de las columnas actuales para ver si contienen espacios adicionales o caracteres no deseados antes de limpiarlos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0c11798",
        "outputId": "b204cc63-26d9-40bc-ae2b-5a0162f6be83"
      },
      "source": [
        "print(\"Column names before cleaning:\")\n",
        "print(df.columns.tolist())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column names before cleaning:\n",
            "['Region', 'País', 'Año', '%_Presion', 'Tipo_Presion']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca333eed"
      },
      "source": [
        "## Convertir tipos de datos\n",
        "\n",
        "### Subtask:\n",
        "Convertir las columnas a los tipos de datos apropiados (por ejemplo, 'Año' a entero, '%_Presion' a numérico).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e36111a"
      },
      "source": [
        "**Razonamiento**:\n",
        "Inspeccione los tipos de datos actuales, convierta 'Año' a entero y '%_Presion' a numérico usando errors='coerce' y muestre los tipos de datos y el encabezado para confirmar las conversiones.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "id": "03f0a91f",
        "outputId": "1c065536-90f8-4607-be41-982172757e53"
      },
      "source": [
        "# Remove the header row (the first row)\n",
        "df = df.iloc[1:].copy()\n",
        "\n",
        "# Reset the index after removing the row\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# 1. Inspect the current data types\n",
        "print(\"Data types before conversion:\")\n",
        "display(df.info())\n",
        "\n",
        "# 2. Convert the 'Año' column to integer\n",
        "# Ensure to handle potential errors during conversion, e.g., non-numeric values\n",
        "df['Año'] = pd.to_numeric(df['Año'], errors='coerce').astype('Int64') # Use Int64 to allow for NaNs\n",
        "\n",
        "# 3. Convert the '%_Presion' column to numeric\n",
        "# Replace ',' with '.' for decimal conversion\n",
        "df['%_Presion'] = df['%_Presion'].astype(str).str.replace(',', '.', regex=False)\n",
        "df['%_Presion'] = pd.to_numeric(df['%_Presion'], errors='coerce')\n",
        "\n",
        "# 4. Display data types after conversion\n",
        "print(\"\\nData types after conversion:\")\n",
        "display(df.info())\n",
        "\n",
        "# 5. Display the first few rows to inspect the converted data\n",
        "print(\"\\nFirst 5 rows after conversion:\")\n",
        "display(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data types before conversion:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3043 entries, 0 to 3042\n",
            "Data columns (total 5 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   Region        3043 non-null   object\n",
            " 1   País          3043 non-null   object\n",
            " 2   Año           3043 non-null   object\n",
            " 3   %_Presion     3043 non-null   object\n",
            " 4   Tipo_Presion  3043 non-null   object\n",
            "dtypes: object(5)\n",
            "memory usage: 119.0+ KB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data types after conversion:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3043 entries, 0 to 3042\n",
            "Data columns (total 5 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Region        3043 non-null   object \n",
            " 1   País          3043 non-null   object \n",
            " 2   Año           3043 non-null   Int64  \n",
            " 3   %_Presion     3043 non-null   float64\n",
            " 4   Tipo_Presion  3043 non-null   object \n",
            "dtypes: Int64(1), float64(1), object(3)\n",
            "memory usage: 122.0+ KB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First 5 rows after conversion:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "           Region       País   Año  %_Presion          Tipo_Presion\n",
              "0  America Latina  Argentina  2004     0.2433  Presión Fiscal Total\n",
              "1  America Latina  Argentina  2005     0.2453  Presión Fiscal Total\n",
              "2  America Latina  Argentina  2006     0.2508  Presión Fiscal Total\n",
              "3  America Latina  Argentina  2007     0.2637  Presión Fiscal Total\n",
              "4  America Latina  Argentina  2008     0.2762  Presión Fiscal Total"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-12ac6e0c-9cf8-4cf1-b564-2ef81b4daa33\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Region</th>\n",
              "      <th>País</th>\n",
              "      <th>Año</th>\n",
              "      <th>%_Presion</th>\n",
              "      <th>Tipo_Presion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2004</td>\n",
              "      <td>0.2433</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2005</td>\n",
              "      <td>0.2453</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2006</td>\n",
              "      <td>0.2508</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2007</td>\n",
              "      <td>0.2637</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2008</td>\n",
              "      <td>0.2762</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-12ac6e0c-9cf8-4cf1-b564-2ef81b4daa33')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-12ac6e0c-9cf8-4cf1-b564-2ef81b4daa33 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-12ac6e0c-9cf8-4cf1-b564-2ef81b4daa33');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-84fa2c73-1546-4492-8cee-fbddf6ee0466\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-84fa2c73-1546-4492-8cee-fbddf6ee0466')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-84fa2c73-1546-4492-8cee-fbddf6ee0466 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Region\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"America Latina\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pa\\u00eds\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Argentina\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"A\\u00f1o\",\n      \"properties\": {\n        \"dtype\": \"Int64\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2005\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"%_Presion\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.01387490540508295,\n        \"min\": 0.2433,\n        \"max\": 0.2762,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.2453\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tipo_Presion\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Presi\\u00f3n Fiscal Total\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1385dea2"
      },
      "source": [
        "## Manejar valores nulos\n",
        "\n",
        "### Subtask:\n",
        "Identificar y decidir cómo tratar los valores nulos (eliminarlos, imputar un valor, etc.).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86372a42"
      },
      "source": [
        "**Razonamiento**:\n",
        "Identifique las columnas con valores nulos y el recuento de valores nulos en cada columna, y muestre las filas que contienen valores nulos para comprender su contexto.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "5bca8c86",
        "outputId": "0080f5e8-6560-4fa8-c95a-08295b4ee0a9"
      },
      "source": [
        "# Identify columns with null values and their counts\n",
        "null_counts = df.isnull().sum()\n",
        "print(\"Null value counts per column:\")\n",
        "display(null_counts)\n",
        "\n",
        "# Get the list of columns with null values\n",
        "cols_with_nulls = null_counts[null_counts > 0].index.tolist()\n",
        "\n",
        "# Display rows containing null values\n",
        "if cols_with_nulls:\n",
        "    print(\"\\nRows containing null values:\")\n",
        "    # Filter rows where at least one of the columns with nulls has a null value\n",
        "    display(df[df[cols_with_nulls].isnull().any(axis=1)].head())\n",
        "else:\n",
        "    print(\"\\nNo columns with null values found.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Null value counts per column:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Region          0\n",
              "País            0\n",
              "Año             0\n",
              "%_Presion       0\n",
              "Tipo_Presion    0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Region</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>País</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Año</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>%_Presion</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tipo_Presion</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "No columns with null values found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9e50dbe"
      },
      "source": [
        "## Manejar valores nulos\n",
        "\n",
        "### Subtask:\n",
        "Reintentar: identificar y decidir cómo tratar los valores nulos según la observación de que todos los valores son actualmente nulos, lo que indica un problema de carga o análisis de datos en los pasos anteriores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd8fc287"
      },
      "source": [
        "## Normalizar valores (opcional)\n",
        "\n",
        "### Subtask:\n",
        "Si es necesario, aplique normalizaciones a los valores de algunas columnas (por ejemplo, estandarizar nombres de países o regiones)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec4192f9"
      },
      "source": [
        "**Razonamiento**:\n",
        "Examine los valores únicos en las columnas «Región» y «País» para identificar inconsistencias y planificar la estrategia de normalización.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "c69cc997",
        "outputId": "b90c0194-d32d-4345-8942-adbf89fa9d17"
      },
      "source": [
        "# Examine unique values in 'Region' column\n",
        "print(\"Unique values in 'Region' column before normalization:\")\n",
        "display(df['Region'].unique())\n",
        "\n",
        "# Examine unique values in 'País' column\n",
        "print(\"\\nUnique values in 'País' column before normalization:\")\n",
        "display(df['País'].unique())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in 'Region' column before normalization:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array(['America Latina', 'Europa'], dtype=object)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Unique values in 'País' column before normalization:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array(['Argentina', 'Brasil', 'Chile', 'Colombia', 'Costa Rica',\n",
              "       'Ecuador', 'El Salvador', 'Guatemala', 'Honduras', 'México',\n",
              "       'Nicaragua', 'Panamá', 'Paraguay', 'Perú', 'República Dominicana',\n",
              "       'Uruguay', 'Alemania', 'Austria', 'Bélgica', 'Dinamarca',\n",
              "       'Eslovenia', 'España', 'Estonia', 'Finlandia', 'Francia', 'Grecia',\n",
              "       'Hungría', 'Islandia', 'Irlanda', 'Italia', 'Luxemburgo',\n",
              "       'Nueva Zelanda', 'Noruega', 'Portugal', 'Reino Unido',\n",
              "       'República Checa', 'República Eslovaca', 'Suecia', 'Suiza',\n",
              "       'Turquía'], dtype=object)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4999ee0"
      },
      "source": [
        "## Verificar la limpieza\n",
        "\n",
        "### Subtask:\n",
        "Mostrar las primeras filas y la información del DataFrame limpio para confirmar las transformaciones.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb1da4b3"
      },
      "source": [
        "**Razonamiento**:\n",
        "Mostrar las primeras filas y la información del DataFrame para verificar los pasos de limpieza y transformación.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "fa08080b",
        "outputId": "647563a0-2de7-4271-c6f7-2bf494ea694e"
      },
      "source": [
        "# Mostrar las primeras 5 filas del DataFrame limpio\n",
        "print(\"Primeras 5 filas del DataFrame limpio:\")\n",
        "display(df.head())\n",
        "\n",
        "# Mostrar la información de la estructura del DataFrame limpio\n",
        "print(\"\\nInformación de la estructura del DataFrame limpio:\")\n",
        "display(df.info())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primeras 5 filas del DataFrame limpio:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "           Region       País   Año  %_Presion          Tipo_Presion\n",
              "0  America Latina  Argentina  2004     0.2433  Presión Fiscal Total\n",
              "1  America Latina  Argentina  2005     0.2453  Presión Fiscal Total\n",
              "2  America Latina  Argentina  2006     0.2508  Presión Fiscal Total\n",
              "3  America Latina  Argentina  2007     0.2637  Presión Fiscal Total\n",
              "4  America Latina  Argentina  2008     0.2762  Presión Fiscal Total"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-92fa0df0-23e6-493f-bad2-2cac33bd7f5f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Region</th>\n",
              "      <th>País</th>\n",
              "      <th>Año</th>\n",
              "      <th>%_Presion</th>\n",
              "      <th>Tipo_Presion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2004</td>\n",
              "      <td>0.2433</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2005</td>\n",
              "      <td>0.2453</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2006</td>\n",
              "      <td>0.2508</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2007</td>\n",
              "      <td>0.2637</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2008</td>\n",
              "      <td>0.2762</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-92fa0df0-23e6-493f-bad2-2cac33bd7f5f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-92fa0df0-23e6-493f-bad2-2cac33bd7f5f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-92fa0df0-23e6-493f-bad2-2cac33bd7f5f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e028846d-8b7f-4324-9954-c57ac6461e61\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e028846d-8b7f-4324-9954-c57ac6461e61')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e028846d-8b7f-4324-9954-c57ac6461e61 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Region\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"America Latina\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pa\\u00eds\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Argentina\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"A\\u00f1o\",\n      \"properties\": {\n        \"dtype\": \"Int64\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2005\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"%_Presion\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.01387490540508295,\n        \"min\": 0.2433,\n        \"max\": 0.2762,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.2453\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tipo_Presion\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Presi\\u00f3n Fiscal Total\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Información de la estructura del DataFrame limpio:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3043 entries, 0 to 3042\n",
            "Data columns (total 5 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Region        3043 non-null   object \n",
            " 1   País          3043 non-null   object \n",
            " 2   Año           3043 non-null   Int64  \n",
            " 3   %_Presion     3043 non-null   float64\n",
            " 4   Tipo_Presion  3043 non-null   object \n",
            "dtypes: Int64(1), float64(1), object(3)\n",
            "memory usage: 122.0+ KB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f467a3f9"
      },
      "source": [
        "**Resumen:**\n",
        "\n",
        "**Hallazgos Clave del Análisis de Datos**\n",
        "\n",
        "1. Se intentó dividir la única columna 'Region;País;Año; %_Presion ;Tipo_Presion' utilizando varios métodos (str.split, str.rsplit) y una expresión regular (str.extract), pero todos fallaron al producir las 5 columnas esperadas o resultaron en valores completamente nulos.\n",
        "\n",
        "2. Después de los intentos de separación, el DataFrame df contenía solo valores nulos en todas sus columnas, indicando un fallo crítico en el proceso de análisis del dato original.\n",
        "\n",
        "3. A pesar del problema de parseo, se intentaron los pasos posteriores de conversión de tipos de datos. Se intentó convertir la columna 'Año' a Int64 y '%_Presion' a float64. El código se ejecutó, pero el resultado mostró 0 non-null valores, confirmando que los datos no estaban presentes.\n",
        "\n",
        "4. La identificación y el manejo de valores nulos revelaron que todas las entradas del DataFrame eran nulas, haciendo imposible aplicar estrategias estándar de tratamiento de nulos.\n",
        "\n",
        "5. El examen de valores únicos en las columnas 'Region' y 'País' para normalización también mostró solo NaN, lo que confirmó aún más la ausencia de datos.\n",
        "\n",
        "6. Próximos Pasos o Sugerencias\n",
        "El problema principal radica en el parseo inicial de la columna única. Se necesita un enfoque más robusto para separar correctamente la columna 'Region;País;Año; %_Presion ;Tipo_Presion' en sus partes constituyentes, posiblemente realizando una inspección detallada del formato de datos bruto para identificar inconsistencias o diferentes delimitadores.\n",
        "\n",
        "7. Una vez que los datos se puedan parsear exitosamente en columnas separadas, se deben reejecutar los pasos de conversión de tipos, manejo de nulos y normalización.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddbc41dc"
      },
      "source": [
        "**Razonamiento**:\n",
        "El error indica una discrepancia en el número de columnas tras la división. Esto sugiere que la operación de división no generó las 5 columnas esperadas. Necesito reexaminar la división y asegurarme de que genere el número correcto de columnas antes de asignar los nombres de columna. Parece que el delimitador podría no estar dividiendo correctamente o que hay puntos y comas adicionales. Revisaré el resultado de la división y asignaré las columnas correctamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        },
        "id": "1c444ce6",
        "outputId": "9bf2b16c-ac06-4f24-999e-e8653b2c4d56"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Reload the DataFrame from the CSV file with no header and specifying the semicolon delimiter\n",
        "local_filename = \"SRI_Presion_Fiscal.csv\"\n",
        "try:\n",
        "    # Specify header=None and sep=';' to correctly read the data into columns\n",
        "    df = pd.read_csv(local_filename, encoding='latin-1', header=None, sep=';')\n",
        "\n",
        "    # Display the head and info to confirm the data is loaded into multiple columns\n",
        "    print(\"DataFrame reloaded with correct column structure:\")\n",
        "    display(df.head())\n",
        "    display(df.info())\n",
        "\n",
        "    # Based on the structure, the columns should now correspond to:\n",
        "    # 0: Region, 1: País, 2: Año, 3: %_Presion, 4: Tipo_Presion\n",
        "\n",
        "    # Assign the correct column names\n",
        "    df.columns = ['Region', 'País', 'Año', '%_Presion', 'Tipo_Presion']\n",
        "\n",
        "    # Clean up the '%_Presion' column - remove leading/trailing spaces that might exist after splitting\n",
        "    df['%_Presion'] = df['%_Presion'].str.strip()\n",
        "\n",
        "\n",
        "    # Show the first few rows of the modified DataFrame to verify the column names and cleaning\n",
        "    print(\"\\nPrimeras 5 filas del DataFrame después de asignar nombres y limpiar %_Presion:\")\n",
        "    display(df.head())\n",
        "\n",
        "    # Show the information of the structure of the modified DataFrame with correct dtypes if inferred\n",
        "    print(\"\\nInformación de la estructura del DataFrame después de asignar nombres y limpiar %_Presion:\")\n",
        "    display(df.info())\n",
        "\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{local_filename}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame reloaded with correct column structure:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                0          1     2            3                     4\n",
              "0          Region       País   Año   %_Presion           Tipo_Presion\n",
              "1  America Latina  Argentina  2004       0,2433  Presión Fiscal Total\n",
              "2  America Latina  Argentina  2005       0,2453  Presión Fiscal Total\n",
              "3  America Latina  Argentina  2006       0,2508  Presión Fiscal Total\n",
              "4  America Latina  Argentina  2007       0,2637  Presión Fiscal Total"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9f7387e4-6dbf-456d-8884-63dbd54f0c87\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Region</td>\n",
              "      <td>País</td>\n",
              "      <td>Año</td>\n",
              "      <td>%_Presion</td>\n",
              "      <td>Tipo_Presion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2004</td>\n",
              "      <td>0,2433</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2005</td>\n",
              "      <td>0,2453</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2006</td>\n",
              "      <td>0,2508</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2007</td>\n",
              "      <td>0,2637</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f7387e4-6dbf-456d-8884-63dbd54f0c87')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9f7387e4-6dbf-456d-8884-63dbd54f0c87 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9f7387e4-6dbf-456d-8884-63dbd54f0c87');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7bafe7cd-a639-4064-9d4d-1f1039cdadf4\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7bafe7cd-a639-4064-9d4d-1f1039cdadf4')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7bafe7cd-a639-4064-9d4d-1f1039cdadf4 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(f\\\"An unexpected error occurred: {e}\\\")\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"America Latina\",\n          \"Region\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Argentina\",\n          \"Pa\\u00eds\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 2,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2004\",\n          \"2007\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 3,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"0,2433\",\n          \"0,2637\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 4,\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Presi\\u00f3n Fiscal Total\",\n          \"Tipo_Presion\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3044 entries, 0 to 3043\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   0       3044 non-null   object\n",
            " 1   1       3044 non-null   object\n",
            " 2   2       3044 non-null   object\n",
            " 3   3       3044 non-null   object\n",
            " 4   4       3044 non-null   object\n",
            "dtypes: object(5)\n",
            "memory usage: 119.0+ KB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Primeras 5 filas del DataFrame después de asignar nombres y limpiar %_Presion:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "           Region       País   Año  %_Presion          Tipo_Presion\n",
              "0          Region       País   Año  %_Presion          Tipo_Presion\n",
              "1  America Latina  Argentina  2004     0,2433  Presión Fiscal Total\n",
              "2  America Latina  Argentina  2005     0,2453  Presión Fiscal Total\n",
              "3  America Latina  Argentina  2006     0,2508  Presión Fiscal Total\n",
              "4  America Latina  Argentina  2007     0,2637  Presión Fiscal Total"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0ed84042-f445-4b6c-94ce-5ed2d65f70ee\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Region</th>\n",
              "      <th>País</th>\n",
              "      <th>Año</th>\n",
              "      <th>%_Presion</th>\n",
              "      <th>Tipo_Presion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Region</td>\n",
              "      <td>País</td>\n",
              "      <td>Año</td>\n",
              "      <td>%_Presion</td>\n",
              "      <td>Tipo_Presion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2004</td>\n",
              "      <td>0,2433</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2005</td>\n",
              "      <td>0,2453</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2006</td>\n",
              "      <td>0,2508</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2007</td>\n",
              "      <td>0,2637</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ed84042-f445-4b6c-94ce-5ed2d65f70ee')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0ed84042-f445-4b6c-94ce-5ed2d65f70ee button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0ed84042-f445-4b6c-94ce-5ed2d65f70ee');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-17707e55-2791-4584-b89c-6ffd29733225\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-17707e55-2791-4584-b89c-6ffd29733225')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-17707e55-2791-4584-b89c-6ffd29733225 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(f\\\"An unexpected error occurred: {e}\\\")\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Region\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"America Latina\",\n          \"Region\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pa\\u00eds\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Argentina\",\n          \"Pa\\u00eds\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"A\\u00f1o\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2004\",\n          \"2007\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"%_Presion\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"0,2433\",\n          \"0,2637\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tipo_Presion\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Presi\\u00f3n Fiscal Total\",\n          \"Tipo_Presion\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Información de la estructura del DataFrame después de asignar nombres y limpiar %_Presion:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3044 entries, 0 to 3043\n",
            "Data columns (total 5 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   Region        3044 non-null   object\n",
            " 1   País          3044 non-null   object\n",
            " 2   Año           3044 non-null   object\n",
            " 3   %_Presion     3044 non-null   object\n",
            " 4   Tipo_Presion  3044 non-null   object\n",
            "dtypes: object(5)\n",
            "memory usage: 119.0+ KB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c530a37"
      },
      "source": [
        "## Convertir tipos de datos\n",
        "\n",
        "### Subtask:\n",
        "Convertir las columnas a los tipos de datos apropiados (por ejemplo, 'Año' a entero, '%_Presion' a numérico)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ec607eb"
      },
      "source": [
        "**Reasoning**:\n",
        "Inspect the current data types, convert 'Año' to integer and '%_Presion' to numeric using errors='coerce', and display the data types and head to confirm the conversions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b028c16"
      },
      "source": [
        "## Manejar valores nulos\n",
        "\n",
        "### Subtask:\n",
        "Identificar y decidir cómo tratar los valores nulos (eliminarlos, imputar un valor, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfd4d082"
      },
      "source": [
        "## Normalizar valores (opcional)\n",
        "\n",
        "### Subtask:\n",
        "Si es necesario, aplique normalizaciones a los valores de algunas columnas (por ejemplo, estandarizar nombres de países o regiones)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c5d82f3"
      },
      "source": [
        "## Verificar la limpieza\n",
        "\n",
        "### Subtask:\n",
        "Mostrar las primeras filas y la información del DataFrame limpio para confirmar las transformaciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cab7366c"
      },
      "source": [
        "### 7. Carga de Datos a Google BigQuery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17dbe2e9"
      },
      "source": [
        "Este bloque carga el DataFrame limpio a una tabla específica en Google BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ce3d508",
        "outputId": "2fb3d6c5-9b74-4663-fdcd-282340fe70cf"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Cargar credenciales - using the environment variable set after uploading the file\n",
        "credentials = service_account.Credentials.from_service_account_file(os.environ['GOOGLE_APPLICATION_CREDENTIALS'])\n",
        "\n",
        "# Inicializar cliente\n",
        "client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
        "\n",
        "# Crear Dataset\n",
        "dataset_id = f\"{client.project}.sri_presion_fiscal\"\n",
        "dataset = bigquery.Dataset(dataset_id)\n",
        "dataset.location = \"US\"  # Cambia según tu región\n",
        "\n",
        "try:\n",
        "    dataset = client.create_dataset(dataset, exists_ok=True)\n",
        "    print(f\"Dataset creado o ya existente: {dataset_id}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al crear el dataset: {e}\")\n",
        "\n",
        "# Crear Tabla\n",
        "table_id = f\"{dataset_id}.sri_presion_fiscal\"\n",
        "schema = [\n",
        "    bigquery.SchemaField(\"Region\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"País\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"Año\", \"INTEGER\"),\n",
        "    bigquery.SchemaField(\"%_Presion\", \"FLOAT\"),\n",
        "    bigquery.SchemaField(\"Tipo_Presion\", \"STRING\")\n",
        "]\n",
        "\n",
        "table = bigquery.Table(table_id, schema=schema)\n",
        "\n",
        "# Explicitly drop the table if it exists, as requested by the user\n",
        "try:\n",
        "    client.delete_table(table_id, not_found_ok=True)\n",
        "    print(f\"Tabla eliminada si existía: {table_id}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al eliminar la tabla: {e}\")\n",
        "\n",
        "\n",
        "try:\n",
        "    table = client.create_table(table) # Create the table again\n",
        "    print(f\"Tabla creada: {table_id}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al crear la tabla: {e}\")\n",
        "\n",
        "# Cargar datos desde el DataFrame a BigQuery\n",
        "# The DataFrame 'df' is available from previous steps.\n",
        "\n",
        "# Remove the header row before loading to BigQuery as it's part of the data now\n",
        "df_to_load = df.iloc[1:].copy()\n",
        "\n",
        "# --- Explicit Data Type Conversion Before Loading ---\n",
        "\n",
        "# Convert 'Año' to integer, coercing errors to NaN and then to Int64 to handle potential NaNs\n",
        "df_to_load['Año'] = pd.to_numeric(df_to_load['Año'], errors='coerce').astype('Int64')\n",
        "\n",
        "# Convert '%_Presion' to float, replacing comma with dot and coercing errors to NaN\n",
        "df_to_load['%_Presion'] = df_to_load['%_Presion'].astype(str).str.replace(',', '.', regex=False)\n",
        "df_to_load['%_Presion'] = pd.to_numeric(df_to_load['%_Presion'], errors='coerce')\n",
        "\n",
        "# --- End of Explicit Data Type Conversion ---\n",
        "\n",
        "\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    schema=schema,\n",
        "    write_disposition=\"WRITE_TRUNCATE\", # Overwrite the table if it already exists (though we just dropped it)\n",
        ")\n",
        "\n",
        "try:\n",
        "    job = client.load_table_from_dataframe(\n",
        "        df_to_load, table_id, job_config=job_config\n",
        "    )  # Make an API request.\n",
        "\n",
        "    job.result()  # Wait for the job to complete.\n",
        "\n",
        "    print(f\"Datos cargados a la tabla {table_id}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar datos a BigQuery: {e}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset creado o ya existente: etl-sistemas-sri.sri_presion_fiscal\n",
            "Tabla eliminada si existía: etl-sistemas-sri.sri_presion_fiscal.sri_presion_fiscal\n",
            "Tabla creada: etl-sistemas-sri.sri_presion_fiscal.sri_presion_fiscal\n",
            "Datos cargados a la tabla etl-sistemas-sri.sri_presion_fiscal.sri_presion_fiscal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Orquestación con Airflow (documentación + DAG en .py)"
      ],
      "metadata": {
        "id": "rQv3Pgze-7t9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El flujo ETL completo se orquesta mediante un DAG definido en Python usando Apache Airflow. El archivo `etl_presion_fiscal_dag.py` contiene la definición del DAG."
      ],
      "metadata": {
        "id": "LQzLayEc_HhG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f9132a6"
      },
      "source": [
        "# Task\n",
        "Crea un DAG de Airflow en Python que descargue un archivo CSV desde una URL, lo transforme utilizando el código de transformación proporcionado previamente y cargue los datos resultantes en una tabla de Google BigQuery. El DAG debe incluir operadores, dependencias y comentarios explicativos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50723f5a"
      },
      "source": [
        "## Definir el dag\n",
        "\n",
        "### Subtask:\n",
        "Crear la estructura básica del DAG de Airflow, incluyendo imports necesarios, argumentos por defecto y la programación.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "da51d94c",
        "outputId": "f1143520-7334-45cd-8b97-e3b73445d0c4"
      },
      "source": [
        "from airflow import DAG\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Define default arguments for the DAG\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2023, 1, 1),\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "# Define the DAG\n",
        "dag = DAG(\n",
        "    'etl_presion_fiscal_sri',\n",
        "    default_args=default_args,\n",
        "    description='ETL pipeline to download, transform, and load SRI tax pressure data to BigQuery',\n",
        "    schedule_interval=None, # Set to None for manual trigger, or use a schedule like '@daily'\n",
        "    catchup=False,\n",
        "    tags=['etl', 'bigquery', 'sri'],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'airflow'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-63-2192615732.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mairflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDAG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define default arguments for the DAG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m default_args = {\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'airflow'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2b5534f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the `airflow` module is not installed in the current environment. Since Airflow is required for the subtask, I need to install it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "frh4s098_Y8o",
        "outputId": "e330320c-b367-480e-beb8-f47d4dcb0754"
      },
      "source": [
        "!pip install apache-airflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting apache-airflow\n",
            "  Downloading apache_airflow-3.0.2-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting apache-airflow-core==3.0.2 (from apache-airflow)\n",
            "  Downloading apache_airflow_core-3.0.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting apache-airflow-task-sdk<1.1.0,>=1.0.0 (from apache-airflow)\n",
            "  Downloading apache_airflow_task_sdk-1.0.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting a2wsgi>=1.10.8 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading a2wsgi-1.10.10-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting aiosqlite>=0.20.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting alembic<2.0,>=1.13.1 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting apache-airflow-providers-common-compat>=1.6.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading apache_airflow_providers_common_compat-1.7.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting apache-airflow-providers-common-io>=1.5.3 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading apache_airflow_providers_common_io-1.6.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting apache-airflow-providers-common-sql>=1.26.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading apache_airflow_providers_common_sql-1.27.2-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting apache-airflow-providers-smtp>=2.0.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading apache_airflow_providers_smtp-2.1.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting apache-airflow-providers-standard>=0.4.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading apache_airflow_providers_standard-1.3.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting argcomplete>=1.10 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading argcomplete-3.6.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting asgiref>=2.3.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: attrs!=25.2.0,>=22.1.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (25.3.0)\n",
            "Collecting cadwyn>=5.2.1 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading cadwyn-5.4.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting colorlog>=6.8.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting cron-descriptor>=1.2.24 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading cron_descriptor-1.4.5-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting croniter>=2.0.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading croniter-6.0.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: cryptography>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (43.0.3)\n",
            "Collecting deprecated>=1.2.13 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: dill>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (0.3.7)\n",
            "Requirement already satisfied: fastapi!=0.115.10,>=0.115.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow) (0.115.13)\n",
            "Requirement already satisfied: flask>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (3.1.1)\n",
            "Collecting gunicorn>=20.1.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.5 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (8.7.0)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.5 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=4.19.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (4.24.0)\n",
            "Collecting lazy-object-proxy>=1.2.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading lazy_object_proxy-1.11.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting libcst>=1.1.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading libcst-1.8.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: linkify-it-py>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.0.3)\n",
            "Collecting lockfile>=0.12.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading lockfile-0.12.2-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting methodtools>=0.4.7 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading methodtools-0.4.7-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting opentelemetry-api>=1.26.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp>=1.26.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_exporter_otlp-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (24.2)\n",
            "Collecting pathspec>=0.9.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pendulum<4.0,>=2.1.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading pendulum-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: pluggy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (1.6.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (5.9.5)\n",
            "Requirement already satisfied: pydantic>=2.11.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.11.7)\n",
            "Requirement already satisfied: pygments!=2.19.0,>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.19.2)\n",
            "Requirement already satisfied: pyjwt>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.10.1)\n",
            "Collecting python-daemon>=3.0.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading python_daemon-3.1.2-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify>=5.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (8.0.4)\n",
            "Requirement already satisfied: requests<3,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.32.3)\n",
            "Collecting rich-argparse>=1.0.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading rich_argparse-1.7.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: rich>=13.6.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (13.9.4)\n",
            "Requirement already satisfied: setproctitle>=1.3.3 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (1.3.6)\n",
            "Collecting sqlalchemy-jsonfield>=1.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading SQLAlchemy_JSONField-1.0.2-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting sqlalchemy-utils>=0.41.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading SQLAlchemy_Utils-0.41.2-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting sqlalchemy<2.0,>=1.4.49 (from sqlalchemy[asyncio]<2.0,>=1.4.49->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading SQLAlchemy-1.4.54-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting svcs>=25.1.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading svcs-25.1.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (0.9.0)\n",
            "Requirement already satisfied: tenacity!=8.2.0,>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (8.5.0)\n",
            "Requirement already satisfied: termcolor>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (3.1.0)\n",
            "Collecting typing-extensions!=4.14.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting universal-pathlib!=0.2.4,>=0.2.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading universal_pathlib-0.2.6-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting uuid6>=2024.7.10 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading uuid6-2025.0.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting aiologic>=0.14.0 (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow)\n",
            "  Downloading aiologic-0.14.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow) (2025.3.2)\n",
            "Collecting msgspec>=0.19.0 (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow)\n",
            "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting psutil>=5.8.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting retryhttp!=1.3.0,>=1.2.0 (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow)\n",
            "  Downloading retryhttp-1.3.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting structlog>=25.4.0 (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow)\n",
            "  Downloading structlog-25.4.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: wrapt>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from aiologic>=0.14.0->apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow) (1.17.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic<2.0,>=1.13.1->apache-airflow-core==3.0.2->apache-airflow) (1.1.3)\n",
            "Requirement already satisfied: sqlparse>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-common-sql>=1.26.0->apache-airflow-core==3.0.2->apache-airflow) (0.5.3)\n",
            "Requirement already satisfied: more-itertools>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-common-sql>=1.26.0->apache-airflow-core==3.0.2->apache-airflow) (10.7.0)\n",
            "Requirement already satisfied: starlette>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from cadwyn>=5.2.1->apache-airflow-core==3.0.2->apache-airflow) (0.46.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from cadwyn>=5.2.1->apache-airflow-core==3.0.2->apache-airflow) (0.4.1)\n",
            "Requirement already satisfied: pytz>2021.1 in /usr/local/lib/python3.11/dist-packages (from croniter>=2.0.2->apache-airflow-core==3.0.2->apache-airflow) (2025.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=41.0.0->apache-airflow-core==3.0.2->apache-airflow) (1.17.1)\n",
            "Collecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow) (0.0.20)\n",
            "Collecting email-validator>=2.0.0 (from fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow) (0.34.3)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask>=2.1.1->apache-airflow-core==3.0.2->apache-airflow) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=2.1.1->apache-airflow-core==3.0.2->apache-airflow) (8.2.1)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask>=2.1.1->apache-airflow-core==3.0.2->apache-airflow) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from flask>=2.1.1->apache-airflow-core==3.0.2->apache-airflow) (3.1.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->apache-airflow-core==3.0.2->apache-airflow) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->apache-airflow-core==3.0.2->apache-airflow) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->apache-airflow-core==3.0.2->apache-airflow) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->apache-airflow-core==3.0.2->apache-airflow) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->apache-airflow-core==3.0.2->apache-airflow) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.5->apache-airflow-core==3.0.2->apache-airflow) (3.23.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.1->apache-airflow-core==3.0.2->apache-airflow) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.1->apache-airflow-core==3.0.2->apache-airflow) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.1->apache-airflow-core==3.0.2->apache-airflow) (0.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.2 in /usr/local/lib/python3.11/dist-packages (from libcst>=1.1.0->apache-airflow-core==3.0.2->apache-airflow) (6.0.2)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.11/dist-packages (from linkify-it-py>=2.0.0->apache-airflow-core==3.0.2->apache-airflow) (1.0.3)\n",
            "Collecting wirerope>=0.4.7 (from methodtools>=0.4.7->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading wirerope-1.0.0-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc==1.34.1 (from opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http==1.34.1 (from opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.34.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.63.2 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow) (1.73.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk~=1.34.1 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: protobuf<6.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-proto==1.34.1->opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow) (5.29.5)\n",
            "Requirement already satisfied: tzdata>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pendulum<4.0,>=2.1.2->apache-airflow-core==3.0.2->apache-airflow) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.0->apache-airflow-core==3.0.2->apache-airflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.0->apache-airflow-core==3.0.2->apache-airflow) (2.33.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7.0->apache-airflow-core==3.0.2->apache-airflow) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify>=5.0->apache-airflow-core==3.0.2->apache-airflow) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.31.0->apache-airflow-core==3.0.2->apache-airflow) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.31.0->apache-airflow-core==3.0.2->apache-airflow) (2.4.0)\n",
            "Collecting types-requests (from retryhttp!=1.3.0,>=1.2.0->apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow)\n",
            "  Downloading types_requests-2.32.4.20250611-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.6.0->apache-airflow-core==3.0.2->apache-airflow) (3.0.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<2.0,>=1.4.49->sqlalchemy[asyncio]<2.0,>=1.4.49->apache-airflow-core==3.0.2->apache-airflow) (3.2.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=41.0.0->apache-airflow-core==3.0.2->apache-airflow) (2.22)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow) (0.16.0)\n",
            "Collecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading rich_toolkit-0.14.7-py3-none-any.whl.metadata (999 bytes)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.6.0->apache-airflow-core==3.0.2->apache-airflow) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->apache-airflow-core==3.0.2->apache-airflow) (1.3.1)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow) (15.0.1)\n",
            "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk~=1.34.1->opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow) (1.5.4)\n",
            "Downloading apache_airflow-3.0.2-py3-none-any.whl (12 kB)\n",
            "Downloading apache_airflow_core-3.0.2-py3-none-any.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading apache_airflow_task_sdk-1.0.2-py3-none-any.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.5/241.5 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading a2wsgi-1.10.10-py3-none-any.whl (17 kB)\n",
            "Downloading aiologic-0.14.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
            "Downloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading apache_airflow_providers_common_compat-1.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading apache_airflow_providers_common_io-1.6.0-py3-none-any.whl (19 kB)\n",
            "Downloading apache_airflow_providers_common_sql-1.27.2-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading apache_airflow_providers_smtp-2.1.0-py3-none-any.whl (21 kB)\n",
            "Downloading apache_airflow_providers_standard-1.3.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading argcomplete-3.6.2-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading cadwyn-5.4.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading cron_descriptor-1.4.5-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading croniter-6.0.0-py2.py3-none-any.whl (25 kB)\n",
            "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lazy_object_proxy-1.11.0-py3-none-any.whl (16 kB)\n",
            "Downloading libcst-1.8.2-cp311-cp311-manylinux_2_28_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lockfile-0.12.2-py2.py3-none-any.whl (13 kB)\n",
            "Downloading methodtools-0.4.7-py2.py3-none-any.whl (4.0 kB)\n",
            "Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp-1.34.1-py3-none-any.whl (7.0 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_http-1.34.1-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.34.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading pendulum-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (353 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m353.7/353.7 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_daemon-3.1.2-py3-none-any.whl (30 kB)\n",
            "Downloading retryhttp-1.3.3-py3-none-any.whl (17 kB)\n",
            "Downloading rich_argparse-1.7.1-py3-none-any.whl (25 kB)\n",
            "Downloading SQLAlchemy-1.4.54-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy_JSONField-1.0.2-py3-none-any.whl (10 kB)\n",
            "Downloading SQLAlchemy_Utils-0.41.2-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading structlog-25.4.0-py3-none-any.whl (68 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.7/68.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading svcs-25.1.0-py3-none-any.whl (19 kB)\n",
            "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading universal_pathlib-0.2.6-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uuid6-2025.0.0-py3-none-any.whl (7.0 kB)\n",
            "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\n",
            "Downloading wirerope-1.0.0-py2.py3-none-any.whl (9.2 kB)\n",
            "Downloading types_requests-2.32.4.20250611-py3-none-any.whl (20 kB)\n",
            "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading rich_toolkit-0.14.7-py3-none-any.whl (24 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lockfile, cron-descriptor, wirerope, uvloop, uuid6, universal-pathlib, typing-extensions, types-requests, svcs, structlog, sqlalchemy, python-dotenv, python-daemon, psutil, pathspec, opentelemetry-proto, msgspec, libcst, lazy-object-proxy, httptools, gunicorn, dnspython, deprecated, colorlog, asgiref, argcomplete, aiologic, a2wsgi, sqlalchemy-utils, sqlalchemy-jsonfield, pendulum, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, methodtools, email-validator, croniter, alembic, aiosqlite, watchfiles, rich-toolkit, rich-argparse, opentelemetry-semantic-conventions, retryhttp, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, fastapi-cli, cadwyn, opentelemetry-exporter-otlp, apache-airflow-providers-common-compat, apache-airflow-providers-standard, apache-airflow-providers-smtp, apache-airflow-providers-common-sql, apache-airflow-providers-common-io, apache-airflow-task-sdk, apache-airflow-core, apache-airflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.14.0\n",
            "    Uninstalling typing_extensions-4.14.0:\n",
            "      Successfully uninstalled typing_extensions-4.14.0\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.41\n",
            "    Uninstalling SQLAlchemy-2.0.41:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.41\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\n",
            "typeguard 4.4.4 requires typing_extensions>=4.14.0, but you have typing-extensions 4.13.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed a2wsgi-1.10.10 aiologic-0.14.0 aiosqlite-0.21.0 alembic-1.16.2 apache-airflow-3.0.2 apache-airflow-core-3.0.2 apache-airflow-providers-common-compat-1.7.1 apache-airflow-providers-common-io-1.6.0 apache-airflow-providers-common-sql-1.27.2 apache-airflow-providers-smtp-2.1.0 apache-airflow-providers-standard-1.3.0 apache-airflow-task-sdk-1.0.2 argcomplete-3.6.2 asgiref-3.8.1 cadwyn-5.4.2 colorlog-6.9.0 cron-descriptor-1.4.5 croniter-6.0.0 deprecated-1.2.18 dnspython-2.7.0 email-validator-2.2.0 fastapi-cli-0.0.7 gunicorn-23.0.0 httptools-0.6.4 lazy-object-proxy-1.11.0 libcst-1.8.2 lockfile-0.12.2 methodtools-0.4.7 msgspec-0.19.0 opentelemetry-api-1.34.1 opentelemetry-exporter-otlp-1.34.1 opentelemetry-exporter-otlp-proto-common-1.34.1 opentelemetry-exporter-otlp-proto-grpc-1.34.1 opentelemetry-exporter-otlp-proto-http-1.34.1 opentelemetry-proto-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 pathspec-0.12.1 pendulum-3.1.0 psutil-7.0.0 python-daemon-3.1.2 python-dotenv-1.1.1 retryhttp-1.3.3 rich-argparse-1.7.1 rich-toolkit-0.14.7 sqlalchemy-1.4.54 sqlalchemy-jsonfield-1.0.2 sqlalchemy-utils-0.41.2 structlog-25.4.0 svcs-25.1.0 types-requests-2.32.4.20250611 typing-extensions-4.13.2 universal-pathlib-0.2.6 uuid6-2025.0.0 uvloop-0.21.0 watchfiles-1.1.0 wirerope-1.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              },
              "id": "13da78e6554d4a44be6b8473142eadcb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce762c07"
      },
      "source": [
        "**Razonamiento**:\n",
        "Airflow se ha instalado correctamente. Ahora puedo proceder a crear la estructura básica del DAG de Airflow, como se solicita en la subtarea. Esto implica importar los módulos necesarios, definir los argumentos predeterminados e instanciar el objeto DAG.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "73896b4c",
        "outputId": "6559037c-20b5-46e0-ee58-8d06077e9564"
      },
      "source": [
        "from airflow import DAG\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Define default arguments for the DAG\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2023, 1, 1),\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "# Define the DAG\n",
        "dag = DAG(\n",
        "    'etl_presion_fiscal_sri',\n",
        "    default_args=default_args,\n",
        "    description='ETL pipeline to download, transform, and load SRI tax pressure data to BigQuery',\n",
        "    schedule_interval=None, # Set to None for manual trigger, or use a schedule like '@daily'\n",
        "    catchup=False,\n",
        "    tags=['etl', 'bigquery', 'sri'],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "DAG.__init__() got an unexpected keyword argument 'schedule_interval'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-2192615732.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Define the DAG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m dag = DAG(\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;34m'etl_presion_fiscal_sri'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdefault_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: DAG.__init__() got an unexpected keyword argument 'schedule_interval'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64672d71"
      },
      "source": [
        "**Razonamiento**:\n",
        "El seguimiento indica que el constructor `DAG` en la versión instalada de Airflow no acepta el argumento `schedule_interval`. Este argumento quedó obsoleto y se reemplazó por `schedule`. Necesito actualizar la instanciación de DAG para usar `schedule` en lugar de `schedule_interval`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOnsCioV_mGU"
      },
      "source": [
        "from airflow import DAG\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Define default arguments for the DAG\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2023, 1, 1),\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "# Define the DAG\n",
        "dag = DAG(\n",
        "    'etl_presion_fiscal_sri',\n",
        "    default_args=default_args,\n",
        "    description='ETL pipeline to download, transform, and load SRI tax pressure data to BigQuery',\n",
        "    schedule=None, # Use 'schedule' instead of 'schedule_interval'\n",
        "    catchup=False,\n",
        "    tags=['etl', 'bigquery', 'sri'],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfaa7718"
      },
      "source": [
        "## Tarea de descarga\n",
        "\n",
        "### Subtask:\n",
        "Definir una tarea en el DAG para descargar el archivo CSV.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        },
        "id": "f41c4c0c",
        "outputId": "7f25f386-cf23-4291-b98d-1607a341b173"
      },
      "source": [
        "import requests\n",
        "\n",
        "def download_csv_from_url(csv_url, local_filename):\n",
        "    \"\"\"Downloads a CSV file from a given URL and saves it locally.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(csv_url)\n",
        "        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "        with open(local_filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        print(f\"Archivo '{local_filename}' descargado exitosamente.\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error al descargar el archivo: {e}\")\n",
        "        # In a real Airflow DAG, you might want to raise an exception\n",
        "        # to fail the task, e.g., raise ValueError(f\"Download failed: {e}\")\n",
        "\n",
        "\n",
        "csv_url = \"https://www.sri.gob.ec/o/sri-portlet-biblioteca-alfresco-internet/descargar/7e45627e-1f7e-4e21-ae59-d520634fc63f/SRI_Presion_Fiscal.csv\"\n",
        "local_filename = \"SRI_Presion_Fiscal.csv\"\n",
        "\n",
        "# Define the download task using PythonOperator\n",
        "from airflow.operators.python import PythonOperator\n",
        "\n",
        "download_csv_task = PythonOperator(\n",
        "    task_id='download_csv',\n",
        "    python_callable=download_csv_from_url,\n",
        "    op_kwargs={'csv_url': csv_url, 'local_filename': local_filename},\n",
        "    dag=dag,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m/tmp/\u001b[0m\u001b[1;33mipython-input-3-3312975861.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m24\u001b[0m\u001b[1;33m DeprecationWarning\u001b[0m\u001b[33m: The `airflow.operators.python.PythonOperator` class is deprecated. Please use `\u001b[0m\u001b[33m'airflow.providers.standard.operators.python.PythonOperator'\u001b[0m\u001b[33m`.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/tmp/ipython-input-3-3312975861.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">24</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> DeprecationWarning</span><span style=\"color: #808000; text-decoration-color: #808000\">: The `airflow.operators.python.PythonOperator` class is deprecated. Please use `</span><span style=\"color: #808000; text-decoration-color: #808000\">'airflow.providers.standard.operators.python.PythonOperator'</span><span style=\"color: #808000; text-decoration-color: #808000\">`.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de1125a7"
      },
      "source": [
        "## Tarea de transformación\n",
        "\n",
        "### Subtask:\n",
        "Definir una tarea que ejecute el código de transformación de datos que desarrollaste en Colab (limpieza, conversión de tipos, etc.).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07c91134"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a Python function for data transformation and instantiate a PythonOperator task to execute it, passing the filename.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "106f390f",
        "outputId": "1ac38a90-1a51-40c3-d159-c489e62012d8"
      },
      "source": [
        "import pandas as pd\n",
        "from airflow.providers.standard.operators.python import PythonOperator\n",
        "\n",
        "def transform_data(local_filename):\n",
        "    \"\"\"\n",
        "    Loads data from a CSV, cleans and transforms it, and returns a DataFrame.\n",
        "\n",
        "    Steps:\n",
        "    1. Reload the CSV assuming semicolon as a separator and no header.\n",
        "    2. Remove the header row which is present as the first data row.\n",
        "    3. Rename columns to meaningful names.\n",
        "    4. Clean the '%_Presion' column by removing leading/trailing spaces.\n",
        "    5. Convert 'Año' to integer and '%_Presion' to float, coercing errors.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Reload the CSV with correct separator and no header\n",
        "        df = pd.read_csv(local_filename, encoding='latin-1', header=None, sep=';')\n",
        "\n",
        "        # 2. Remove the header row (the first row)\n",
        "        df = df.iloc[1:].copy()\n",
        "\n",
        "        # Reset index after removing the row\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        # 3. Rename columns\n",
        "        df.columns = ['Region', 'País', 'Año', '%_Presion', 'Tipo_Presion']\n",
        "\n",
        "        # 4. Clean up the '%_Presion' column\n",
        "        df['%_Presion'] = df['%_Presion'].astype(str).str.strip()\n",
        "\n",
        "        # 5. Convert data types\n",
        "        # Convert 'Año' to integer, coercing errors to NaN and then to Int64\n",
        "        df['Año'] = pd.to_numeric(df['Año'], errors='coerce').astype('Int64')\n",
        "\n",
        "        # Convert '%_Presion' to float, replacing comma with dot and coercing errors\n",
        "        df['%_Presion'] = df['%_Presion'].astype(str).str.replace(',', '.', regex=False)\n",
        "        df['%_Presion'] = pd.to_numeric(df['%_Presion'], errors='coerce')\n",
        "\n",
        "        print(\"Data transformation completed successfully.\")\n",
        "        print(\"Transformed DataFrame head:\")\n",
        "        display(df.head())\n",
        "        print(\"\\nTransformed DataFrame info:\")\n",
        "        display(df.info())\n",
        "\n",
        "        return df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{local_filename}' was not found during transformation.\")\n",
        "        # In a real Airflow DAG, you might want to raise an exception\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during transformation: {e}\")\n",
        "        # In a real Airflow DAG, you might want to raise an exception\n",
        "        raise\n",
        "\n",
        "# Define the transform task using PythonOperator\n",
        "# Ensure this is only defined once within the DAG context\n",
        "transform_data_task = PythonOperator(\n",
        "    task_id='transform_data',\n",
        "    python_callable=transform_data,\n",
        "    op_kwargs={'local_filename': local_filename}, # Pass the filename to the transformation function\n",
        "    dag=dag,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'local_filename' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-1033852589.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mtask_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'transform_data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mpython_callable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mop_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'local_filename'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlocal_filename\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Pass the filename to the transformation function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mdag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'local_filename' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de291fb5"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `local_filename` was not defined in the current execution scope when instantiating the `PythonOperator`. `local_filename` was defined in a previous code cell but is not automatically available. I need to redefine `local_filename` before creating the `transform_data_task`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "FKespm5W_zSw",
        "outputId": "d88444a5-5153-4ef9-9dca-5cc39a64f323"
      },
      "source": [
        "import pandas as pd\n",
        "from airflow.providers.standard.operators.python import PythonOperator\n",
        "\n",
        "# Define the local filename again so it's available in this scope\n",
        "local_filename = \"SRI_Presion_Fiscal.csv\"\n",
        "\n",
        "\n",
        "def transform_data(local_filename):\n",
        "    \"\"\"\n",
        "    Loads data from a CSV, cleans and transforms it, and returns a DataFrame.\n",
        "\n",
        "    Steps:\n",
        "    1. Reload the CSV assuming semicolon as a separator and no header.\n",
        "    2. Remove the header row which is present as the first data row.\n",
        "    3. Rename columns to meaningful names.\n",
        "    4. Clean the '%_Presion' column by removing leading/trailing spaces.\n",
        "    5. Convert 'Año' to integer and '%_Presion' to float, coercing errors.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Reload the CSV with correct separator and no header\n",
        "        df = pd.read_csv(local_filename, encoding='latin-1', header=None, sep=';')\n",
        "\n",
        "        # 2. Remove the header row (the first row)\n",
        "        df = df.iloc[1:].copy()\n",
        "\n",
        "        # Reset index after removing the row\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        # 3. Rename columns\n",
        "        df.columns = ['Region', 'País', 'Año', '%_Presion', 'Tipo_Presion']\n",
        "\n",
        "        # 4. Clean up the '%_Presion' column\n",
        "        df['%_Presion'] = df['%_Presion'].astype(str).str.strip()\n",
        "\n",
        "        # 5. Convert data types\n",
        "        # Convert 'Año' to integer, coercing errors to NaN and then to Int64\n",
        "        df['Año'] = pd.to_numeric(df['Año'], errors='coerce').astype('Int64')\n",
        "\n",
        "        # Convert '%_Presion' to float, replacing comma with dot and coercing errors\n",
        "        df['%_Presion'] = df['%_Presion'].astype(str).str.replace(',', '.', regex=False)\n",
        "        df['%_Presion'] = pd.to_numeric(df['%_Presion'], errors='coerce')\n",
        "\n",
        "\n",
        "        print(\"Data transformation completed successfully.\")\n",
        "        print(\"Transformed DataFrame head:\")\n",
        "        display(df.head())\n",
        "        print(\"\\nTransformed DataFrame info:\")\n",
        "        display(df.info())\n",
        "\n",
        "\n",
        "        return df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{local_filename}' was not found during transformation.\")\n",
        "        # In a real Airflow DAG, you might want to raise an exception\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during transformation: {e}\")\n",
        "        # In a real Airflow DAG, you might want to raise an exception\n",
        "        raise\n",
        "\n",
        "\n",
        "# Define the transform task using PythonOperator\n",
        "# Ensure this is only defined once within the DAG context\n",
        "transform_data_task = PythonOperator(\n",
        "    task_id='transform_data',\n",
        "    python_callable=transform_data,\n",
        "    op_kwargs={'local_filename': local_filename}, # Pass the filename to the transformation function\n",
        "    dag=dag,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dag' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-2000902420.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mpython_callable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mop_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'local_filename'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlocal_filename\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Pass the filename to the transformation function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mdag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dag' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7496391"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the `dag` object was not defined in the current execution scope. It was defined in a previous cell. I need to redefine the basic DAG structure, including the necessary imports and default arguments, in this cell before creating the `transform_data_task`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHmPKnXf_1Hs"
      },
      "source": [
        "from airflow import DAG\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "from airflow.providers.standard.operators.python import PythonOperator\n",
        "\n",
        "# Define default arguments for the DAG (redefined for this cell)\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2023, 1, 1),\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "# Define the DAG (redefined for this cell)\n",
        "dag = DAG(\n",
        "    'etl_presion_fiscal_sri',\n",
        "    default_args=default_args,\n",
        "    description='ETL pipeline to download, transform, and load SRI tax pressure data to BigQuery',\n",
        "    schedule=None,\n",
        "    catchup=False,\n",
        "    tags=['etl', 'bigquery', 'sri'],\n",
        ")\n",
        "\n",
        "# Define the local filename again so it's available in this scope\n",
        "local_filename = \"SRI_Presion_Fiscal.csv\"\n",
        "\n",
        "\n",
        "def transform_data(local_filename):\n",
        "    \"\"\"\n",
        "    Loads data from a CSV, cleans and transforms it, and returns a DataFrame.\n",
        "\n",
        "    Steps:\n",
        "    1. Reload the CSV assuming semicolon as a separator and no header.\n",
        "    2. Remove the header row which is present as the first data row.\n",
        "    3. Rename columns to meaningful names.\n",
        "    4. Clean the '%_Presion' column by removing leading/trailing spaces.\n",
        "    5. Convert 'Año' to integer and '%_Presion' to float, coercing errors.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Reload the CSV with correct separator and no header\n",
        "        df = pd.read_csv(local_filename, encoding='latin-1', header=None, sep=';')\n",
        "\n",
        "        # 2. Remove the header row (the first row)\n",
        "        df = df.iloc[1:].copy()\n",
        "\n",
        "        # Reset index after removing the row\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        # 3. Rename columns\n",
        "        df.columns = ['Region', 'País', 'Año', '%_Presion', 'Tipo_Presion']\n",
        "\n",
        "        # 4. Clean up the '%_Presion' column\n",
        "        df['%_Presion'] = df['%_Presion'].astype(str).str.strip()\n",
        "\n",
        "        # 5. Convert data types\n",
        "        # Convert 'Año' to integer, coercing errors to NaN and then to Int64\n",
        "        df['Año'] = pd.to_numeric(df['Año'], errors='coerce').astype('Int64')\n",
        "\n",
        "        # Convert '%_Presion' to float, replacing comma with dot and coercing errors\n",
        "        df['%_Presion'] = df['%_Presion'].astype(str).str.replace(',', '.', regex=False)\n",
        "        df['%_Presion'] = pd.to_numeric(df['%_Presion'], errors='coerce')\n",
        "\n",
        "        print(\"Data transformation completed successfully.\")\n",
        "        print(\"Transformed DataFrame head:\")\n",
        "        display(df.head())\n",
        "        print(\"\\nTransformed DataFrame info:\")\n",
        "        display(df.info())\n",
        "\n",
        "        return df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{local_filename}' was not found during transformation.\")\n",
        "        # In a real Airflow DAG, you might want to raise an exception\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during transformation: {e}\")\n",
        "        # In a real Airflow DAG, you might want to raise an exception\n",
        "        raise\n",
        "\n",
        "\n",
        "# Define the transform task using PythonOperator\n",
        "# Ensure this is only defined once within the DAG context\n",
        "transform_data_task = PythonOperator(\n",
        "    task_id='transform_data',\n",
        "    python_callable=transform_data,\n",
        "    op_kwargs={'local_filename': local_filename}, # Pass the filename to the transformation function\n",
        "    dag=dag,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf3d3522"
      },
      "source": [
        "## Tarea de carga a bigquery\n",
        "\n",
        "### Subtask:\n",
        "Definir una tarea para cargar el DataFrame transformado en la tabla de Google BigQuery.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67853cfb"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary BigQuery operators and define the Python function to load data to BigQuery using the BigQuery client. Then, instantiate the PythonOperator for the loading task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "4f27a8f4",
        "outputId": "3a2f82f0-e2ac-4e66-e7d5-4710be11e265"
      },
      "source": [
        "from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator\n",
        "from google.cloud import bigquery\n",
        "import os\n",
        "\n",
        "# Define BigQuery parameters (replace with your actual values)\n",
        "PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT', credentials.project_id) # Use the project ID from credentials\n",
        "DATASET_ID = \"sri_presion_fiscal\"\n",
        "TABLE_ID = \"sri_presion_fiscal\"\n",
        "BIGQUERY_CONN_ID = 'google_cloud_default' # Ensure you have a Google Cloud connection configured in Airflow\n",
        "\n",
        "def load_data_to_bigquery(df: pd.DataFrame, project_id: str, dataset_id: str, table_id: str):\n",
        "    \"\"\"\n",
        "    Loads a Pandas DataFrame into a Google BigQuery table.\n",
        "    Assumes GOOGLE_APPLICATION_CREDENTIALS environment variable is set or\n",
        "    Airflow connection handles authentication.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize BigQuery client\n",
        "        # The client will automatically use credentials from GOOGLE_APPLICATION_CREDENTIALS\n",
        "        # or from the Airflow connection if configured.\n",
        "        client = bigquery.Client(project=project_id)\n",
        "\n",
        "        table_ref = client.dataset(dataset_id).table(table_id)\n",
        "\n",
        "        # Define the schema based on the DataFrame columns and expected types\n",
        "        # This assumes the DataFrame columns are already in the correct order and types\n",
        "        # after the transformation step.\n",
        "        schema = [\n",
        "            bigquery.SchemaField(\"Region\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"País\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"Año\", \"INT64\"), # Use INT64 for pandas Int64 dtype\n",
        "            bigquery.SchemaField(\"%_Presion\", \"FLOAT64\"), # Use FLOAT64 for pandas float64 dtype\n",
        "            bigquery.SchemaField(\"Tipo_Presion\", \"STRING\"),\n",
        "        ]\n",
        "\n",
        "        # Configure the load job\n",
        "        job_config = bigquery.LoadJobConfig(\n",
        "            schema=schema,\n",
        "            write_disposition=\"WRITE_TRUNCATE\",  # Overwrite the table\n",
        "            # You might need to specify source_format if loading from a file,\n",
        "            # but when loading from DataFrame, BigQuery client handles this.\n",
        "        )\n",
        "\n",
        "        # Load data from DataFrame\n",
        "        job = client.load_table_from_dataframe(\n",
        "            df, table_ref, job_config=job_config\n",
        "        )\n",
        "\n",
        "        print(f\"Starting BigQuery load job: {job.job_id}\")\n",
        "        job.result()  # Wait for the job to complete\n",
        "\n",
        "        print(f\"DataFrame successfully loaded to {project_id}.{dataset_id}.{table_id}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data to BigQuery: {e}\")\n",
        "        # In a real Airflow DAG, you might want to raise an exception\n",
        "        raise\n",
        "\n",
        "\n",
        "# Instantiate the PythonOperator for the loading task\n",
        "# We need to pass the DataFrame from the transform_data_task to this task.\n",
        "# In Airflow, this is typically done by using XComs.\n",
        "# However, passing a large DataFrame via XComs is not recommended due to performance issues.\n",
        "# A better approach would be to save the transformed DataFrame to a temporary location (e.g., GCS)\n",
        "# in the transform_data_task and then use a BigQueryLoadOperator to load from that location.\n",
        "# For the purpose of this exercise, we will assume the DataFrame is passed (conceptually via XComs)\n",
        "# or available in a shared location that the load task can access.\n",
        "# Since we are defining the DAG structure, we will define the task and indicate\n",
        "# that it depends on the output of the transform task. The actual data passing\n",
        "# mechanism would be handled by Airflow's execution environment (e.g., XComs or shared storage).\n",
        "\n",
        "# For demonstration purposes in this DAG definition script, we will define the task\n",
        "# and assume the 'df' object (returned by transform_data_task) is somehow available.\n",
        "# In a real Airflow execution, the 'python_callable' would receive the XCom value\n",
        "# if the previous task pushed it.\n",
        "\n",
        "load_to_bigquery_task = PythonOperator(\n",
        "    task_id='load_to_bigquery',\n",
        "    python_callable=load_data_to_bigquery,\n",
        "    op_kwargs={\n",
        "        'project_id': PROJECT_ID,\n",
        "        'dataset_id': DATASET_ID,\n",
        "        'table_id': TABLE_ID,\n",
        "        # We need to pass the DataFrame here. In a real DAG, this would\n",
        "        # typically be pulled from XComs or read from a file saved by the\n",
        "        # previous task. For this definition, we'll conceptually represent\n",
        "        # the dependency without explicitly handling the XCom pull in the op_kwargs\n",
        "        # as the callable function expects the DataFrame directly.\n",
        "        # A common pattern is to have the callable function pull the XCom itself.\n",
        "        # Let's modify the callable to accept a task instance and pull XCom.\n",
        "    },\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "# Let's redefine the load_data_to_bigquery function to accept task_instance\n",
        "# and pull the DataFrame from XComs.\n",
        "def load_data_to_bigquery_with_xcom(ti, project_id: str, dataset_id: str, table_id: str):\n",
        "    \"\"\"\n",
        "    Loads a Pandas DataFrame (pulled from XCom) into a Google BigQuery table.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Pull the DataFrame from the previous task's XCom\n",
        "        df = ti.xcom_pull(task_ids='transform_data')\n",
        "\n",
        "        if df is None:\n",
        "            raise ValueError(\"No DataFrame received from the transform_data task via XCom.\")\n",
        "\n",
        "        # Initialize BigQuery client\n",
        "        client = bigquery.Client(project=project_id)\n",
        "\n",
        "        table_ref = client.dataset(dataset_id).table(table_id)\n",
        "\n",
        "        # Define the schema based on the DataFrame columns and expected types\n",
        "        schema = [\n",
        "            bigquery.SchemaField(\"Region\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"País\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"Año\", \"INT64\"),\n",
        "            bigquery.SchemaField(\"%_Presion\", \"FLOAT64\"),\n",
        "            bigquery.SchemaField(\"Tipo_Presion\", \"STRING\"),\n",
        "        ]\n",
        "\n",
        "        # Configure the load job\n",
        "        job_config = bigquery.LoadJobConfig(\n",
        "            schema=schema,\n",
        "            write_disposition=\"WRITE_TRUNCATE\",\n",
        "        )\n",
        "\n",
        "        # Load data from DataFrame\n",
        "        job = client.load_table_from_dataframe(\n",
        "            df, table_ref, job_config=job_config\n",
        "        )\n",
        "\n",
        "        print(f\"Starting BigQuery load job: {job.job_id}\")\n",
        "        job.result()\n",
        "\n",
        "        print(f\"DataFrame successfully loaded to {project_id}.{dataset_id}.{table_id}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data to BigQuery: {e}\")\n",
        "        raise\n",
        "\n",
        "# Redefine the load_to_bigquery_task using the updated callable\n",
        "load_to_bigquery_task = PythonOperator(\n",
        "    task_id='load_to_bigquery',\n",
        "    python_callable=load_data_to_bigquery_with_xcom,\n",
        "    op_kwargs={\n",
        "        'project_id': PROJECT_ID,\n",
        "        'dataset_id': DATASET_ID,\n",
        "        'table_id': TABLE_ID,\n",
        "    },\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "# Add the dependency: transform_data_task must complete before load_to_bigquery_task starts\n",
        "# This dependency also implies that the output of transform_data_task (the DataFrame)\n",
        "# will be available for load_to_bigquery_task via XComs.\n",
        "# This line should be placed after both tasks are defined.\n",
        "# transform_data_task >> load_to_bigquery_task # This will be added in a later step.\n",
        "\n",
        "print(f\"Task '{load_to_bigquery_task.task_id}' defined for loading data to BigQuery.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'airflow.providers.google'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-2865656035.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mairflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproviders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigquery\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBigQueryInsertJobOperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbigquery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define BigQuery parameters (replace with your actual values)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'airflow.providers.google'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1aa7630"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the `airflow.providers.google` module was not found. This means the Google Cloud provider package for Airflow is not installed. I need to install the necessary Airflow provider package for Google Cloud before I can import the BigQuery operators. I will install `apache-airflow-providers-google`. After installing the provider, I will re-attempt the previous code block to define the BigQuery loading task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6yWakwzC_7JK",
        "outputId": "479136aa-66f1-4f44-8bf5-8e41be845667"
      },
      "source": [
        "!pip install apache-airflow-providers-google"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting apache-airflow-providers-google\n",
            "  Downloading apache_airflow_providers_google-16.0.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: apache-airflow>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (3.0.2)\n",
            "Requirement already satisfied: apache-airflow-providers-common-compat>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (1.7.1)\n",
            "Requirement already satisfied: apache-airflow-providers-common-sql>=1.27.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (1.27.2)\n",
            "Requirement already satisfied: asgiref>=3.5.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (3.8.1)\n",
            "Requirement already satisfied: dill>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (0.3.7)\n",
            "Collecting gcloud-aio-auth>=5.2.0 (from apache-airflow-providers-google)\n",
            "  Downloading gcloud_aio_auth-5.4.2-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting gcloud-aio-bigquery>=6.1.2 (from apache-airflow-providers-google)\n",
            "  Downloading gcloud_aio_bigquery-7.1.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting gcloud-aio-storage>=9.0.0 (from apache-airflow-providers-google)\n",
            "  Downloading gcloud_aio_storage-9.4.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: gcsfs>=2023.10.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (2025.3.2)\n",
            "Collecting google-ads>=26.0.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_ads-27.0.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting google-analytics-admin>=0.9.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_analytics_admin-0.24.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: google-api-core!=2.16.0,!=2.18.0,>=2.11.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (2.173.0)\n",
            "Requirement already satisfied: google-auth>=2.29.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (2.38.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (0.2.0)\n",
            "Requirement already satisfied: google-cloud-aiplatform>=1.73.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (1.99.0)\n",
            "Collecting google-cloud-alloydb>=0.4.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_alloydb-0.4.8-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting google-cloud-automl>=2.12.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_automl-2.16.4-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.21.*,!=3.22.0,!=3.23.*,>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (3.34.0)\n",
            "Collecting google-cloud-bigquery-datatransfer>=3.13.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_bigquery_datatransfer-3.19.2-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting google-cloud-bigtable>=2.17.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_bigtable-2.31.0-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting google-cloud-build>=3.31.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_build-3.31.2-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting google-cloud-compute>=1.10.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_compute-1.31.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting google-cloud-container>=2.52.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_container-2.57.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting google-cloud-datacatalog>=3.23.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_datacatalog-3.27.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting google-cloud-dataflow-client>=0.8.6 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_dataflow_client-0.9.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting google-cloud-dataform>=0.5.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_dataform-0.6.2-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting google-cloud-dataplex>=2.6.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_dataplex-2.10.2-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: google-cloud-dataproc>=5.12.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (5.20.0)\n",
            "Collecting google-cloud-dataproc-metastore>=1.12.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_dataproc_metastore-1.18.3-py3-none-any.whl.metadata (10.0 kB)\n",
            "Collecting google-cloud-dlp>=3.12.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_dlp-3.31.0-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting google-cloud-kms>=2.15.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_kms-3.5.1-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: google-cloud-language>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (2.17.2)\n",
            "Collecting google-cloud-logging>=3.5.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_logging-3.12.1-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting google-cloud-managedkafka>=0.1.6 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_managedkafka-0.1.11-py3-none-any.whl.metadata (9.6 kB)\n",
            "Collecting google-cloud-memcache>=1.7.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_memcache-1.12.2-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting google-cloud-monitoring>=2.18.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_monitoring-2.27.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting google-cloud-orchestration-airflow>=1.10.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_orchestration_airflow-1.17.5-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting google-cloud-os-login>=2.9.1 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_os_login-2.17.2-py3-none-any.whl.metadata (9.6 kB)\n",
            "Collecting google-cloud-pubsub>=2.21.3 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_pubsub-2.30.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting google-cloud-redis>=2.12.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_redis-2.18.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting google-cloud-secret-manager>=2.16.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_secret_manager-2.24.0-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: google-cloud-spanner!=3.49.0,>=3.11.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (3.55.0)\n",
            "Collecting google-cloud-speech>=2.18.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_speech-2.33.0-py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: google-cloud-storage>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (2.19.0)\n",
            "Collecting google-cloud-storage-transfer>=1.4.1 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_storage_transfer-1.17.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting google-cloud-tasks>=2.13.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_tasks-2.19.3-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting google-cloud-texttospeech>=2.14.1 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_texttospeech-2.27.0-py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: google-cloud-translate>=3.16.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (3.20.3)\n",
            "Collecting google-cloud-videointelligence>=2.11.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_videointelligence-2.16.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting google-cloud-vision>=3.4.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_vision-3.10.2-py3-none-any.whl.metadata (9.6 kB)\n",
            "Collecting google-cloud-workflows>=1.10.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_workflows-1.18.2-py3-none-any.whl.metadata (9.6 kB)\n",
            "Collecting google-cloud-run>=0.10.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_run-0.10.18-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting google-cloud-batch>=0.13.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_batch-0.17.36-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting grpcio-gcp>=0.2.2 (from apache-airflow-providers-google)\n",
            "  Downloading grpcio_gcp-0.2.2-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (0.28.1)\n",
            "Collecting looker-sdk!=24.18.0,>=22.4.0 (from apache-airflow-providers-google)\n",
            "  Downloading looker_sdk-25.10.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: pandas-gbq>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (0.29.1)\n",
            "Requirement already satisfied: proto-plus>=1.19.6 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (1.26.1)\n",
            "Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (18.1.0)\n",
            "Requirement already satisfied: python-slugify>=7.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (8.0.4)\n",
            "Requirement already satisfied: PyOpenSSL>=23.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (24.2.1)\n",
            "Collecting sqlalchemy-bigquery>=1.2.1 (from apache-airflow-providers-google)\n",
            "  Downloading sqlalchemy_bigquery-1.15.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting sqlalchemy-spanner>=1.6.2 (from apache-airflow-providers-google)\n",
            "  Downloading sqlalchemy_spanner-1.14.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: tenacity>=8.3.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (8.5.0)\n",
            "Requirement already satisfied: immutabledict>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (4.2.1)\n",
            "Collecting types-protobuf!=5.29.1.20250402,>=5.27.0 (from apache-airflow-providers-google)\n",
            "  Downloading types_protobuf-6.30.2.20250516-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: apache-airflow-core==3.0.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow>=2.10.0->apache-airflow-providers-google) (3.0.2)\n",
            "Requirement already satisfied: apache-airflow-task-sdk<1.1.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow>=2.10.0->apache-airflow-providers-google) (1.0.2)\n",
            "Requirement already satisfied: a2wsgi>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.10.10)\n",
            "Requirement already satisfied: aiosqlite>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.21.0)\n",
            "Requirement already satisfied: alembic<2.0,>=1.13.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.16.2)\n",
            "Requirement already satisfied: apache-airflow-providers-common-io>=1.5.3 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.6.0)\n",
            "Requirement already satisfied: apache-airflow-providers-smtp>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.1.0)\n",
            "Requirement already satisfied: apache-airflow-providers-standard>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.3.0)\n",
            "Requirement already satisfied: argcomplete>=1.10 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (3.6.2)\n",
            "Requirement already satisfied: attrs!=25.2.0,>=22.1.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (25.3.0)\n",
            "Requirement already satisfied: cadwyn>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (5.4.2)\n",
            "Requirement already satisfied: colorlog>=6.8.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (6.9.0)\n",
            "Requirement already satisfied: cron-descriptor>=1.2.24 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.4.5)\n",
            "Requirement already satisfied: croniter>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (6.0.0)\n",
            "Requirement already satisfied: cryptography>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (43.0.3)\n",
            "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.2.18)\n",
            "Requirement already satisfied: fastapi!=0.115.10,>=0.115.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.115.13)\n",
            "Requirement already satisfied: flask>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (3.1.1)\n",
            "Requirement already satisfied: gunicorn>=20.1.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (23.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=6.5 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (8.7.0)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.5 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=4.19.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (4.24.0)\n",
            "Requirement already satisfied: lazy-object-proxy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.11.0)\n",
            "Requirement already satisfied: libcst>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.8.2)\n",
            "Requirement already satisfied: linkify-it-py>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.0.3)\n",
            "Requirement already satisfied: lockfile>=0.12.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.12.2)\n",
            "Requirement already satisfied: methodtools>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.4.7)\n",
            "Requirement already satisfied: opentelemetry-api>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.34.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (24.2)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.12.1)\n",
            "Requirement already satisfied: pendulum<4.0,>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (3.1.0)\n",
            "Requirement already satisfied: pluggy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.6.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (7.0.0)\n",
            "Requirement already satisfied: pydantic>=2.11.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.11.7)\n",
            "Requirement already satisfied: pygments!=2.19.0,>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.19.2)\n",
            "Requirement already satisfied: pyjwt>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.10.1)\n",
            "Requirement already satisfied: python-daemon>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.9.0.post0)\n",
            "Requirement already satisfied: requests<3,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.32.3)\n",
            "Requirement already satisfied: rich-argparse>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.7.1)\n",
            "Requirement already satisfied: rich>=13.6.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (13.9.4)\n",
            "Requirement already satisfied: setproctitle>=1.3.3 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.3.6)\n",
            "Requirement already satisfied: sqlalchemy-jsonfield>=1.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.0.2)\n",
            "Requirement already satisfied: sqlalchemy-utils>=0.41.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.41.2)\n",
            "Requirement already satisfied: sqlalchemy<2.0,>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]<2.0,>=1.4.49->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.4.54)\n",
            "Requirement already satisfied: svcs>=25.1.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (25.1.0)\n",
            "Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.9.0)\n",
            "Requirement already satisfied: termcolor>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions!=4.14.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (4.13.2)\n",
            "Requirement already satisfied: universal-pathlib!=0.2.4,>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.2.6)\n",
            "Requirement already satisfied: uuid6>=2024.7.10 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2025.0.0)\n",
            "Requirement already satisfied: sqlparse>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-common-sql>=1.27.0->apache-airflow-providers-google) (0.5.3)\n",
            "Requirement already satisfied: more-itertools>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-common-sql>=1.27.0->apache-airflow-providers-google) (10.7.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.0 in /usr/local/lib/python3.11/dist-packages (from gcloud-aio-auth>=5.2.0->apache-airflow-providers-google) (3.11.15)\n",
            "Collecting backoff<3.0.0,>=1.0.0 (from gcloud-aio-auth>=5.2.0->apache-airflow-providers-google)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: chardet<6.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gcloud-aio-auth>=5.2.0->apache-airflow-providers-google) (5.2.0)\n",
            "Requirement already satisfied: aiofiles<25.0.0,>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from gcloud-aio-storage>=9.0.0->apache-airflow-providers-google) (24.1.0)\n",
            "Collecting pyasn1-modules<0.4.2,>=0.2.1 (from gcloud-aio-storage>=9.0.0->apache-airflow-providers-google)\n",
            "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: rsa<5.0.0,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from gcloud-aio-storage>=9.0.0->apache-airflow-providers-google) (4.9.1)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs>=2023.10.0->apache-airflow-providers-google) (4.4.2)\n",
            "Requirement already satisfied: fsspec==2025.3.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs>=2023.10.0->apache-airflow-providers-google) (2025.3.2)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (from gcsfs>=2023.10.0->apache-airflow-providers-google) (1.2.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.3 in /usr/local/lib/python3.11/dist-packages (from google-ads>=26.0.0->apache-airflow-providers-google) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.59.0 in /usr/local/lib/python3.11/dist-packages (from google-ads>=26.0.0->apache-airflow-providers-google) (1.73.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.59.0 in /usr/local/lib/python3.11/dist-packages (from google-ads>=26.0.0->apache-airflow-providers-google) (1.71.0)\n",
            "Requirement already satisfied: PyYAML<7.0,>=5.1 in /usr/local/lib/python3.11/dist-packages (from google-ads>=26.0.0->apache-airflow-providers-google) (6.0.2)\n",
            "Requirement already satisfied: protobuf<7.0.0,>=4.25.0 in /usr/local/lib/python3.11/dist-packages (from google-ads>=26.0.0->apache-airflow-providers-google) (5.29.5)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=2.0.2->apache-airflow-providers-google) (0.22.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=2.0.2->apache-airflow-providers-google) (4.2.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.29.0->apache-airflow-providers-google) (5.5.2)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform>=1.73.0->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (1.14.2)\n",
            "Requirement already satisfied: shapely<3.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform>=1.73.0->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (2.1.1)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform>=1.73.0->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (1.21.1)\n",
            "Requirement already satisfied: docstring_parser<1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform>=1.73.0->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (0.16)\n",
            "Collecting setuptools<70.0.0 (from google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google)\n",
            "  Downloading setuptools-69.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting ray<=2.42.0,>=2.5 (from ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google)\n",
            "  Downloading ray-2.42.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: google-cloud-bigquery-storage in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (2.32.0)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.23.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (1.6.1)\n",
            "Collecting ruamel.yaml (from google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google)\n",
            "  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-alloydb>=0.4.0->apache-airflow-providers-google) (0.14.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery!=3.21.*,!=3.22.0,!=3.23.*,>=3.4.0->apache-airflow-providers-google) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery!=3.21.*,!=3.22.0,!=3.23.*,>=3.4.0->apache-airflow-providers-google) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0.0dev,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigtable>=2.17.0->apache-airflow-providers-google) (1.7.1)\n",
            "Collecting google-cloud-appengine-logging<2.0.0,>=0.1.3 (from google-cloud-logging>=3.5.0->apache-airflow-providers-google)\n",
            "  Downloading google_cloud_appengine_logging-1.6.2-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting google-cloud-audit-log<1.0.0,>=0.3.1 (from google-cloud-logging>=3.5.0->apache-airflow-providers-google)\n",
            "  Downloading google_cloud_audit_log-0.3.2-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.27.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-pubsub>=2.21.3->apache-airflow-providers-google) (1.34.1)\n",
            "Requirement already satisfied: grpc-interceptor>=0.15.4 in /usr/local/lib/python3.11/dist-packages (from google-cloud-spanner!=3.49.0,>=3.11.1->apache-airflow-providers-google) (0.15.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->apache-airflow-providers-google) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->apache-airflow-providers-google) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->apache-airflow-providers-google) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->apache-airflow-providers-google) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->apache-airflow-providers-google) (0.16.0)\n",
            "Collecting cattrs>=1.3 (from looker-sdk!=24.18.0,>=22.4.0->apache-airflow-providers-google)\n",
            "  Downloading cattrs-25.1.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: db-dtypes<2.0.0,>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from pandas-gbq>=0.7.0->apache-airflow-providers-google) (1.4.3)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.11/dist-packages (from pandas-gbq>=0.7.0->apache-airflow-providers-google) (2.0.2)\n",
            "Requirement already satisfied: pydata-google-auth>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from pandas-gbq>=0.7.0->apache-airflow-providers-google) (1.9.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify>=7.0.0->apache-airflow-providers-google) (1.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.0->gcloud-aio-auth>=5.2.0->apache-airflow-providers-google) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.0->gcloud-aio-auth>=5.2.0->apache-airflow-providers-google) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.0->gcloud-aio-auth>=5.2.0->apache-airflow-providers-google) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.0->gcloud-aio-auth>=5.2.0->apache-airflow-providers-google) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.0->gcloud-aio-auth>=5.2.0->apache-airflow-providers-google) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.0->gcloud-aio-auth>=5.2.0->apache-airflow-providers-google) (1.20.1)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic<2.0,>=1.13.1->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.1.3)\n",
            "Requirement already satisfied: aiologic>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.14.0)\n",
            "Requirement already satisfied: msgspec>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.19.0)\n",
            "Requirement already satisfied: retryhttp!=1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.3.3)\n",
            "Requirement already satisfied: structlog>=25.4.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow>=2.10.0->apache-airflow-providers-google) (25.4.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=41.0.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.17.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib->gcsfs>=2023.10.0->apache-airflow-providers-google) (2.0.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform>=1.73.0->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (15.0.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->apache-airflow-providers-google) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client>=2.0.2->apache-airflow-providers-google) (3.2.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.1->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.1->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.1->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.25.1)\n",
            "Requirement already satisfied: wirerope>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from methodtools>=0.4.7->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.0.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.27.0->google-cloud-pubsub>=2.21.3->apache-airflow-providers-google) (0.55b1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (2025.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules<0.4.2,>=0.2.1->gcloud-aio-storage>=9.0.0->apache-airflow-providers-google) (0.6.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.17.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray<=2.42.0,>=2.5->ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (8.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray<=2.42.0,>=2.5->ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (3.18.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray<=2.42.0,>=2.5->ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (1.1.1)\n",
            "Collecting aiohttp-cors (from ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google)\n",
            "  Downloading aiohttp_cors-0.8.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting colorful (from ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google)\n",
            "  Downloading colorful-0.5.6-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Collecting opencensus (from ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google)\n",
            "  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (0.22.1)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.11/dist-packages (from ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (7.1.0)\n",
            "Collecting virtualenv!=20.21.1,>=20.0.24 (from ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google)\n",
            "  Downloading virtualenv-20.31.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting py-spy>=0.2.0 (from ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google)\n",
            "  Downloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.31.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.31.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.4.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<2.0,>=1.4.49->sqlalchemy[asyncio]<2.0,>=1.4.49->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (3.2.3)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (3.6.0)\n",
            "Requirement already satisfied: wrapt>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from aiologic>=0.14.0->apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.17.2)\n",
            "Requirement already satisfied: starlette>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from cadwyn>=5.2.1->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.46.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=41.0.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.22)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.0.7)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.0.20)\n",
            "Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.2.0)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.34.3)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask>=2.1.1->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.9.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask>=2.1.1->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from flask>=2.1.1->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (3.1.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.5->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (3.23.0)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.11/dist-packages (from linkify-it-py>=2.0.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.0.3)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.34.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs>=2023.10.0->apache-airflow-providers-google) (3.3.1)\n",
            "Requirement already satisfied: types-requests in /usr/local/lib/python3.11/dist-packages (from retryhttp!=1.3.0,>=1.2.0->apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.32.4.20250611)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.6.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (3.0.0)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (4.3.8)\n",
            "Collecting opencensus-context>=0.1.3 (from opencensus->ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google)\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.7.0)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.16.0)\n",
            "Requirement already satisfied: rich-toolkit>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.14.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.6.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.1.2)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.1.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.1.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.5.4)\n",
            "Downloading apache_airflow_providers_google-16.0.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gcloud_aio_auth-5.4.2-py3-none-any.whl (16 kB)\n",
            "Downloading gcloud_aio_bigquery-7.1.0-py3-none-any.whl (12 kB)\n",
            "Downloading gcloud_aio_storage-9.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading google_ads-27.0.0-py3-none-any.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_analytics_admin-0.24.1-py3-none-any.whl (397 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m398.0/398.0 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_alloydb-0.4.8-py3-none-any.whl (520 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.9/520.9 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_automl-2.16.4-py3-none-any.whl (367 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.3/367.3 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_batch-0.17.36-py3-none-any.whl (176 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_bigquery_datatransfer-3.19.2-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_bigtable-2.31.0-py3-none-any.whl (488 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.1/488.1 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_build-3.31.2-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.0/180.0 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_compute-1.31.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_container-2.57.0-py3-none-any.whl (308 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.9/308.9 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_datacatalog-3.27.1-py3-none-any.whl (367 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.6/367.6 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_dataflow_client-0.9.0-py3-none-any.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.3/284.3 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_dataform-0.6.2-py3-none-any.whl (250 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.6/250.6 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_dataplex-2.10.2-py3-none-any.whl (507 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_dataproc_metastore-1.18.3-py3-none-any.whl (433 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.8/433.8 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_dlp-3.31.0-py3-none-any.whl (215 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.7/215.7 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_kms-3.5.1-py3-none-any.whl (270 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.8/270.8 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_logging-3.12.1-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.5/229.5 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_managedkafka-0.1.11-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_memcache-1.12.2-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_monitoring-2.27.2-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_orchestration_airflow-1.17.5-py3-none-any.whl (281 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.0/281.0 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_os_login-2.17.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_pubsub-2.30.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_redis-2.18.1-py3-none-any.whl (145 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.3/145.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_run-0.10.18-py3-none-any.whl (333 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.8/333.8 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_secret_manager-2.24.0-py3-none-any.whl (218 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.1/218.1 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_speech-2.33.0-py3-none-any.whl (335 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.7/335.7 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_storage_transfer-1.17.0-py3-none-any.whl (90 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.8/90.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_tasks-2.19.3-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.7/289.7 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_texttospeech-2.27.0-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.4/189.4 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_videointelligence-2.16.2-py3-none-any.whl (275 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_vision-3.10.2-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.9/527.9 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_workflows-1.18.2-py3-none-any.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio_gcp-0.2.2-py2.py3-none-any.whl (9.4 kB)\n",
            "Downloading looker_sdk-25.10.0-py3-none-any.whl (236 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.1/236.1 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sqlalchemy_bigquery-1.15.0-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sqlalchemy_spanner-1.14.0-py3-none-any.whl (31 kB)\n",
            "Downloading types_protobuf-6.30.2.20250516-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading cattrs-25.1.1-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.4/69.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_appengine_logging-1.6.2-py3-none-any.whl (16 kB)\n",
            "Downloading google_cloud_audit_log-0.3.2-py3-none-any.whl (32 kB)\n",
            "Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.42.0-cp311-cp311-manylinux2014_x86_64.whl (67.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.4/67.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-69.5.1-py3-none-any.whl (894 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m894.6/894.6 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading virtualenv-20.31.2-py3-none-any.whl (6.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_cors-0.8.1-py3-none-any.whl (25 kB)\n",
            "Downloading colorful-0.5.6-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Installing collected packages: py-spy, opencensus-context, distlib, colorful, virtualenv, types-protobuf, setuptools, ruamel.yaml.clib, pyasn1-modules, grpcio-gcp, cattrs, backoff, ruamel.yaml, looker-sdk, google-cloud-audit-log, gcloud-aio-auth, aiohttp-cors, ray, opencensus, google-ads, gcloud-aio-storage, gcloud-aio-bigquery, google-cloud-workflows, google-cloud-vision, google-cloud-videointelligence, google-cloud-texttospeech, google-cloud-tasks, google-cloud-storage-transfer, google-cloud-speech, google-cloud-secret-manager, google-cloud-run, google-cloud-redis, google-cloud-pubsub, google-cloud-os-login, google-cloud-orchestration-airflow, google-cloud-monitoring, google-cloud-memcache, google-cloud-managedkafka, google-cloud-kms, google-cloud-dlp, google-cloud-dataproc-metastore, google-cloud-dataplex, google-cloud-dataform, google-cloud-dataflow-client, google-cloud-datacatalog, google-cloud-container, google-cloud-compute, google-cloud-build, google-cloud-bigtable, google-cloud-bigquery-datatransfer, google-cloud-batch, google-cloud-automl, google-cloud-appengine-logging, google-cloud-alloydb, google-analytics-admin, sqlalchemy-spanner, sqlalchemy-bigquery, google-cloud-logging, apache-airflow-providers-google\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: pyasn1-modules\n",
            "    Found existing installation: pyasn1_modules 0.4.2\n",
            "    Uninstalling pyasn1_modules-0.4.2:\n",
            "      Successfully uninstalled pyasn1_modules-0.4.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-cors-0.8.1 apache-airflow-providers-google-16.0.0 backoff-2.2.1 cattrs-25.1.1 colorful-0.5.6 distlib-0.3.9 gcloud-aio-auth-5.4.2 gcloud-aio-bigquery-7.1.0 gcloud-aio-storage-9.4.0 google-ads-27.0.0 google-analytics-admin-0.24.1 google-cloud-alloydb-0.4.8 google-cloud-appengine-logging-1.6.2 google-cloud-audit-log-0.3.2 google-cloud-automl-2.16.4 google-cloud-batch-0.17.36 google-cloud-bigquery-datatransfer-3.19.2 google-cloud-bigtable-2.31.0 google-cloud-build-3.31.2 google-cloud-compute-1.31.0 google-cloud-container-2.57.0 google-cloud-datacatalog-3.27.1 google-cloud-dataflow-client-0.9.0 google-cloud-dataform-0.6.2 google-cloud-dataplex-2.10.2 google-cloud-dataproc-metastore-1.18.3 google-cloud-dlp-3.31.0 google-cloud-kms-3.5.1 google-cloud-logging-3.12.1 google-cloud-managedkafka-0.1.11 google-cloud-memcache-1.12.2 google-cloud-monitoring-2.27.2 google-cloud-orchestration-airflow-1.17.5 google-cloud-os-login-2.17.2 google-cloud-pubsub-2.30.0 google-cloud-redis-2.18.1 google-cloud-run-0.10.18 google-cloud-secret-manager-2.24.0 google-cloud-speech-2.33.0 google-cloud-storage-transfer-1.17.0 google-cloud-tasks-2.19.3 google-cloud-texttospeech-2.27.0 google-cloud-videointelligence-2.16.2 google-cloud-vision-3.10.2 google-cloud-workflows-1.18.2 grpcio-gcp-0.2.2 looker-sdk-25.10.0 opencensus-0.11.4 opencensus-context-0.1.3 py-spy-0.4.0 pyasn1-modules-0.4.1 ray-2.42.0 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12 setuptools-69.5.1 sqlalchemy-bigquery-1.15.0 sqlalchemy-spanner-1.14.0 types-protobuf-6.30.2.20250516 virtualenv-20.31.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "airflow",
                  "google",
                  "pkg_resources"
                ]
              },
              "id": "ba24dc7750844272b4b76613cf76333f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d782a040"
      },
      "source": [
        "**Reasoning**:\n",
        "I have installed the necessary Google Cloud provider for Airflow. Now I can retry the previous code block which defines the Python function to load data to BigQuery and instantiates the corresponding PythonOperator task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "341bc0d9",
        "outputId": "34972256-e0e3-4f63-b8eb-1b31e5c7c463"
      },
      "source": [
        "from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator\n",
        "from google.cloud import bigquery\n",
        "import os\n",
        "import pandas as pd # Import pandas as it's used in the callable\n",
        "\n",
        "# Define BigQuery parameters (replace with your actual values)\n",
        "# Assuming 'credentials' and 'dag' objects are available from previous cells\n",
        "PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT', credentials.project_id) # Use the project ID from credentials\n",
        "DATASET_ID = \"sri_presion_fiscal\"\n",
        "TABLE_ID = \"sri_presion_fiscal\"\n",
        "# BIGQUERY_CONN_ID = 'google_cloud_default' # This is for operators that use connections, PythonOperator uses the client directly\n",
        "\n",
        "def load_data_to_bigquery_with_xcom(ti, project_id: str, dataset_id: str, table_id: str):\n",
        "    \"\"\"\n",
        "    Loads a Pandas DataFrame (pulled from XCom) into a Google BigQuery table.\n",
        "    Assumes GOOGLE_APPLICATION_CREDENTIALS environment variable is set or\n",
        "    Airflow connection handles authentication.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Pull the DataFrame from the previous task's XCom\n",
        "        # The key is the task_id of the upstream task\n",
        "        df = ti.xcom_pull(task_ids='transform_data')\n",
        "\n",
        "        if df is None:\n",
        "            raise ValueError(\"No DataFrame received from the transform_data task via XCom.\")\n",
        "\n",
        "        # Initialize BigQuery client\n",
        "        # The client will automatically use credentials from GOOGLE_APPLICATION_CREDENTIALS\n",
        "        # or from the Airflow connection if configured.\n",
        "        client = bigquery.Client(project=project_id)\n",
        "\n",
        "        table_ref = client.dataset(dataset_id).table(table_id)\n",
        "\n",
        "        # Define the schema based on the DataFrame columns and expected types\n",
        "        # This assumes the DataFrame columns are already in the correct order and types\n",
        "        # after the transformation step.\n",
        "        schema = [\n",
        "            bigquery.SchemaField(\"Region\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"País\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"Año\", \"INT64\"), # Use INT64 for pandas Int64 dtype\n",
        "            bigquery.SchemaField(\"%_Presion\", \"FLOAT64\"), # Use FLOAT64 for pandas float64 dtype\n",
        "            bigquery.SchemaField(\"Tipo_Presion\", \"STRING\"),\n",
        "        ]\n",
        "\n",
        "        # Configure the load job\n",
        "        job_config = bigquery.LoadJobConfig(\n",
        "            schema=schema,\n",
        "            write_disposition=\"WRITE_TRUNCATE\",  # Overwrite the table\n",
        "            # You might need to specify source_format if loading from a file,\n",
        "            # but when loading from DataFrame, BigQuery client handles this.\n",
        "        )\n",
        "\n",
        "        # Load data from DataFrame\n",
        "        job = client.load_table_from_dataframe(\n",
        "            df, table_ref, job_config=job_config\n",
        "        )\n",
        "\n",
        "        print(f\"Starting BigQuery load job: {job.job_id}\")\n",
        "        job.result()  # Wait for the job to complete\n",
        "\n",
        "        print(f\"DataFrame successfully loaded to {project_id}.{dataset_id}.{table_id}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data to BigQuery: {e}\")\n",
        "        # In a real Airflow DAG, you might want to raise an exception\n",
        "        raise\n",
        "\n",
        "\n",
        "# Instantiate the PythonOperator for the loading task\n",
        "# We need to pass the DataFrame from the transform_data_task to this task.\n",
        "# In Airflow, this is typically done by using XComs.\n",
        "# However, passing a large DataFrame via XComs is not the most efficient way for large datasets in Airflow.\n",
        "# A better approach would be to save the transformed data to a temporary file (e.g., Parquet or CSV) in GCS\n",
        "# in the transform_data_task and using a BigQuery load operator that reads from GCS.\n",
        "# For the purpose of this exercise, we are using XComs as per the instruction to pass the DataFrame.\n",
        "\n",
        "load_to_bigquery_task = PythonOperator(\n",
        "    task_id='load_to_bigquery',\n",
        "    python_callable=load_data_to_bigquery_with_xcom,\n",
        "    op_kwargs={\n",
        "        'project_id': PROJECT_ID,\n",
        "        'dataset_id': DATASET_ID,\n",
        "        'table_id': TABLE_ID,\n",
        "        # The DataFrame is passed implicitly via XComs and pulled by the callable function.\n",
        "    },\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "# Add the dependency: transform_data_task must complete before load_to_bigquery_task starts\n",
        "# This dependency also implies that the output of transform_data_task (the DataFrame)\n",
        "# will be available for load_to_bigquery_task via XComs.\n",
        "# This line should be placed after both tasks are defined.\n",
        "# transform_data_task >> load_to_bigquery_task # This dependency will be set in a later step.\n",
        "\n",
        "print(f\"Task '{load_to_bigquery_task.task_id}' defined for loading data to BigQuery.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-29T23:23:47.330+0000] {providers_manager.py:953} INFO - The hook_class 'airflow.providers.standard.hooks.filesystem.FSHook' is not fully initialized (UI widgets will be missing), because the 'flask_appbuilder' package is not installed, however it is not required for Airflow components to work\n",
            "[2025-06-29T23:23:47.334+0000] {providers_manager.py:953} INFO - The hook_class 'airflow.providers.standard.hooks.package_index.PackageIndexHook' is not fully initialized (UI widgets will be missing), because the 'flask_appbuilder' package is not installed, however it is not required for Airflow components to work\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'credentials' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-1747915344.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Define BigQuery parameters (replace with your actual values)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Assuming 'credentials' and 'dag' objects are available from previous cells\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mPROJECT_ID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GOOGLE_CLOUD_PROJECT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Use the project ID from credentials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mDATASET_ID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sri_presion_fiscal\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mTABLE_ID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sri_presion_fiscal\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'credentials' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e73a02f8"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block failed because the `credentials` object was not defined in the current scope. Although it was defined in a previous code block, the interactive notebook environment does not guarantee its persistence. I need to redefine the `credentials` object within this code block to ensure it's available when the `PROJECT_ID` is being defined. I will copy the relevant credential loading code from the earlier cells.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "WgPPC-i4AUIQ",
        "outputId": "37e71294-3e7d-4228-cacf-8e759a43965b"
      },
      "source": [
        "from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator\n",
        "from google.cloud import bigquery\n",
        "import os\n",
        "import pandas as pd # Import pandas as it's used in the callable\n",
        "from google.oauth2 import service_account # Import service_account\n",
        "\n",
        "# Load credentials again to ensure they are available in this scope\n",
        "# Assumes the GOOGLE_APPLICATION_CREDENTIALS environment variable is already set\n",
        "try:\n",
        "    credentials = service_account.Credentials.from_service_account_file(os.environ['GOOGLE_APPLICATION_CREDENTIALS'])\n",
        "    print(\"Google Cloud credentials loaded.\")\n",
        "except KeyError:\n",
        "    print(\"Error: GOOGLE_APPLICATION_CREDENTIALS environment variable not set.\")\n",
        "    # Handle this error appropriately in a real DAG, e.g., raise an exception\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred loading credentials: {e}\")\n",
        "    # Handle other potential exceptions\n",
        "\n",
        "\n",
        "# Define BigQuery parameters (replace with your actual values)\n",
        "# Assuming 'dag' object is available from previous cells\n",
        "PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT', credentials.project_id) # Use the project ID from credentials\n",
        "DATASET_ID = \"sri_presion_fiscal\"\n",
        "TABLE_ID = \"sri_presion_fiscal\"\n",
        "# BIGQUERY_CONN_ID = 'google_cloud_default' # This is for operators that use connections, PythonOperator uses the client directly\n",
        "\n",
        "def load_data_to_bigquery_with_xcom(ti, project_id: str, dataset_id: str, table_id: str):\n",
        "    \"\"\"\n",
        "    Loads a Pandas DataFrame (pulled from XCom) into a Google BigQuery table.\n",
        "    Assumes GOOGLE_APPLICATION_CREDENTIALS environment variable is set or\n",
        "    Airflow connection handles authentication.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Pull the DataFrame from the previous task's XCom\n",
        "        # The key is the task_id of the upstream task\n",
        "        df = ti.xcom_pull(task_ids='transform_data')\n",
        "\n",
        "        if df is None:\n",
        "            # If df is None, it might mean the previous task didn't push anything or failed.\n",
        "            # Check if the previous task ran successfully.\n",
        "            # For this notebook context, we'll just raise an error.\n",
        "             raise ValueError(\"No DataFrame received from the transform_data task via XCom. Check if 'transform_data' task ran and returned a DataFrame.\")\n",
        "\n",
        "\n",
        "        # Initialize BigQuery client\n",
        "        # The client will automatically use credentials from GOOGLE_APPLICATION_CREDENTIALS\n",
        "        # or from the Airflow connection if configured.\n",
        "        # Pass credentials explicitly to the client if needed, or rely on env var.\n",
        "        # Let's explicitly pass credentials for clarity, assuming they are loaded outside this function.\n",
        "        # However, in a standard Airflow environment, relying on the environment variable\n",
        "        # or the Airflow connection is more common for operators.\n",
        "        # Since this is a PythonOperator, let's keep it simple and rely on the environment variable\n",
        "        # which is set up in the Colab environment. The `bigquery.Client()` should pick it up.\n",
        "\n",
        "        client = bigquery.Client(project=project_id)\n",
        "\n",
        "        table_ref = client.dataset(dataset_id).table(table_id)\n",
        "\n",
        "        # Define the schema based on the DataFrame columns and expected types\n",
        "        # This assumes the DataFrame columns are already in the correct order and types\n",
        "        # after the transformation step.\n",
        "        schema = [\n",
        "            bigquery.SchemaField(\"Region\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"País\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"Año\", \"INT64\"), # Use INT64 for pandas Int64 dtype\n",
        "            bigquery.SchemaField(\"%_Presion\", \"FLOAT64\"), # Use FLOAT64 for pandas float64 dtype\n",
        "            bigquery.SchemaField(\"Tipo_Presion\", \"STRING\"),\n",
        "        ]\n",
        "\n",
        "        # Configure the load job\n",
        "        job_config = bigquery.LoadJobConfig(\n",
        "            schema=schema,\n",
        "            write_disposition=\"WRITE_TRUNCATE\",  # Overwrite the table\n",
        "            # You might need to specify source_format if loading from a file,\n",
        "            # but when loading from DataFrame, BigQuery client handles this.\n",
        "        )\n",
        "\n",
        "        # Load data from DataFrame\n",
        "        job = client.load_table_from_dataframe(\n",
        "            df, table_ref, job_config=job_config\n",
        "        )\n",
        "\n",
        "        print(f\"Starting BigQuery load job: {job.job_id}\")\n",
        "        job.result()  # Wait for the job to complete\n",
        "\n",
        "        print(f\"DataFrame successfully loaded to {project_id}.{dataset_id}.{table_id}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data to BigQuery: {e}\")\n",
        "        # In a real Airflow DAG, you might want to raise an exception\n",
        "        raise\n",
        "\n",
        "\n",
        "# Instantiate the PythonOperator for the loading task\n",
        "# We need to pass the DataFrame from the transform_data_task to this task.\n",
        "# In Airflow, this is typically done by using XComs.\n",
        "# However, passing a large DataFrame via XComs is not the most efficient way for large datasets in Airflow.\n",
        "# A better approach would be to save the transformed data to a temporary file (e.g., Parquet or CSV) in GCS\n",
        "# in the transform_data_task and using a BigQuery load operator that reads from GCS.\n",
        "# For the purpose of this exercise, we are using XComs as per the instruction to pass the DataFrame.\n",
        "\n",
        "load_to_bigquery_task = PythonOperator(\n",
        "    task_id='load_to_bigquery',\n",
        "    python_callable=load_data_to_bigquery_with_xcom,\n",
        "    op_kwargs={\n",
        "        'project_id': PROJECT_ID,\n",
        "        'dataset_id': DATASET_ID,\n",
        "        'table_id': TABLE_ID,\n",
        "        # The DataFrame is passed implicitly via XComs and pulled by the callable function.\n",
        "    },\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "# Add the dependency: transform_data_task must complete before load_to_bigquery_task starts\n",
        "# This dependency also implies that the output of transform_data_task (the DataFrame)\n",
        "# will be available for load_to_bigquery_task via XComs.\n",
        "# This line should be placed after both tasks are defined.\n",
        "# transform_data_task >> load_to_bigquery_task # This dependency will be set in a later step.\n",
        "\n",
        "print(f\"Task '{load_to_bigquery_task.task_id}' defined for loading data to BigQuery.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: GOOGLE_APPLICATION_CREDENTIALS environment variable not set.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'credentials' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-378415444.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Define BigQuery parameters (replace with your actual values)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Assuming 'dag' object is available from previous cells\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mPROJECT_ID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GOOGLE_CLOUD_PROJECT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Use the project ID from credentials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mDATASET_ID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sri_presion_fiscal\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mTABLE_ID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sri_presion_fiscal\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'credentials' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f951b802"
      },
      "source": [
        "**Reasoning**:\n",
        "The `credentials` object is still not defined, even after attempting to load it again. This is likely because the `try...except KeyError` block caught the error, printed a message, but did not stop execution or raise the exception, so the code proceeded to try and use `credentials.project_id` which was never assigned. I need to ensure that if the credentials cannot be loaded from the environment variable, the code stops or handles this error properly before attempting to use the `credentials` object. For this task, I will modify the credential loading to raise the `KeyError` if the environment variable is not set, which will stop the execution of the cell and prevent the subsequent `NameError`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "oPkDN4ROAW3t",
        "outputId": "671c5b60-0bc2-409c-e1d9-7403c631a682"
      },
      "source": [
        "from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator\n",
        "from google.cloud import bigquery\n",
        "import os\n",
        "import pandas as pd\n",
        "from google.oauth2 import service_account\n",
        "\n",
        "# Load credentials again, raising KeyError if the environment variable is not set\n",
        "try:\n",
        "    credentials = service_account.Credentials.from_service_account_file(os.environ['GOOGLE_APPLICATION_CREDENTIALS'])\n",
        "    print(\"Google Cloud credentials loaded.\")\n",
        "except KeyError:\n",
        "    # Raise the KeyError to stop execution if the environment variable is not set\n",
        "    raise KeyError(\"Error: GOOGLE_APPLICATION_CREDENTIALS environment variable not set. Please set it before running this cell.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred loading credentials: {e}\")\n",
        "    # In a real DAG, decide if this should stop the task or be handled differently\n",
        "    raise # Re-raise the exception to stop execution\n",
        "\n",
        "\n",
        "# Define BigQuery parameters (replace with your actual values)\n",
        "# Assuming 'dag' object is available from previous cells\n",
        "PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT', credentials.project_id) # Use the project ID from credentials\n",
        "DATASET_ID = \"sri_presion_fiscal\"\n",
        "TABLE_ID = \"sri_presion_fiscal\"\n",
        "# BIGQUERY_CONN_ID = 'google_cloud_default' # This is for operators that use connections, PythonOperator uses the client directly\n",
        "\n",
        "def load_data_to_bigquery_with_xcom(ti, project_id: str, dataset_id: str, table_id: str):\n",
        "    \"\"\"\n",
        "    Loads a Pandas DataFrame (pulled from XCom) into a Google BigQuery table.\n",
        "    Assumes GOOGLE_APPLICATION_CREDENTIALS environment variable is set or\n",
        "    Airflow connection handles authentication.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Pull the DataFrame from the previous task's XCom\n",
        "        # The key is the task_id of the upstream task\n",
        "        df = ti.xcom_pull(task_ids='transform_data')\n",
        "\n",
        "        if df is None:\n",
        "            # If df is None, it might mean the previous task didn't push anything or failed.\n",
        "            # Check if the previous task ran successfully.\n",
        "            # For this notebook context, we'll just raise an error.\n",
        "             raise ValueError(\"No DataFrame received from the transform_data task via XCom. Check if 'transform_data' task ran and returned a DataFrame.\")\n",
        "\n",
        "\n",
        "        # Initialize BigQuery client\n",
        "        # The client will automatically use credentials from GOOGLE_APPLICATION_CREDENTIALS\n",
        "        # or from the Airflow connection if configured.\n",
        "        # Pass credentials explicitly to the client if needed, or rely on env var.\n",
        "        # Let's explicitly pass credentials for clarity, assuming they are loaded outside this function.\n",
        "        # However, in a standard Airflow environment, relying on the environment variable\n",
        "        # or the Airflow connection is more common for operators.\n",
        "        # Since this is a PythonOperator, let's keep it simple and rely on the environment variable\n",
        "        # which is set up in the Colab environment. The `bigquery.Client()` should pick it up.\n",
        "\n",
        "        client = bigquery.Client(project=project_id)\n",
        "\n",
        "        table_ref = client.dataset(dataset_id).table(table_id)\n",
        "\n",
        "        # Define the schema based on the DataFrame columns and expected types\n",
        "        # This assumes the DataFrame columns are already in the correct order and types\n",
        "        # after the transformation step.\n",
        "        schema = [\n",
        "            bigquery.SchemaField(\"Region\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"País\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"Año\", \"INT64\"), # Use INT64 for pandas Int64 dtype\n",
        "            bigquery.SchemaField(\"%_Presion\", \"FLOAT64\"), # Use FLOAT64 for pandas float64 dtype\n",
        "            bigquery.SchemaField(\"Tipo_Presion\", \"STRING\"),\n",
        "        ]\n",
        "\n",
        "        # Configure the load job\n",
        "        job_config = bigquery.LoadJobConfig(\n",
        "            schema=schema,\n",
        "            write_disposition=\"WRITE_TRUNCATE\",  # Overwrite the table\n",
        "            # You might need to specify source_format if loading from a file,\n",
        "            # but when loading from DataFrame, BigQuery client handles this.\n",
        "        )\n",
        "\n",
        "        # Load data from DataFrame\n",
        "        job = client.load_table_from_dataframe(\n",
        "            df, table_ref, job_config=job_config\n",
        "        )\n",
        "\n",
        "        print(f\"Starting BigQuery load job: {job.job_id}\")\n",
        "        job.result()  # Wait for the job to complete\n",
        "\n",
        "        print(f\"DataFrame successfully loaded to {project_id}.{dataset_id}.{table_id}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data to BigQuery: {e}\")\n",
        "        # In a real Airflow DAG, you might want to raise an exception\n",
        "        raise\n",
        "\n",
        "\n",
        "# Instantiate the PythonOperator for the loading task\n",
        "# We need to pass the DataFrame from the transform_data_task to this task.\n",
        "# In Airflow, this is typically done by using XComs.\n",
        "# However, passing a large DataFrame via XComs is not the most efficient way for large datasets in Airflow.\n",
        "# A better approach would be to save the transformed data to a temporary file (e.g., Parquet or CSV) in GCS\n",
        "# in the transform_data_task and using a BigQuery load operator that reads from GCS.\n",
        "# For the purpose of this exercise, we are using XComs as per the instruction to pass the DataFrame.\n",
        "\n",
        "load_to_bigquery_task = PythonOperator(\n",
        "    task_id='load_to_bigquery',\n",
        "    python_callable=load_data_to_bigquery_with_xcom,\n",
        "    op_kwargs={\n",
        "        'project_id': PROJECT_ID,\n",
        "        'dataset_id': DATASET_ID,\n",
        "        'table_id': TABLE_ID,\n",
        "        # The DataFrame is passed implicitly via XComs and pulled by the callable function.\n",
        "    },\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "# Add the dependency: transform_data_task must complete before load_to_bigquery_task starts\n",
        "# This dependency also implies that the output of transform_data_task (the DataFrame)\n",
        "# will be available for load_to_bigquery_task via XComs.\n",
        "# This line should be placed after both tasks are defined.\n",
        "# transform_data_task >> load_to_bigquery_task # This dependency will be set in a later step.\n",
        "\n",
        "print(f\"Task '{load_to_bigquery_task.task_id}' defined for loading data to BigQuery.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Error: GOOGLE_APPLICATION_CREDENTIALS environment variable not set. Please set it before running this cell.'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-3477569284.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcredentials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mservice_account\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCredentials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_service_account_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'GOOGLE_APPLICATION_CREDENTIALS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Google Cloud credentials loaded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'GOOGLE_APPLICATION_CREDENTIALS'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-3477569284.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Raise the KeyError to stop execution if the environment variable is not set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error: GOOGLE_APPLICATION_CREDENTIALS environment variable not set. Please set it before running this cell.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"An unexpected error occurred loading credentials: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Error: GOOGLE_APPLICATION_CREDENTIALS environment variable not set. Please set it before running this cell.'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17af3dc4"
      },
      "source": [
        "**Reasoning**:\n",
        "The execution failed because the `GOOGLE_APPLICATION_CREDENTIALS` environment variable is not set in this environment, as indicated by the `KeyError`. This variable is necessary for authenticating with Google Cloud services like BigQuery. I need to explicitly set this environment variable in the Colab environment before attempting to load the credentials or use BigQuery. The credential file was uploaded earlier and the environment variable was set in a previous cell (`cell_id: 33ac978c`), but it seems the environment variable did not persist or is not accessible in this new execution context. I will add a step to set the environment variable using the known filename of the uploaded credentials file. After setting the environment variable, I will re-execute the code to define the BigQuery loading task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15NxjdDlAZt-",
        "outputId": "5ffb0ab9-876b-4e9d-e829-ecc753933706"
      },
      "source": [
        "from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator\n",
        "from google.cloud import bigquery\n",
        "import os\n",
        "import pandas as pd\n",
        "from google.oauth2 import service_account\n",
        "\n",
        "# Set the GOOGLE_APPLICATION_CREDENTIALS environment variable\n",
        "# Assumes the file \"etl-sistemas-sri-819ded2ef88d.json\" was uploaded in a previous step\n",
        "# Replace with the actual filename if it was different\n",
        "credential_filename = \"etl-sistemas-sri-819ded2ef88d.json\"\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_filename\n",
        "print(f\"GOOGLE_APPLICATION_CREDENTIALS environment variable set to: {credential_filename}\")\n",
        "\n",
        "\n",
        "# Load credentials using the environment variable\n",
        "try:\n",
        "    credentials = service_account.Credentials.from_service_account_file(os.environ['GOOGLE_APPLICATION_CREDENTIALS'])\n",
        "    print(\"Google Cloud credentials loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Credential file '{credential_filename}' not found. Please upload it again.\")\n",
        "    raise # Re-raise the exception if the file is not found\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred loading credentials: {e}\")\n",
        "    raise # Re-raise any other exception\n",
        "\n",
        "\n",
        "# Define BigQuery parameters (replace with your actual values)\n",
        "# Assuming 'dag' object is available from previous cells\n",
        "PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT', credentials.project_id) # Use the project ID from credentials\n",
        "DATASET_ID = \"sri_presion_fiscal\"\n",
        "TABLE_ID = \"sri_presion_fiscal\"\n",
        "# BIGQUERY_CONN_ID is not needed for the PythonOperator using the client directly\n",
        "\n",
        "def load_data_to_bigquery_with_xcom(ti, project_id: str, dataset_id: str, table_id: str):\n",
        "    \"\"\"\n",
        "    Loads a Pandas DataFrame (pulled from XCom) into a Google BigQuery table.\n",
        "    Assumes GOOGLE_APPLICATION_CREDENTIALS environment variable is set or\n",
        "    Airflow connection handles authentication.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Pull the DataFrame from the previous task's XCom\n",
        "        # The key is the task_id of the upstream task\n",
        "        df = ti.xcom_pull(task_ids='transform_data')\n",
        "\n",
        "        if df is None:\n",
        "            # If df is None, it might mean the previous task didn't push anything or failed.\n",
        "            # Check if the previous task ran successfully.\n",
        "            # For this notebook context, we'll just raise an error.\n",
        "             raise ValueError(\"No DataFrame received from the transform_data task via XCom. Check if 'transform_data' task ran and returned a DataFrame.\")\n",
        "\n",
        "\n",
        "        # Initialize BigQuery client\n",
        "        # The client will automatically use credentials from GOOGLE_APPLICATION_CREDENTIALS\n",
        "        # or from the Airflow connection if configured.\n",
        "        client = bigquery.Client(project=project_id)\n",
        "\n",
        "        table_ref = client.dataset(dataset_id).table(table_id)\n",
        "\n",
        "        # Define the schema based on the DataFrame columns and expected types\n",
        "        # This assumes the DataFrame columns are already in the correct order and types\n",
        "        # after the transformation step.\n",
        "        schema = [\n",
        "            bigquery.SchemaField(\"Region\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"País\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"Año\", \"INT64\"), # Use INT64 for pandas Int64 dtype\n",
        "            bigquery.SchemaField(\"%_Presion\", \"FLOAT64\"), # Use FLOAT64 for pandas float64 dtype\n",
        "            bigquery.SchemaField(\"Tipo_Presion\", \"STRING\"),\n",
        "        ]\n",
        "\n",
        "        # Configure the load job\n",
        "        job_config = bigquery.LoadJobConfig(\n",
        "            schema=schema,\n",
        "            write_disposition=\"WRITE_TRUNCATE\",  # Overwrite the table\n",
        "            # You might need to specify source_format if loading from a file,\n",
        "            # but when loading from DataFrame, BigQuery client handles this.\n",
        "        )\n",
        "\n",
        "        # Load data from DataFrame\n",
        "        job = client.load_table_from_dataframe(\n",
        "            df, table_ref, job_config=job_config\n",
        "        )\n",
        "\n",
        "        print(f\"Starting BigQuery load job: {job.job_id}\")\n",
        "        job.result()  # Wait for the job to complete\n",
        "\n",
        "        print(f\"DataFrame successfully loaded to {project_id}.{dataset_id}.{table_id}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data to BigQuery: {e}\")\n",
        "        # In a real Airflow DAG, you might want to raise an exception\n",
        "        raise\n",
        "\n",
        "\n",
        "# Instantiate the PythonOperator for the loading task\n",
        "# We are using XComs to pass the DataFrame from the transform task.\n",
        "\n",
        "load_to_bigquery_task = PythonOperator(\n",
        "    task_id='load_to_bigquery',\n",
        "    python_callable=load_data_to_bigquery_with_xcom,\n",
        "    op_kwargs={\n",
        "        'project_id': PROJECT_ID,\n",
        "        'dataset_id': DATASET_ID,\n",
        "        'table_id': TABLE_ID,\n",
        "        # The DataFrame is passed implicitly via XComs and pulled by the callable function.\n",
        "    },\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "# Add the dependency: transform_data_task must complete before load_to_bigquery_task starts\n",
        "# This dependency also implies that the output of transform_data_task (the DataFrame)\n",
        "# will be available for load_to_bigquery_task via XComs.\n",
        "# This line should be placed after both tasks are defined.\n",
        "# transform_data_task >> load_to_bigquery_task # This dependency will be set in a later step.\n",
        "\n",
        "print(f\"Task '{load_to_bigquery_task.task_id}' defined for loading data to BigQuery.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GOOGLE_APPLICATION_CREDENTIALS environment variable set to: etl-sistemas-sri-819ded2ef88d.json\n",
            "Google Cloud credentials loaded successfully.\n",
            "Task 'load_to_bigquery' defined for loading data to BigQuery.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3331345f"
      },
      "source": [
        "## Definir dependencias\n",
        "\n",
        "### Subtask:\n",
        "Establecer el orden de ejecución de las tareas (descargar -> transformar -> cargar).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f4223d2"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the task dependencies using the bitshift operator to establish the execution order: download, then transform, then load.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "c04df5fb",
        "outputId": "64b1df9a-a0e8-4d3b-848d-14e0eb71e24a"
      },
      "source": [
        "# Set the order of execution for the tasks\n",
        "download_csv_task >> transform_data_task >> load_to_bigquery_task\n",
        "\n",
        "print(\"\\nTask dependencies set:\")\n",
        "print(f\"'{download_csv_task.task_id}' >> '{transform_data_task.task_id}' >> '{load_to_bigquery_task.task_id}'\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'download_csv_task' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-506067998.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Set the order of execution for the tasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdownload_csv_task\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mtransform_data_task\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mload_to_bigquery_task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTask dependencies set:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{download_csv_task.task_id}' >> '{transform_data_task.task_id}' >> '{load_to_bigquery_task.task_id}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'download_csv_task' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a7694e1"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates that the task variables (`download_csv_task`, `transform_data_task`, `load_to_bigquery_task`) are not defined in the current scope. This is due to the nature of the notebook environment where variables defined in previous cells might not be accessible directly. To fix this, I need to redefine the tasks within the current cell so that their variables are available when setting the dependencies. I will copy the task definitions from the previous successful steps and then define the dependencies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcryYdQMAgHQ",
        "outputId": "10d3e749-7158-48c6-8710-b93e418dd6e8"
      },
      "source": [
        "from airflow import DAG\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "from airflow.providers.standard.operators.python import PythonOperator\n",
        "from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator\n",
        "from google.cloud import bigquery\n",
        "import os\n",
        "from google.oauth2 import service_account\n",
        "\n",
        "# Redefine default arguments for the DAG\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2023, 1, 1),\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "# Redefine the DAG\n",
        "dag = DAG(\n",
        "    'etl_presion_fiscal_sri',\n",
        "    default_args=default_args,\n",
        "    description='ETL pipeline to download, transform, and load SRI tax pressure data to BigQuery',\n",
        "    schedule=None,\n",
        "    catchup=False,\n",
        "    tags=['etl', 'bigquery', 'sri'],\n",
        ")\n",
        "\n",
        "# Redefine the local filename\n",
        "local_filename = \"SRI_Presion_Fiscal.csv\"\n",
        "credential_filename = \"etl-sistemas-sri-819ded2ef88d.json\" # Replace with your actual filename\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_filename\n",
        "\n",
        "# Redefine the download function and task\n",
        "def download_csv_from_url(csv_url, local_filename):\n",
        "    \"\"\"Downloads a CSV file from a given URL and saves it locally.\"\"\"\n",
        "    import requests # Import inside the function for Airflow execution context\n",
        "    try:\n",
        "        response = requests.get(csv_url)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        with open(local_filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        print(f\"Archivo '{local_filename}' descargado exitosamente.\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error al descargar el archivo: {e}\")\n",
        "        raise\n",
        "\n",
        "csv_url = \"https://www.sri.gob.ec/o/sri-portlet-biblioteca-alfresco-internet/descargar/7e45627e-1f7e-4e21-ae59-d520634fc63f/SRI_Presion_Fiscal.csv\"\n",
        "download_csv_task = PythonOperator(\n",
        "    task_id='download_csv',\n",
        "    python_callable=download_csv_from_url,\n",
        "    op_kwargs={'csv_url': csv_url, 'local_filename': local_filename},\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "# Redefine the transform function and task\n",
        "def transform_data(local_filename):\n",
        "    \"\"\"\n",
        "    Loads data from a CSV, cleans and transforms it, and returns a DataFrame.\n",
        "    \"\"\"\n",
        "    import pandas as pd # Import inside the function for Airflow execution context\n",
        "    try:\n",
        "        df = pd.read_csv(local_filename, encoding='latin-1', header=None, sep=';')\n",
        "        df = df.iloc[1:].copy()\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "        df.columns = ['Region', 'País', 'Año', '%_Presion', 'Tipo_Presion']\n",
        "        df['%_Presion'] = df['%_Presion'].astype(str).str.strip()\n",
        "        df['Año'] = pd.to_numeric(df['Año'], errors='coerce').astype('Int64')\n",
        "        df['%_Presion'] = df['%_Presion'].astype(str).str.replace(',', '.', regex=False)\n",
        "        df['%_Presion'] = pd.to_numeric(df['%_Presion'], errors='coerce')\n",
        "        print(\"Data transformation completed successfully.\")\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{local_filename}' was not found during transformation.\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during transformation: {e}\")\n",
        "        raise\n",
        "\n",
        "transform_data_task = PythonOperator(\n",
        "    task_id='transform_data',\n",
        "    python_callable=transform_data,\n",
        "    op_kwargs={'local_filename': local_filename},\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "# Redefine the load function and task\n",
        "def load_data_to_bigquery_with_xcom(ti, project_id: str, dataset_id: str, table_id: str):\n",
        "    \"\"\"\n",
        "    Loads a Pandas DataFrame (pulled from XCom) into a Google BigQuery table.\n",
        "    \"\"\"\n",
        "    from google.cloud import bigquery # Import inside the function\n",
        "    from google.oauth2 import service_account # Import inside the function\n",
        "    import os # Import inside the function\n",
        "    import pandas as pd # Import inside the function\n",
        "\n",
        "    try:\n",
        "        # Ensure credentials are loaded within the task execution context\n",
        "        # This might be redundant if using Airflow connections, but safer in this context\n",
        "        # Assumes GOOGLE_APPLICATION_CREDENTIALS env var is set\n",
        "        credentials = service_account.Credentials.from_service_account_file(os.environ['GOOGLE_APPLICATION_CREDENTIALS'])\n",
        "\n",
        "        df = ti.xcom_pull(task_ids='transform_data')\n",
        "        if df is None:\n",
        "             raise ValueError(\"No DataFrame received from the transform_data task via XCom.\")\n",
        "\n",
        "        client = bigquery.Client(project=project_id, credentials=credentials)\n",
        "        table_ref = client.dataset(dataset_id).table(table_id)\n",
        "\n",
        "        schema = [\n",
        "            bigquery.SchemaField(\"Region\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"País\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"Año\", \"INT64\"),\n",
        "            bigquery.SchemaField(\"%_Presion\", \"FLOAT64\"),\n",
        "            bigquery.SchemaField(\"Tipo_Presion\", \"STRING\"),\n",
        "        ]\n",
        "\n",
        "        job_config = bigquery.LoadJobConfig(\n",
        "            schema=schema,\n",
        "            write_disposition=\"WRITE_TRUNCATE\",\n",
        "        )\n",
        "\n",
        "        job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
        "        print(f\"Starting BigQuery load job: {job.job_id}\")\n",
        "        job.result()\n",
        "\n",
        "        print(f\"DataFrame successfully loaded to {project_id}.{dataset_id}.{table_id}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data to BigQuery: {e}\")\n",
        "        raise\n",
        "\n",
        "PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT', None) # Get project ID from env or set later\n",
        "if PROJECT_ID is None:\n",
        "    # Attempt to get project ID from credentials if env var is not set\n",
        "    try:\n",
        "        credentials = service_account.Credentials.from_service_account_file(os.environ['GOOGLE_APPLICATION_CREDENTIALS'])\n",
        "        PROJECT_ID = credentials.project_id\n",
        "    except Exception as e:\n",
        "        print(f\"Could not determine Project ID: {e}\")\n",
        "        # Handle error - maybe raise or set a default\n",
        "        PROJECT_ID = \"your-gcp-project-id\" # Replace with your actual project ID or handle error\n",
        "\n",
        "DATASET_ID = \"sri_presion_fiscal\"\n",
        "TABLE_ID = \"sri_presion_fiscal\"\n",
        "\n",
        "load_to_bigquery_task = PythonOperator(\n",
        "    task_id='load_to_bigquery',\n",
        "    python_callable=load_data_to_bigquery_with_xcom,\n",
        "    op_kwargs={\n",
        "        'project_id': PROJECT_ID,\n",
        "        'dataset_id': DATASET_ID,\n",
        "        'table_id': TABLE_ID,\n",
        "    },\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "# Set the order of execution for the tasks\n",
        "download_csv_task >> transform_data_task >> load_to_bigquery_task\n",
        "\n",
        "print(\"Task dependencies set:\")\n",
        "print(f\"'{download_csv_task.task_id}' >> '{transform_data_task.task_id}' >> '{load_to_bigquery_task.task_id}'\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task dependencies set:\n",
            "'download_csv' >> 'transform_data' >> 'load_to_bigquery'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c8c3035"
      },
      "source": [
        "## Añadir comentarios y documentación\n",
        "\n",
        "### Subtask:\n",
        "Incluir comentarios relevantes en el código del DAG para explicar cada parte (imports, default_args, definición del DAG, funciones de tareas, operadores, dependencias) y añadir una descripción general del DAG.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "824af9f1"
      },
      "source": [
        "**Reasoning**:\n",
        "Add comments to the Python code explaining the purpose of each part of the DAG script as requested by the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "id": "d2875122",
        "outputId": "834343fb-847e-4db4-ff78-d7de6273a8f3"
      },
      "source": [
        "# Import necessary modules from Apache Airflow and other libraries\n",
        "from airflow import DAG # DAG object to define the workflow\n",
        "from datetime import datetime, timedelta # For specifying start_date and retry_delay\n",
        "import pandas as pd # For data transformation using DataFrames\n",
        "from airflow.operators.python import PythonOperator # Operator to run Python functions\n",
        "# from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator # While used before, we are now using PythonOperator for loading\n",
        "from google.cloud import bigquery # For interacting with Google BigQuery API\n",
        "import os # For accessing environment variables\n",
        "from google.oauth2 import service_account # For loading Google Cloud service account credentials\n",
        "\n",
        "# Define default arguments for the DAG\n",
        "# These arguments are passed to all tasks unless overridden\n",
        "default_args = {\n",
        "    'owner': 'airflow', # The owner of the DAG\n",
        "    'depends_on_past': False, # Set to true if you want tasks to wait for the previous run to succeed\n",
        "    'start_date': datetime(2023, 1, 1), # The date when the DAG starts being scheduled\n",
        "    'email_on_failure': False, # Set to true to receive emails on task failure\n",
        "    'email_on_retry': False, # Set to true to receive emails on task retry\n",
        "    'retries': 1, # Number of times to retry a task on failure\n",
        "    'retry_delay': timedelta(minutes=5), # Time to wait between retries\n",
        "}\n",
        "\n",
        "# Define the DAG object\n",
        "# 'etl_presion_fiscal_sri' is the unique DAG ID\n",
        "dag = DAG(\n",
        "    'etl_presion_fiscal_sri',\n",
        "    default_args=default_args, # Apply the default arguments\n",
        "    description='ETL pipeline to download, transform, and load SRI tax pressure data to BigQuery', # A brief description of the DAG\n",
        "    schedule=None, # Set the schedule interval using a cron expression or timedelta, or None for manual runs\n",
        "    catchup=False, # Set to true to run the DAG for past dates starting from the start_date\n",
        "    tags=['etl', 'bigquery', 'sri'], # Tags to help categorize and filter DAGs in the Airflow UI\n",
        ")\n",
        "\n",
        "# Define the local filename for the downloaded CSV\n",
        "local_filename = \"SRI_Presion_Fiscal.csv\"\n",
        "\n",
        "# Define the filename for the Google Cloud service account credentials\n",
        "credential_filename = \"etl-sistemas-sri-819ded2ef88d.json\" # !!! Replace with your actual filename !!!\n",
        "# Set the environment variable for Google Cloud authentication\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_filename\n",
        "\n",
        "\n",
        "# Define the Python function for the download task\n",
        "def download_csv_from_url(csv_url, local_filename):\n",
        "    \"\"\"Downloads a CSV file from a given URL and saves it locally.\"\"\"\n",
        "    import requests # Import inside the function for Airflow execution context isolation\n",
        "    print(f\"Attempting to download file from: {csv_url}\")\n",
        "    try:\n",
        "        response = requests.get(csv_url)\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "        with open(local_filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        print(f\"Archivo '{local_filename}' descargado exitosamente.\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error al descargar el archivo: {e}\")\n",
        "        # Raise an exception to fail the task in Airflow\n",
        "        raise ValueError(f\"Download task failed: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during download: {e}\")\n",
        "        raise ValueError(f\"Download task failed unexpectedly: {e}\")\n",
        "\n",
        "\n",
        "# Instantiate the PythonOperator for the download task\n",
        "# Executes the 'download_csv_from_url' function\n",
        "download_csv_task = PythonOperator(\n",
        "    task_id='download_csv', # Unique identifier for the task within the DAG\n",
        "    python_callable=download_csv_from_url, # The Python function to execute\n",
        "    op_kwargs={'csv_url': csv_url, 'local_filename': local_filename}, # Keyword arguments to pass to the function\n",
        "    dag=dag, # Associate the task with the DAG object\n",
        ")\n",
        "\n",
        "\n",
        "# Define the Python function for the data transformation task\n",
        "def transform_data(local_filename):\n",
        "    \"\"\"\n",
        "    Loads data from a CSV, cleans and transforms it, and returns a DataFrame.\n",
        "    This function replicates the successful transformation steps from the notebook.\n",
        "    \"\"\"\n",
        "    import pandas as pd # Import inside the function for Airflow execution context isolation\n",
        "    print(f\"Attempting to transform data from: {local_filename}\")\n",
        "    try:\n",
        "        # Reload the CSV assuming semicolon as a separator and no header\n",
        "        df = pd.read_csv(local_filename, encoding='latin-1', header=None, sep=';')\n",
        "\n",
        "        # Remove the header row which is present as the first data row\n",
        "        df = df.iloc[1:].copy()\n",
        "\n",
        "        # Reset index after removing the row\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        # Rename columns to meaningful names\n",
        "        df.columns = ['Region', 'País', 'Año', '%_Presion', 'Tipo_Presion']\n",
        "\n",
        "        # Clean up the '%_Presion' column by removing leading/trailing spaces\n",
        "        df['%_Presion'] = df['%_Presion'].astype(str).str.strip()\n",
        "\n",
        "        # Convert data types\n",
        "        # Convert 'Año' to integer, coercing errors to NaN and then to Int64 (allows for NaNs)\n",
        "        df['Año'] = pd.to_numeric(df['Año'], errors='coerce').astype('Int64')\n",
        "\n",
        "        # Convert '%_Presion' to float, replacing comma with dot and coercing errors to NaN\n",
        "        df['%_Presion'] = df['%_Presion'].astype(str).str.replace(',', '.', regex=False)\n",
        "        df['%_Presion'] = pd.to_numeric(df['%_Presion'], errors='coerce')\n",
        "\n",
        "        print(\"Data transformation completed successfully.\")\n",
        "        print(\"Transformed DataFrame head (first 5 rows):\")\n",
        "        # In a real Airflow task, avoid printing large DataFrames directly\n",
        "        # display(df.head()) # display() is for notebooks, not standard Python scripts\n",
        "        print(df.head().to_string()) # Use to_string() for printing in a script\n",
        "\n",
        "        print(\"\\nTransformed DataFrame info:\")\n",
        "        # display(df.info()) # display() is for notebooks\n",
        "        df.info() # info() prints to console in a script\n",
        "\n",
        "        # Return the transformed DataFrame so it can be pushed to XCom\n",
        "        return df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{local_filename}' was not found during transformation.\")\n",
        "        raise ValueError(f\"Transform task failed: Input file not found: {local_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during transformation: {e}\")\n",
        "        raise ValueError(f\"Transform task failed unexpectedly: {e}\")\n",
        "\n",
        "\n",
        "# Instantiate the PythonOperator for the transformation task\n",
        "# Executes the 'transform_data' function\n",
        "# XCom is enabled by default for PythonOperator, allowing the returned DataFrame to be pushed\n",
        "transform_data_task = PythonOperator(\n",
        "    task_id='transform_data', # Unique identifier\n",
        "    python_callable=transform_data, # The function to execute\n",
        "    op_kwargs={'local_filename': local_filename}, # Pass the filename\n",
        "    dag=dag, # Associate with the DAG\n",
        ")\n",
        "\n",
        "\n",
        "# Define the Python function for the BigQuery loading task\n",
        "# This function receives the DataFrame from XCom\n",
        "def load_data_to_bigquery_with_xcom(ti, project_id: str, dataset_id: str, table_id: str):\n",
        "    \"\"\"\n",
        "    Loads a Pandas DataFrame (pulled from XCom) into a Google BigQuery table.\n",
        "    Assumes GOOGLE_APPLICATION_CREDENTIALS environment variable is set or\n",
        "    Airflow connection handles authentication.\n",
        "    \"\"\"\n",
        "    # Import inside the function for Airflow execution context isolation\n",
        "    from google.cloud import bigquery\n",
        "    from google.oauth2 import service_account\n",
        "    import os\n",
        "    import pandas as pd\n",
        "\n",
        "    print(f\"Attempting to load data to BigQuery table: {project_id}.{dataset_id}.{table_id}\")\n",
        "\n",
        "    try:\n",
        "        # Pull the DataFrame from the previous task's XCom\n",
        "        # 'ti' (TaskInstance) is automatically passed to the python_callable\n",
        "        df = ti.xcom_pull(task_ids='transform_data')\n",
        "\n",
        "        if df is None:\n",
        "             # If df is None, the previous task might not have returned a DataFrame or failed\n",
        "             raise ValueError(\"No DataFrame received from the 'transform_data' task via XCom. Check the upstream task.\")\n",
        "\n",
        "        print(f\"Successfully pulled DataFrame from XCom with {len(df)} rows.\")\n",
        "\n",
        "        # Initialize BigQuery client\n",
        "        # The client will automatically use credentials from GOOGLE_APPLICATION_CREDENTIALS\n",
        "        # or from the Airflow connection if configured.\n",
        "        client = bigquery.Client(project=project_id)\n",
        "\n",
        "        # Define the target table reference\n",
        "        table_ref = client.dataset(dataset_id).table(table_id)\n",
        "\n",
        "        # Define the schema for the BigQuery table\n",
        "        # Ensure schema matches the DataFrame structure and data types\n",
        "        schema = [\n",
        "            bigquery.SchemaField(\"Region\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"País\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"Año\", \"INT64\"), # BigQuery type for pandas Int64\n",
        "            bigquery.SchemaField(\"%_Presion\", \"FLOAT64\"), # BigQuery type for pandas float64\n",
        "            bigquery.SchemaField(\"Tipo_Presion\", \"STRING\"),\n",
        "        ]\n",
        "\n",
        "        # Configure the load job\n",
        "        job_config = bigquery.LoadJobConfig(\n",
        "            schema=schema, # Specify the schema\n",
        "            write_disposition=\"WRITE_TRUNCATE\",  # Overwrite the table if it exists\n",
        "            # source_format is not needed when loading from DataFrame\n",
        "        )\n",
        "\n",
        "        # Check if the dataset exists, create if not (optional, can also be a separate task)\n",
        "        try:\n",
        "            client.get_dataset(dataset_id)\n",
        "            print(f\"Dataset '{dataset_id}' already exists.\")\n",
        "        except Exception:\n",
        "            print(f\"Dataset '{dataset_id}' not found, creating...\")\n",
        "            dataset = bigquery.Dataset(client.dataset(dataset_id))\n",
        "            dataset.location = \"US\" # Set location\n",
        "            client.create_dataset(dataset, exists_ok=True)\n",
        "            print(f\"Dataset '{dataset_id}' created.\")\n",
        "\n",
        "\n",
        "        # Check if the table exists, create if not (optional, WRITE_TRUNCATE handles existence)\n",
        "        # If using WRITE_TRUNCATE, the table will be created if it doesn't exist,\n",
        "        # but defining schema explicitly requires the table to exist or be created.\n",
        "        # Let's ensure the table exists or is created with the correct schema.\n",
        "        # Using client.create_table with exists_ok=True is a safe way.\n",
        "        table = bigquery.Table(table_ref, schema=schema)\n",
        "        try:\n",
        "             client.create_table(table, exists_ok=True)\n",
        "             print(f\"Table '{table_id}' created or already exists.\")\n",
        "        except Exception as e:\n",
        "             print(f\"Error ensuring table '{table_id}' exists: {e}\")\n",
        "             # Depending on error, might need to raise or retry\n",
        "\n",
        "        # Load data from DataFrame to BigQuery\n",
        "        # This makes the API request to load the data\n",
        "        job = client.load_table_from_dataframe(\n",
        "            df, table_ref, job_config=job_config\n",
        "        )\n",
        "\n",
        "        print(f\"Starting BigQuery load job: {job.job_id}\")\n",
        "        job.result()  # Wait for the job to complete\n",
        "\n",
        "        print(f\"DataFrame successfully loaded to {project_id}.{dataset_id}.{table_id}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data to BigQuery: {e}\")\n",
        "        # Raise an exception to fail the task in Airflow\n",
        "        raise ValueError(f\"BigQuery load task failed: {e}\")\n",
        "\n",
        "\n",
        "# Define BigQuery parameters for the load task\n",
        "# Get Project ID from the environment variable GOOGLE_CLOUD_PROJECT, falling back to credentials\n",
        "PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT', None)\n",
        "if PROJECT_ID is None:\n",
        "    try:\n",
        "        credentials_check = service_account.Credentials.from_service_account_file(os.environ['GOOGLE_APPLICATION_CREDENTIALS'])\n",
        "        PROJECT_ID = credentials_check.project_id\n",
        "    except Exception as e:\n",
        "        print(f\"Could not automatically determine Google Cloud Project ID. Please set GOOGLE_CLOUD_PROJECT environment variable or ensure GOOGLE_APPLICATION_CREDENTIALS is correctly set: {e}\")\n",
        "        # Fallback or set a default - In a production DAG, this should likely fail or use an Airflow connection\n",
        "        PROJECT_ID = \"your-gcp-project-id\" # !!! REPLACE WITH YOUR ACTUAL GCP PROJECT ID !!!\n",
        "        print(f\"Using fallback Project ID: {PROJECT_ID}\")\n",
        "\n",
        "\n",
        "DATASET_ID = \"sri_presion_fiscal\" # BigQuery Dataset ID\n",
        "TABLE_ID = \"sri_presion_fiscal\" # BigQuery Table ID\n",
        "\n",
        "\n",
        "# Instantiate the PythonOperator for the BigQuery loading task\n",
        "# Executes the 'load_data_to_bigquery_with_xcom' function\n",
        "# This task implicitly pulls the DataFrame from the upstream task's XCom\n",
        "load_to_bigquery_task = PythonOperator(\n",
        "    task_id='load_to_bigquery', # Unique identifier\n",
        "    python_callable=load_data_to_bigquery_with_xcom, # The function to execute\n",
        "    op_kwargs={\n",
        "        'project_id': PROJECT_ID, # Pass project ID\n",
        "        'dataset_id': DATASET_ID, # Pass dataset ID\n",
        "        'table_id': TABLE_ID, # Pass table ID\n",
        "        # The DataFrame is passed via XComs, no need to pass it here\n",
        "    },\n",
        "    dag=dag, # Associate with the DAG\n",
        ")\n",
        "\n",
        "# Set the order of execution for the tasks using the bitshift operator\n",
        "# This defines the dependency: download_csv_task runs first, then transform_data_task, then load_to_bigquery_task\n",
        "download_csv_task >> transform_data_task >> load_to_bigquery_task\n",
        "\n",
        "# Print a confirmation of the task dependencies (for debugging/verification in a script)\n",
        "print(\"Task dependencies set:\")\n",
        "print(f\"'{download_csv_task.task_id}' >> '{transform_data_task.task_id}' >> '{load_to_bigquery_task.task_id}'\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m/tmp/\u001b[0m\u001b[1;33mipython-input-12-3930566489.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m5\u001b[0m\u001b[1;33m DeprecationWarning\u001b[0m\u001b[33m: The `airflow.operators.python.PythonOperator` class is deprecated. Please use `\u001b[0m\u001b[33m'airflow.providers.standard.operators.python.PythonOperator'\u001b[0m\u001b[33m`.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/tmp/ipython-input-12-3930566489.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">5</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> DeprecationWarning</span><span style=\"color: #808000; text-decoration-color: #808000\">: The `airflow.operators.python.PythonOperator` class is deprecated. Please use `</span><span style=\"color: #808000; text-decoration-color: #808000\">'airflow.providers.standard.operators.python.PythonOperator'</span><span style=\"color: #808000; text-decoration-color: #808000\">`.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task dependencies set:\n",
            "'download_csv' >> 'transform_data' >> 'load_to_bigquery'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c7b6f8f"
      },
      "source": [
        "## Generar el archivo .py del dag\n",
        "\n",
        "### Subtask:\n",
        "Empaquetar el código completo del DAG, incluyendo imports, argumentos por defecto, definición del DAG, funciones de tareas, operadores y dependencias, en un archivo Python (`.py`) listo para ser colocado en la carpeta `dags` de Airflow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d65bcd53"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to consolidate all the Python code from the previous steps into a single file (`etl_presion_fiscal_dag.py`), ensuring all necessary imports, definitions, tasks, and dependencies are included, and remove any Colab-specific code. I will use the `%%writefile` magic command to create the `.py` file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6ce4e20",
        "outputId": "b3f150fd-ea58-46ed-8442-bbfffbf3eed7"
      },
      "source": [
        "%%writefile etl_presion_fiscal_dag.py\n",
        "\n",
        "# Import necessary modules from Apache Airflow and other libraries\n",
        "from airflow import DAG # DAG object to define the workflow\n",
        "from datetime import datetime, timedelta # For specifying start_date and retry_delay\n",
        "import pandas as pd # For data transformation using DataFrames\n",
        "from airflow.operators.python import PythonOperator # Operator to run Python functions\n",
        "# from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator # While used before, we are now using PythonOperator for loading\n",
        "from google.cloud import bigquery # For interacting with Google BigQuery API\n",
        "import os # For accessing environment variables\n",
        "from google.oauth2 import service_account # For loading Google Cloud service account credentials\n",
        "import requests # For downloading the CSV file\n",
        "\n",
        "# Define default arguments for the DAG\n",
        "# These arguments are passed to all tasks unless overridden\n",
        "default_args = {\n",
        "    'owner': 'airflow', # The owner of the DAG\n",
        "    'depends_on_past': False, # Set to true if you want tasks to wait for the previous run to succeed\n",
        "    'start_date': datetime(2023, 1, 1), # The date when the DAG starts being scheduled\n",
        "    'email_on_failure': False, # Set to true to receive emails on task failure\n",
        "    'email_on_retry': False, # Set to true to receive emails on task retry\n",
        "    'retries': 1, # Number of times to retry a task on failure\n",
        "    'retry_delay': timedelta(minutes=5), # Time to wait between retries\n",
        "}\n",
        "\n",
        "# Define the DAG object\n",
        "# 'etl_presion_fiscal_sri' is the unique DAG ID\n",
        "with DAG(\n",
        "    'etl_presion_fiscal_sri',\n",
        "    default_args=default_args, # Apply the default arguments\n",
        "    description='ETL pipeline to download, transform, and load SRI tax pressure data to BigQuery', # A brief description of the DAG\n",
        "    schedule=None, # Set the schedule interval using a cron expression or timedelta, or None for manual runs\n",
        "    catchup=False, # Set to true to run the DAG for past dates starting from the start_date\n",
        "    tags=['etl', 'bigquery', 'sri'], # Tags to help categorize and filter DAGs in the Airflow UI\n",
        ") as dag:\n",
        "\n",
        "    # Define the local filename for the downloaded CSV\n",
        "    local_filename = \"SRI_Presion_Fiscal.csv\"\n",
        "\n",
        "    # Define the filename for the Google Cloud service account credentials\n",
        "    credential_filename = \"etl-sistemas-sri-819ded2ef88d.json\" # !!! Replace with your actual filename !!!\n",
        "    # Set the environment variable for Google Cloud authentication\n",
        "    # Note: In a real Airflow environment, prefer using Airflow Connections for GCP\n",
        "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_filename\n",
        "\n",
        "\n",
        "    # Define the Python function for the download task\n",
        "    def download_csv_from_url(csv_url, local_filename):\n",
        "        \"\"\"Downloads a CSV file from a given URL and saves it locally.\"\"\"\n",
        "        # import requests # Already imported at the top level\n",
        "        print(f\"Attempting to download file from: {csv_url}\")\n",
        "        try:\n",
        "            response = requests.get(csv_url)\n",
        "            response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "            with open(local_filename, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "\n",
        "            print(f\"Archivo '{local_filename}' descargado exitosamente.\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error al descargar el archivo: {e}\")\n",
        "            # Raise an exception to fail the task in Airflow\n",
        "            raise ValueError(f\"Download task failed: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred during download: {e}\")\n",
        "            raise ValueError(f\"Download task failed unexpectedly: {e}\")\n",
        "\n",
        "\n",
        "    # Instantiate the PythonOperator for the download task\n",
        "    # Executes the 'download_csv_from_url' function\n",
        "    download_csv_task = PythonOperator(\n",
        "        task_id='download_csv', # Unique identifier for the task within the DAG\n",
        "        python_callable=download_csv_from_url, # The Python function to execute\n",
        "        op_kwargs={'csv_url': csv_url, 'local_filename': local_filename}, # Keyword arguments to pass to the function\n",
        "        # dag=dag, # Associate the task with the DAG object implicitly when using 'with DAG(...)'\n",
        "    )\n",
        "\n",
        "\n",
        "    # Define the Python function for the data transformation task\n",
        "    def transform_data(local_filename):\n",
        "        \"\"\"\n",
        "        Loads data from a CSV, cleans and transforms it, and returns a DataFrame.\n",
        "        This function replicates the successful transformation steps from the notebook.\n",
        "        \"\"\"\n",
        "        # import pandas as pd # Already imported at the top level\n",
        "        print(f\"Attempting to transform data from: {local_filename}\")\n",
        "        try:\n",
        "            # Reload the CSV assuming semicolon as a separator and no header\n",
        "            df = pd.read_csv(local_filename, encoding='latin-1', header=None, sep=';')\n",
        "\n",
        "            # Remove the header row which is present as the first data row\n",
        "            df = df.iloc[1:].copy()\n",
        "\n",
        "            # Reset index after removing the row\n",
        "            df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "            # Rename columns to meaningful names\n",
        "            df.columns = ['Region', 'País', 'Año', '%_Presion', 'Tipo_Presion']\n",
        "\n",
        "            # Clean up the '%_Presion' column by removing leading/trailing spaces\n",
        "            df['%_Presion'] = df['%_Presion'].astype(str).str.strip()\n",
        "\n",
        "            # Convert data types\n",
        "            # Convert 'Año' to integer, coercing errors to NaN and then to Int64 (allows for NaNs)\n",
        "            df['Año'] = pd.to_numeric(df['Año'], errors='coerce').astype('Int64')\n",
        "\n",
        "            # Convert '%_Presion' to float, replacing comma with dot and coercing errors to NaN\n",
        "            df['%_Presion'] = df['%_Presion'].astype(str).str.replace(',', '.', regex=False)\n",
        "            df['%_Presion'] = pd.to_numeric(df['%_Presion'], errors='coerce')\n",
        "\n",
        "            print(\"Data transformation completed successfully.\")\n",
        "            # print(\"Transformed DataFrame head (first 5 rows):\")\n",
        "            # print(df.head().to_string()) # Print first few rows for logging/debugging\n",
        "\n",
        "            # print(\"\\nTransformed DataFrame info:\")\n",
        "            # df.info() # Print info for logging/debugging\n",
        "\n",
        "            # Return the transformed DataFrame so it can be pushed to XCom\n",
        "            return df\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: The file '{local_filename}' was not found during transformation.\")\n",
        "            raise ValueError(f\"Transform task failed: Input file not found: {local_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred during transformation: {e}\")\n",
        "            raise ValueError(f\"Transform task failed unexpectedly: {e}\")\n",
        "\n",
        "\n",
        "    # Instantiate the PythonOperator for the transformation task\n",
        "    # Executes the 'transform_data' function\n",
        "    # XCom is enabled by default for PythonOperator, allowing the returned DataFrame to be pushed\n",
        "    transform_data_task = PythonOperator(\n",
        "        task_id='transform_data', # Unique identifier\n",
        "        python_callable=transform_data, # The function to execute\n",
        "        op_kwargs={'local_filename': local_filename}, # Pass the filename\n",
        "        # dag=dag, # Associate with the DAG implicitly\n",
        "    )\n",
        "\n",
        "\n",
        "    # Define the Python function for the BigQuery loading task\n",
        "    # This function receives the DataFrame from XCom\n",
        "    def load_data_to_bigquery_with_xcom(ti, project_id: str, dataset_id: str, table_id: str):\n",
        "        \"\"\"\n",
        "        Loads a Pandas DataFrame (pulled from XCom) into a Google BigQuery table.\n",
        "        Assumes GOOGLE_APPLICATION_CREDENTIALS environment variable is set or\n",
        "        Airflow connection handles authentication.\n",
        "        \"\"\"\n",
        "        # Import inside the function for Airflow execution context isolation\n",
        "        # from google.cloud import bigquery # Already imported at top level\n",
        "        # from google.oauth2 import service_account # Already imported at top level\n",
        "        # import os # Already imported at top level\n",
        "        # import pandas as pd # Already imported at top level\n",
        "\n",
        "        print(f\"Attempting to load data to BigQuery table: {project_id}.{dataset_id}.{table_id}\")\n",
        "\n",
        "        try:\n",
        "            # Pull the DataFrame from the previous task's XCom\n",
        "            # 'ti' (TaskInstance) is automatically passed to the python_callable\n",
        "            df = ti.xcom_pull(task_ids='transform_data')\n",
        "\n",
        "            if df is None:\n",
        "                 # If df is None, the previous task might not have returned a DataFrame or failed\n",
        "                 raise ValueError(\"No DataFrame received from the 'transform_data' task via XCom. Check the upstream task.\")\n",
        "\n",
        "            print(f\"Successfully pulled DataFrame from XCom with {len(df)} rows.\")\n",
        "\n",
        "            # Initialize BigQuery client\n",
        "            # The client will automatically use credentials from GOOGLE_APPLICATION_CREDENTIALS\n",
        "            # or from the Airflow connection if configured.\n",
        "            client = bigquery.Client(project=project_id)\n",
        "\n",
        "            # Define the target table reference\n",
        "            table_ref = client.dataset(dataset_id).table(table_id)\n",
        "\n",
        "            # Define the schema for the BigQuery table\n",
        "            # Ensure schema matches the DataFrame structure and data types\n",
        "            schema = [\n",
        "                bigquery.SchemaField(\"Region\", \"STRING\"),\n",
        "                bigquery.SchemaField(\"País\", \"STRING\"),\n",
        "                bigquery.SchemaField(\"Año\", \"INT64\"), # BigQuery type for pandas Int64\n",
        "                bigquery.SchemaField(\"%_Presion\", \"FLOAT64\"), # BigQuery type for pandas float64\n",
        "                bigquery.SchemaField(\"Tipo_Presion\", \"STRING\"),\n",
        "            ]\n",
        "\n",
        "            # Configure the load job\n",
        "            job_config = bigquery.LoadJobConfig(\n",
        "                schema=schema, # Specify the schema\n",
        "                write_disposition=\"WRITE_TRUNCATE\",  # Overwrite the table if it exists\n",
        "                # source_format is not needed when loading from DataFrame\n",
        "            )\n",
        "\n",
        "            # Check if the dataset exists, create if not (optional, can also be a separate task)\n",
        "            try:\n",
        "                client.get_dataset(dataset_id)\n",
        "                print(f\"Dataset '{dataset_id}' already exists.\")\n",
        "            except Exception: # Use a more specific exception if possible, e.g., google.api_core.exceptions.NotFound\n",
        "                print(f\"Dataset '{dataset_id}' not found, creating...\")\n",
        "                dataset = bigquery.Dataset(client.dataset(dataset_id))\n",
        "                dataset.location = \"US\" # Set location - adjust as needed\n",
        "                client.create_dataset(dataset, exists_ok=True)\n",
        "                print(f\"Dataset '{dataset_id}' created.\")\n",
        "\n",
        "\n",
        "            # Check if the table exists, create if not (optional, WRITE_TRUNCATE handles existence)\n",
        "            # Using client.create_table with exists_ok=True is a safe way to ensure schema is defined\n",
        "            table = bigquery.Table(table_ref, schema=schema)\n",
        "            try:\n",
        "                 client.create_table(table, exists_ok=True)\n",
        "                 print(f\"Table '{table_id}' created or already exists with defined schema.\")\n",
        "            except Exception as e:\n",
        "                 print(f\"Error ensuring table '{table_id}' exists or is created: {e}\")\n",
        "                 # Depending on error, might need to raise or retry\n",
        "\n",
        "\n",
        "            # Load data from DataFrame to BigQuery\n",
        "            # This makes the API request to load the data\n",
        "            job = client.load_table_from_dataframe(\n",
        "                df, table_ref, job_config=job_config\n",
        "            )\n",
        "\n",
        "            print(f\"Starting BigQuery load job: {job.job_id}\")\n",
        "            job.result()  # Wait for the job to complete\n",
        "\n",
        "            print(f\"DataFrame successfully loaded to {project_id}.{dataset_id}.{table_id}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data to BigQuery: {e}\")\n",
        "            # Raise an exception to fail the task in Airflow\n",
        "            raise ValueError(f\"BigQuery load task failed: {e}\")\n",
        "\n",
        "\n",
        "    # Define BigQuery parameters for the load task\n",
        "    # Get Project ID from the environment variable GOOGLE_CLOUD_PROJECT, falling back to credentials\n",
        "    # In a real Airflow environment, prefer using a Variable or Connection\n",
        "    PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT', None)\n",
        "    if PROJECT_ID is None:\n",
        "        try:\n",
        "            # This part might not run reliably in an Airflow worker unless the key is accessible\n",
        "            credentials_check = service_account.Credentials.from_service_account_file(os.environ['GOOGLE_APPLICATION_CREDENTIALS'])\n",
        "            PROJECT_ID = credentials_check.project_id\n",
        "        except Exception as e:\n",
        "            print(f\"Could not automatically determine Google Cloud Project ID. Please set GOOGLE_CLOUD_PROJECT environment variable or ensure GOOGLE_APPLICATION_CREDENTIALS is correctly set: {e}\")\n",
        "            # Fallback or set a default - In a production DAG, this should likely fail or use an Airflow connection\n",
        "            PROJECT_ID = \"your-gcp-project-id\" # !!! REPLACE WITH YOUR ACTUAL GCP PROJECT ID OR USE AIRFLOW CONNECTION/VARIABLE !!!\n",
        "            print(f\"Using fallback Project ID: {PROJECT_ID}\")\n",
        "\n",
        "\n",
        "    DATASET_ID = \"sri_presion_fiscal\" # BigQuery Dataset ID\n",
        "    TABLE_ID = \"sri_presion_fiscal\" # BigQuery Table ID\n",
        "\n",
        "\n",
        "    # Instantiate the PythonOperator for the BigQuery loading task\n",
        "    # Executes the 'load_data_to_bigquery_with_xcom' function\n",
        "    # This task implicitly pulls the DataFrame from the upstream task's XCom\n",
        "    load_to_bigquery_task = PythonOperator(\n",
        "        task_id='load_to_bigquery', # Unique identifier\n",
        "        python_callable=load_data_to_bigquery_with_xcom, # The function to execute\n",
        "        op_kwargs={\n",
        "            'project_id': PROJECT_ID, # Pass project ID\n",
        "            'dataset_id': DATASET_ID, # Pass dataset ID\n",
        "            'table_id': TABLE_ID, # Pass table ID\n",
        "            # The DataFrame is passed via XComs, no need to pass it here\n",
        "        },\n",
        "        # dag=dag, # Associate with the DAG implicitly\n",
        "    )\n",
        "\n",
        "    # Set the order of execution for the tasks using the bitshift operator\n",
        "    # This defines the dependency: download_csv_task runs first, then transform_data_task, then load_to_bigquery_task\n",
        "    download_csv_task >> transform_data_task >> load_to_bigquery_task\n",
        "\n",
        "    # Print a confirmation of the task dependencies (for debugging/verification in a script)\n",
        "    print(\"Task dependencies set:\")\n",
        "    print(f\"'{download_csv_task.task_id}' >> '{transform_data_task.task_id}' >> '{load_to_bigquery_task.task_id}'\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing etl_presion_fiscal_dag.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a17a623"
      },
      "source": [
        "## Resumen:\n",
        "\n",
        "### Hallazgos Clave del Análisis de Datos\n",
        "\n",
        "* El DAG integra correctamente tres tareas principales: descargar un archivo CSV desde una URL específica, transformar los datos descargados (incluyendo la limpieza, el cambio de nombre de las columnas y la conversión de tipos) y cargar los datos transformados en una tabla de Google BigQuery.\n",
        "* El proceso de transformación de datos implica recargar el CSV con el delimitador correcto (punto y coma) y sin encabezado, eliminar la fila de encabezado incorrecta, cambiar el nombre de las columnas a \"Región\", \"País\", \"Año\", \"%_Presion\" y \"Tipo_Presion\", limpiar los espacios iniciales y finales de \"%_Presion\" y convertir \"Año\" a \"Int64\" y \"%_Presion\" a \"Float64\".\n",
        "* La tarea de carga de BigQuery utiliza XComs para recibir el DataFrame transformado de la tarea de transformación. Está configurado para sobrescribir la tabla de destino (`WRITE_TRUNCATE`) e incluye lógica para garantizar que el conjunto de datos y la tabla de destino existan con el esquema correcto.\n",
        "* Las dependencias entre tareas se definen correctamente mediante el operador bitshift (`>>`), lo que garantiza que el orden de ejecución sea descarga -> transformación -> carga.\n",
        "* El DAG incluye comentarios completos que explican el propósito de las importaciones, los argumentos predeterminados, la definición del DAG, las funciones de las tareas, los operadores y las dependencias. También incluye una cadena descriptiva para el DAG.\n",
        "* El archivo del DAG está estructurado para colocarse directamente en la carpeta `dags` de Airflow e incluye las importaciones y las definiciones de funciones necesarias dentro del contexto del DAG. La autenticación para Google Cloud se gestiona mediante la variable de entorno `GOOGLE_APPLICATION_CREDENTIALS`, con una nota sobre la preferencia por las conexiones de Airflow en producción.\n",
        "\n",
        "### Perspectivas o próximos pasos\n",
        "\n",
        "* Para un entorno de producción de Airflow, refactorice la autenticación de Google Cloud para usar conexiones de Airflow en lugar de depender de la variable de entorno `GOOGLE_APPLICATION_CREDENTIALS` dentro del archivo DAG.\n",
        "* Implemente un manejo y registro de errores más robusto dentro de las funciones de tarea para proporcionar una mejor visibilidad de los fallos durante la descarga, la transformación o la carga."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "030fdb15"
      },
      "source": [
        "## Tarea de Descarga\n",
        "\n",
        "### Subtask:\n",
        "Definir una tarea en el DAG para descargar el archivo CSV desde la URL proporcionada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "9c4ab795",
        "outputId": "2a8adad0-6cb9-4ce3-d10e-a66c004e1e0b"
      },
      "source": [
        "import requests\n",
        "from airflow.providers.standard.operators.python import PythonOperator\n",
        "\n",
        "# Define the Python function for the download task\n",
        "def download_csv_from_url(csv_url, local_filename):\n",
        "    \"\"\"Downloads a CSV file from a given URL and saves it locally.\"\"\"\n",
        "    print(f\"Attempting to download file from: {csv_url}\")\n",
        "    try:\n",
        "        response = requests.get(csv_url)\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "        with open(local_filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        print(f\"Archivo '{local_filename}' descargado exitosamente.\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error al descargar el archivo: {e}\")\n",
        "        # Raise an exception to fail the task in Airflow\n",
        "        raise ValueError(f\"Download task failed: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during download: {e}\")\n",
        "        raise ValueError(f\"Download task failed unexpectedly: {e}\")\n",
        "\n",
        "# Define the URL and local filename\n",
        "csv_url = \"https://www.sri.gob.ec/o/sri-portlet-biblioteca-alfresco-internet/descargar/7e45627e-1f7e-4e21-ae59-d520634fc63f/SRI_Presion_Fiscal.csv\"\n",
        "local_filename = \"SRI_Presion_Fiscal.csv\"\n",
        "\n",
        "# Instantiate the PythonOperator for the download task\n",
        "# Executes the 'download_csv_from_url' function\n",
        "download_csv_task = PythonOperator(\n",
        "    task_id='download_csv', # Unique identifier for the task within the DAG\n",
        "    python_callable=download_csv_from_url, # The Python function to execute\n",
        "    op_kwargs={'csv_url': csv_url, 'local_filename': local_filename}, # Keyword arguments to pass to the function\n",
        "    dag=dag, # Associate the task with the DAG object\n",
        ")\n",
        "\n",
        "print(f\"Tarea '{download_csv_task.task_id}' definida para la descarga del CSV.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'airflow'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-316656329.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mairflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproviders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPythonOperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define the Python function for the download task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdownload_csv_from_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'airflow'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2d1b7039",
        "outputId": "49ffafb1-f4a7-4aff-aa71-c6428e214660"
      },
      "source": [
        "!pip install apache-airflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting apache-airflow\n",
            "  Downloading apache_airflow-3.0.2-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting apache-airflow-core==3.0.2 (from apache-airflow)\n",
            "  Downloading apache_airflow_core-3.0.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting apache-airflow-task-sdk<1.1.0,>=1.0.0 (from apache-airflow)\n",
            "  Downloading apache_airflow_task_sdk-1.0.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting a2wsgi>=1.10.8 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading a2wsgi-1.10.10-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting aiosqlite>=0.20.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting alembic<2.0,>=1.13.1 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting apache-airflow-providers-common-compat>=1.6.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading apache_airflow_providers_common_compat-1.7.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting apache-airflow-providers-common-io>=1.5.3 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading apache_airflow_providers_common_io-1.6.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting apache-airflow-providers-common-sql>=1.26.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading apache_airflow_providers_common_sql-1.27.2-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting apache-airflow-providers-smtp>=2.0.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading apache_airflow_providers_smtp-2.1.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting apache-airflow-providers-standard>=0.4.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading apache_airflow_providers_standard-1.3.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting argcomplete>=1.10 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading argcomplete-3.6.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting asgiref>=2.3.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: attrs!=25.2.0,>=22.1.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (25.3.0)\n",
            "Collecting cadwyn>=5.2.1 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading cadwyn-5.4.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting colorlog>=6.8.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting cron-descriptor>=1.2.24 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading cron_descriptor-1.4.5-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting croniter>=2.0.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading croniter-6.0.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: cryptography>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (43.0.3)\n",
            "Collecting deprecated>=1.2.13 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: dill>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (0.3.7)\n",
            "Requirement already satisfied: fastapi!=0.115.10,>=0.115.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow) (0.115.13)\n",
            "Requirement already satisfied: flask>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (3.1.1)\n",
            "Collecting gunicorn>=20.1.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.5 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (8.7.0)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.5 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=4.19.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (4.24.0)\n",
            "Collecting lazy-object-proxy>=1.2.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading lazy_object_proxy-1.11.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting libcst>=1.1.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading libcst-1.8.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: linkify-it-py>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.0.3)\n",
            "Collecting lockfile>=0.12.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading lockfile-0.12.2-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting methodtools>=0.4.7 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading methodtools-0.4.7-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting opentelemetry-api>=1.26.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp>=1.26.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_exporter_otlp-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (24.2)\n",
            "Collecting pathspec>=0.9.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pendulum<4.0,>=2.1.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading pendulum-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: pluggy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (1.6.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (5.9.5)\n",
            "Requirement already satisfied: pydantic>=2.11.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.11.7)\n",
            "Requirement already satisfied: pygments!=2.19.0,>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.19.2)\n",
            "Requirement already satisfied: pyjwt>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.10.1)\n",
            "Collecting python-daemon>=3.0.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading python_daemon-3.1.2-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify>=5.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (8.0.4)\n",
            "Requirement already satisfied: requests<3,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.32.3)\n",
            "Collecting rich-argparse>=1.0.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading rich_argparse-1.7.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: rich>=13.6.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (13.9.4)\n",
            "Requirement already satisfied: setproctitle>=1.3.3 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (1.3.6)\n",
            "Collecting sqlalchemy-jsonfield>=1.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading SQLAlchemy_JSONField-1.0.2-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting sqlalchemy-utils>=0.41.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading SQLAlchemy_Utils-0.41.2-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting sqlalchemy<2.0,>=1.4.49 (from sqlalchemy[asyncio]<2.0,>=1.4.49->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading SQLAlchemy-1.4.54-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting svcs>=25.1.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading svcs-25.1.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (0.9.0)\n",
            "Requirement already satisfied: tenacity!=8.2.0,>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (8.5.0)\n",
            "Requirement already satisfied: termcolor>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (3.1.0)\n",
            "Collecting typing-extensions!=4.14.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting universal-pathlib!=0.2.4,>=0.2.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading universal_pathlib-0.2.6-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting uuid6>=2024.7.10 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading uuid6-2025.0.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting aiologic>=0.14.0 (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow)\n",
            "  Downloading aiologic-0.14.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow) (2025.3.2)\n",
            "Collecting msgspec>=0.19.0 (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow)\n",
            "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting psutil>=5.8.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting retryhttp!=1.3.0,>=1.2.0 (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow)\n",
            "  Downloading retryhttp-1.3.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting structlog>=25.4.0 (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow)\n",
            "  Downloading structlog-25.4.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: wrapt>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from aiologic>=0.14.0->apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow) (1.17.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic<2.0,>=1.13.1->apache-airflow-core==3.0.2->apache-airflow) (1.1.3)\n",
            "Requirement already satisfied: sqlparse>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-common-sql>=1.26.0->apache-airflow-core==3.0.2->apache-airflow) (0.5.3)\n",
            "Requirement already satisfied: more-itertools>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-common-sql>=1.26.0->apache-airflow-core==3.0.2->apache-airflow) (10.7.0)\n",
            "Requirement already satisfied: starlette>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from cadwyn>=5.2.1->apache-airflow-core==3.0.2->apache-airflow) (0.46.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from cadwyn>=5.2.1->apache-airflow-core==3.0.2->apache-airflow) (0.4.1)\n",
            "Requirement already satisfied: pytz>2021.1 in /usr/local/lib/python3.11/dist-packages (from croniter>=2.0.2->apache-airflow-core==3.0.2->apache-airflow) (2025.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=41.0.0->apache-airflow-core==3.0.2->apache-airflow) (1.17.1)\n",
            "Collecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow) (0.0.20)\n",
            "Collecting email-validator>=2.0.0 (from fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow) (0.34.3)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask>=2.1.1->apache-airflow-core==3.0.2->apache-airflow) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=2.1.1->apache-airflow-core==3.0.2->apache-airflow) (8.2.1)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask>=2.1.1->apache-airflow-core==3.0.2->apache-airflow) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from flask>=2.1.1->apache-airflow-core==3.0.2->apache-airflow) (3.1.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->apache-airflow-core==3.0.2->apache-airflow) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->apache-airflow-core==3.0.2->apache-airflow) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->apache-airflow-core==3.0.2->apache-airflow) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->apache-airflow-core==3.0.2->apache-airflow) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->apache-airflow-core==3.0.2->apache-airflow) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.5->apache-airflow-core==3.0.2->apache-airflow) (3.23.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.1->apache-airflow-core==3.0.2->apache-airflow) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.1->apache-airflow-core==3.0.2->apache-airflow) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.1->apache-airflow-core==3.0.2->apache-airflow) (0.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.2 in /usr/local/lib/python3.11/dist-packages (from libcst>=1.1.0->apache-airflow-core==3.0.2->apache-airflow) (6.0.2)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.11/dist-packages (from linkify-it-py>=2.0.0->apache-airflow-core==3.0.2->apache-airflow) (1.0.3)\n",
            "Collecting wirerope>=0.4.7 (from methodtools>=0.4.7->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading wirerope-1.0.0-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc==1.34.1 (from opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http==1.34.1 (from opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.34.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.63.2 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow) (1.73.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk~=1.34.1 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: protobuf<6.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-proto==1.34.1->opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow) (5.29.5)\n",
            "Requirement already satisfied: tzdata>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pendulum<4.0,>=2.1.2->apache-airflow-core==3.0.2->apache-airflow) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.0->apache-airflow-core==3.0.2->apache-airflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.0->apache-airflow-core==3.0.2->apache-airflow) (2.33.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7.0->apache-airflow-core==3.0.2->apache-airflow) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify>=5.0->apache-airflow-core==3.0.2->apache-airflow) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.31.0->apache-airflow-core==3.0.2->apache-airflow) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.31.0->apache-airflow-core==3.0.2->apache-airflow) (2.4.0)\n",
            "Collecting types-requests (from retryhttp!=1.3.0,>=1.2.0->apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow)\n",
            "  Downloading types_requests-2.32.4.20250611-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.6.0->apache-airflow-core==3.0.2->apache-airflow) (3.0.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<2.0,>=1.4.49->sqlalchemy[asyncio]<2.0,>=1.4.49->apache-airflow-core==3.0.2->apache-airflow) (3.2.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=41.0.0->apache-airflow-core==3.0.2->apache-airflow) (2.22)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow) (0.16.0)\n",
            "Collecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading rich_toolkit-0.14.7-py3-none-any.whl.metadata (999 bytes)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.6.0->apache-airflow-core==3.0.2->apache-airflow) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->apache-airflow-core==3.0.2->apache-airflow) (1.3.1)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow) (15.0.1)\n",
            "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk~=1.34.1->opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow) (1.5.4)\n",
            "Downloading apache_airflow-3.0.2-py3-none-any.whl (12 kB)\n",
            "Downloading apache_airflow_core-3.0.2-py3-none-any.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading apache_airflow_task_sdk-1.0.2-py3-none-any.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.5/241.5 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading a2wsgi-1.10.10-py3-none-any.whl (17 kB)\n",
            "Downloading aiologic-0.14.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
            "Downloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading apache_airflow_providers_common_compat-1.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading apache_airflow_providers_common_io-1.6.0-py3-none-any.whl (19 kB)\n",
            "Downloading apache_airflow_providers_common_sql-1.27.2-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading apache_airflow_providers_smtp-2.1.0-py3-none-any.whl (21 kB)\n",
            "Downloading apache_airflow_providers_standard-1.3.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading argcomplete-3.6.2-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading cadwyn-5.4.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading cron_descriptor-1.4.5-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading croniter-6.0.0-py2.py3-none-any.whl (25 kB)\n",
            "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lazy_object_proxy-1.11.0-py3-none-any.whl (16 kB)\n",
            "Downloading libcst-1.8.2-cp311-cp311-manylinux_2_28_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lockfile-0.12.2-py2.py3-none-any.whl (13 kB)\n",
            "Downloading methodtools-0.4.7-py2.py3-none-any.whl (4.0 kB)\n",
            "Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp-1.34.1-py3-none-any.whl (7.0 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_http-1.34.1-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.34.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading pendulum-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (353 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m353.7/353.7 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_daemon-3.1.2-py3-none-any.whl (30 kB)\n",
            "Downloading retryhttp-1.3.3-py3-none-any.whl (17 kB)\n",
            "Downloading rich_argparse-1.7.1-py3-none-any.whl (25 kB)\n",
            "Downloading SQLAlchemy-1.4.54-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy_JSONField-1.0.2-py3-none-any.whl (10 kB)\n",
            "Downloading SQLAlchemy_Utils-0.41.2-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading structlog-25.4.0-py3-none-any.whl (68 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.7/68.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading svcs-25.1.0-py3-none-any.whl (19 kB)\n",
            "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading universal_pathlib-0.2.6-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uuid6-2025.0.0-py3-none-any.whl (7.0 kB)\n",
            "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\n",
            "Downloading wirerope-1.0.0-py2.py3-none-any.whl (9.2 kB)\n",
            "Downloading types_requests-2.32.4.20250611-py3-none-any.whl (20 kB)\n",
            "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading rich_toolkit-0.14.7-py3-none-any.whl (24 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m136.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lockfile, cron-descriptor, wirerope, uvloop, uuid6, universal-pathlib, typing-extensions, types-requests, svcs, structlog, sqlalchemy, python-dotenv, python-daemon, psutil, pathspec, opentelemetry-proto, msgspec, libcst, lazy-object-proxy, httptools, gunicorn, dnspython, deprecated, colorlog, asgiref, argcomplete, aiologic, a2wsgi, sqlalchemy-utils, sqlalchemy-jsonfield, pendulum, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, methodtools, email-validator, croniter, alembic, aiosqlite, watchfiles, rich-toolkit, rich-argparse, opentelemetry-semantic-conventions, retryhttp, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, fastapi-cli, cadwyn, opentelemetry-exporter-otlp, apache-airflow-providers-common-compat, apache-airflow-providers-standard, apache-airflow-providers-smtp, apache-airflow-providers-common-sql, apache-airflow-providers-common-io, apache-airflow-task-sdk, apache-airflow-core, apache-airflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.14.0\n",
            "    Uninstalling typing_extensions-4.14.0:\n",
            "      Successfully uninstalled typing_extensions-4.14.0\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.41\n",
            "    Uninstalling SQLAlchemy-2.0.41:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.41\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\n",
            "typeguard 4.4.4 requires typing_extensions>=4.14.0, but you have typing-extensions 4.13.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed a2wsgi-1.10.10 aiologic-0.14.0 aiosqlite-0.21.0 alembic-1.16.2 apache-airflow-3.0.2 apache-airflow-core-3.0.2 apache-airflow-providers-common-compat-1.7.1 apache-airflow-providers-common-io-1.6.0 apache-airflow-providers-common-sql-1.27.2 apache-airflow-providers-smtp-2.1.0 apache-airflow-providers-standard-1.3.0 apache-airflow-task-sdk-1.0.2 argcomplete-3.6.2 asgiref-3.8.1 cadwyn-5.4.2 colorlog-6.9.0 cron-descriptor-1.4.5 croniter-6.0.0 deprecated-1.2.18 dnspython-2.7.0 email-validator-2.2.0 fastapi-cli-0.0.7 gunicorn-23.0.0 httptools-0.6.4 lazy-object-proxy-1.11.0 libcst-1.8.2 lockfile-0.12.2 methodtools-0.4.7 msgspec-0.19.0 opentelemetry-api-1.34.1 opentelemetry-exporter-otlp-1.34.1 opentelemetry-exporter-otlp-proto-common-1.34.1 opentelemetry-exporter-otlp-proto-grpc-1.34.1 opentelemetry-exporter-otlp-proto-http-1.34.1 opentelemetry-proto-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 pathspec-0.12.1 pendulum-3.1.0 psutil-7.0.0 python-daemon-3.1.2 python-dotenv-1.1.1 retryhttp-1.3.3 rich-argparse-1.7.1 rich-toolkit-0.14.7 sqlalchemy-1.4.54 sqlalchemy-jsonfield-1.0.2 sqlalchemy-utils-0.41.2 structlog-25.4.0 svcs-25.1.0 types-requests-2.32.4.20250611 typing-extensions-4.13.2 universal-pathlib-0.2.6 uuid6-2025.0.0 uvloop-0.21.0 watchfiles-1.1.0 wirerope-1.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              },
              "id": "d00d253c38744995a48b763672445b4d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4c210a1"
      },
      "source": [
        "## Tarea de transformación\n",
        "\n",
        "### Subtask:\n",
        "Definir una tarea que ejecute el código de transformación de datos que desarrollaste en Colab (limpieza, conversión de tipos, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54228609"
      },
      "source": [
        "from airflow import DAG\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "from airflow.providers.standard.operators.python import PythonOperator\n",
        "\n",
        "# Define default arguments for the DAG (redefined for this cell)\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2023, 1, 1),\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "# Define the DAG (redefined for this cell)\n",
        "dag = DAG(\n",
        "    'etl_presion_fiscal_sri',\n",
        "    default_args=default_args,\n",
        "    description='ETL pipeline to download, transform, and load SRI tax pressure data to BigQuery',\n",
        "    schedule=None,\n",
        "    catchup=False,\n",
        "    tags=['etl', 'bigquery', 'sri'],\n",
        ")\n",
        "\n",
        "# Define the local filename again so it's available in this scope\n",
        "local_filename = \"SRI_Presion_Fiscal.csv\"\n",
        "\n",
        "\n",
        "def transform_data(local_filename):\n",
        "    \"\"\"\n",
        "    Loads data from a CSV, cleans and transforms it, and returns a DataFrame.\n",
        "\n",
        "    Steps:\n",
        "    1. Reload the CSV assuming semicolon as a separator and no header.\n",
        "    2. Remove the header row which is present as the first data row.\n",
        "    3. Rename columns to meaningful names.\n",
        "    4. Clean the '%_Presion' column by removing leading/trailing spaces.\n",
        "    5. Convert 'Año' to integer and '%_Presion' to float, coercing errors.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Reload the CSV with correct separator and no header\n",
        "        df = pd.read_csv(local_filename, encoding='latin-1', header=None, sep=';')\n",
        "\n",
        "        # 2. Remove the header row (the first row)\n",
        "        df = df.iloc[1:].copy()\n",
        "\n",
        "        # Reset index after removing the row\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        # 3. Rename columns\n",
        "        df.columns = ['Region', 'País', 'Año', '%_Presion', 'Tipo_Presion']\n",
        "\n",
        "        # 4. Clean up the '%_Presion' column\n",
        "        df['%_Presion'] = df['%_Presion'].astype(str).str.strip()\n",
        "\n",
        "        # 5. Convert data types\n",
        "        # Convert 'Año' to integer, coercing errors to NaN and then to Int64\n",
        "        df['Año'] = pd.to_numeric(df['Año'], errors='coerce').astype('Int64')\n",
        "\n",
        "        # Convert '%_Presion' to float, replacing comma with dot and coercing errors\n",
        "        df['%_Presion'] = df['%_Presion'].astype(str).str.replace(',', '.', regex=False)\n",
        "        df['%_Presion'] = pd.to_numeric(df['%_Presion'], errors='coerce')\n",
        "\n",
        "\n",
        "        print(\"Data transformation completed successfully.\")\n",
        "        print(\"Transformed DataFrame head:\")\n",
        "        display(df.head())\n",
        "        print(\"\\nTransformed DataFrame info:\")\n",
        "        display(df.info())\n",
        "\n",
        "\n",
        "        return df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{local_filename}' was not found during transformation.\")\n",
        "        # In a real Airflow DAG, you might want to raise an exception\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during transformation: {e}\")\n",
        "        # In a real Airflow DAG, you might want to raise an exception\n",
        "        raise\n",
        "\n",
        "\n",
        "# Define the transform task using PythonOperator\n",
        "# Ensure this is only defined once within the DAG context\n",
        "transform_data_task = PythonOperator(\n",
        "    task_id='transform_data',\n",
        "    python_callable=transform_data,\n",
        "    op_kwargs={'local_filename': local_filename}, # Pass the filename to the transformation function\n",
        "    dag=dag,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b16f1a3"
      },
      "source": [
        "## Tarea de carga a bigquery\n",
        "\n",
        "### Subtask:\n",
        "Definir una tarea para cargar el DataFrame transformado en la tabla de Google BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "8bb2d15c",
        "outputId": "4b06762f-0187-47bb-e29e-a16530444879"
      },
      "source": [
        "from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator\n",
        "from google.cloud import bigquery\n",
        "import os\n",
        "import pandas as pd # Import pandas as it's used in the callable\n",
        "\n",
        "# Define BigQuery parameters (replace with your actual values)\n",
        "# Assuming 'credentials' and 'dag' objects are available from previous cells\n",
        "PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT', credentials.project_id) # Use the project ID from credentials\n",
        "DATASET_ID = \"sri_presion_fiscal\"\n",
        "TABLE_ID = \"sri_presion_fiscal\"\n",
        "# BIGQUERY_CONN_ID = 'google_cloud_default' # This is for operators that use connections, PythonOperator uses the client directly\n",
        "\n",
        "def load_data_to_bigquery_with_xcom(ti, project_id: str, dataset_id: str, table_id: str):\n",
        "    \"\"\"\n",
        "    Loads a Pandas DataFrame (pulled from XCom) into a Google BigQuery table.\n",
        "    Assumes GOOGLE_APPLICATION_CREDENTIALS environment variable is set or\n",
        "    Airflow connection handles authentication.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Pull the DataFrame from the previous task's XCom\n",
        "        # The key is the task_id of the upstream task\n",
        "        df = ti.xcom_pull(task_ids='transform_data')\n",
        "\n",
        "        if df is None:\n",
        "            raise ValueError(\"No DataFrame received from the transform_data task via XCom.\")\n",
        "\n",
        "        # Initialize BigQuery client\n",
        "        # The client will automatically use credentials from GOOGLE_APPLICATION_CREDENTIALS\n",
        "        # or from the Airflow connection if configured.\n",
        "        client = bigquery.Client(project=project_id)\n",
        "\n",
        "        table_ref = client.dataset(dataset_id).table(table_id)\n",
        "\n",
        "        # Define the schema based on the DataFrame columns and expected types\n",
        "        # This assumes the DataFrame columns are already in the correct order and types\n",
        "        # after the transformation step.\n",
        "        schema = [\n",
        "            bigquery.SchemaField(\"Region\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"País\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"Año\", \"INT64\"), # Use INT64 for pandas Int64 dtype\n",
        "            bigquery.SchemaField(\"%_Presion\", \"FLOAT64\"), # Use FLOAT64 for pandas float64 dtype\n",
        "            bigquery.SchemaField(\"Tipo_Presion\", \"STRING\"),\n",
        "        ]\n",
        "\n",
        "        # Configure the load job\n",
        "        job_config = bigquery.LoadJobConfig(\n",
        "            schema=schema,\n",
        "            write_disposition=\"WRITE_TRUNCATE\",  # Overwrite the table\n",
        "            # You might need to specify source_format if loading from a file,\n",
        "            # but when loading from DataFrame, BigQuery client handles this.\n",
        "        )\n",
        "\n",
        "        # Load data from DataFrame\n",
        "        job = client.load_table_from_dataframe(\n",
        "            df, table_ref, job_config=job_config\n",
        "        )\n",
        "\n",
        "        print(f\"Starting BigQuery load job: {job.job_id}\")\n",
        "        job.result()  # Wait for the job to complete\n",
        "\n",
        "        print(f\"DataFrame successfully loaded to {project_id}.{dataset_id}.{table_id}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data to BigQuery: {e}\")\n",
        "        # In a real Airflow DAG, you might want to raise an exception\n",
        "        raise\n",
        "\n",
        "\n",
        "# Instantiate the PythonOperator for the loading task\n",
        "# We need to pass the DataFrame from the transform_data_task to this task.\n",
        "# In Airflow, this is typically done by using XComs.\n",
        "# However, passing a large DataFrame via XComs is not the most efficient way for large datasets in Airflow.\n",
        "# A better approach would be to save the transformed data to a temporary location (e.g., GCS)\n",
        "# in the transform_data_task and then use a BigQueryLoadOperator to load from that location.\n",
        "# For the purpose of this exercise, we will assume the DataFrame is passed (conceptually via XComs)\n",
        "# or available in a shared location that the load task can access.\n",
        "# Since we are defining the DAG structure, we will define the task and indicate\n",
        "# that it depends on the output of the transform task. The actual data passing\n",
        "# mechanism would be handled by Airflow's execution environment (e.g., XComs or shared storage).\n",
        "\n",
        "# For demonstration purposes in this DAG definition script, we will define the task\n",
        "# and assume the 'df' object (returned by transform_data_task) is somehow available.\n",
        "# In a real Airflow execution, the 'python_callable' would receive the XCom value\n",
        "# if the previous task pushed it.\n",
        "\n",
        "load_to_bigquery_task = PythonOperator(\n",
        "    task_id='load_to_bigquery',\n",
        "    python_callable=load_data_to_bigquery_with_xcom,\n",
        "    op_kwargs={\n",
        "        'project_id': PROJECT_ID,\n",
        "        'dataset_id': DATASET_ID,\n",
        "        'table_id': TABLE_ID,\n",
        "        # The DataFrame is passed implicitly via XComs and pulled by the callable function.\n",
        "    },\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "# Add the dependency: transform_data_task must complete before load_to_bigquery_task starts\n",
        "# This dependency also implies that the output of transform_data_task (the DataFrame)\n",
        "# will be available for load_to_bigquery_task via XComs.\n",
        "# This line should be placed after both tasks are defined.\n",
        "# transform_data_task >> load_to_bigquery_task # This dependency will be set in a later step.\n",
        "\n",
        "print(f\"Task '{load_to_bigquery_task.task_id}' defined for loading data to BigQuery.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'airflow.providers.google'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-1340269412.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mairflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproviders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigquery\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBigQueryInsertJobOperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbigquery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[0;31m# Import pandas as it's used in the callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'airflow.providers.google'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b39d83c5",
        "outputId": "ca23955f-afae-48c2-da22-e49f74ffa47f"
      },
      "source": [
        "!pip install apache-airflow-providers-google"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting apache-airflow-providers-google\n",
            "  Downloading apache_airflow_providers_google-16.0.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: apache-airflow>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (3.0.2)\n",
            "Requirement already satisfied: apache-airflow-providers-common-compat>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (1.7.1)\n",
            "Requirement already satisfied: apache-airflow-providers-common-sql>=1.27.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (1.27.2)\n",
            "Requirement already satisfied: asgiref>=3.5.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (3.8.1)\n",
            "Requirement already satisfied: dill>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (0.3.7)\n",
            "Collecting gcloud-aio-auth>=5.2.0 (from apache-airflow-providers-google)\n",
            "  Downloading gcloud_aio_auth-5.4.2-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting gcloud-aio-bigquery>=6.1.2 (from apache-airflow-providers-google)\n",
            "  Downloading gcloud_aio_bigquery-7.1.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting gcloud-aio-storage>=9.0.0 (from apache-airflow-providers-google)\n",
            "  Downloading gcloud_aio_storage-9.4.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: gcsfs>=2023.10.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (2025.3.2)\n",
            "Collecting google-ads>=26.0.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_ads-27.0.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting google-analytics-admin>=0.9.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_analytics_admin-0.24.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: google-api-core!=2.16.0,!=2.18.0,>=2.11.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (2.173.0)\n",
            "Requirement already satisfied: google-auth>=2.29.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (2.38.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (0.2.0)\n",
            "Requirement already satisfied: google-cloud-aiplatform>=1.73.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (1.99.0)\n",
            "Collecting google-cloud-alloydb>=0.4.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_alloydb-0.4.8-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting google-cloud-automl>=2.12.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_automl-2.16.4-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.21.*,!=3.22.0,!=3.23.*,>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (3.34.0)\n",
            "Collecting google-cloud-bigquery-datatransfer>=3.13.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_bigquery_datatransfer-3.19.2-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting google-cloud-bigtable>=2.17.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_bigtable-2.31.0-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting google-cloud-build>=3.31.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_build-3.31.2-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting google-cloud-compute>=1.10.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_compute-1.31.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting google-cloud-container>=2.52.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_container-2.57.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting google-cloud-datacatalog>=3.23.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_datacatalog-3.27.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting google-cloud-dataflow-client>=0.8.6 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_dataflow_client-0.9.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting google-cloud-dataform>=0.5.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_dataform-0.6.2-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting google-cloud-dataplex>=2.6.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_dataplex-2.10.2-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: google-cloud-dataproc>=5.12.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (5.20.0)\n",
            "Collecting google-cloud-dataproc-metastore>=1.12.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_dataproc_metastore-1.18.3-py3-none-any.whl.metadata (10.0 kB)\n",
            "Collecting google-cloud-dlp>=3.12.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_dlp-3.31.0-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting google-cloud-kms>=2.15.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_kms-3.5.1-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: google-cloud-language>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (2.17.2)\n",
            "Collecting google-cloud-logging>=3.5.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_logging-3.12.1-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting google-cloud-managedkafka>=0.1.6 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_managedkafka-0.1.11-py3-none-any.whl.metadata (9.6 kB)\n",
            "Collecting google-cloud-memcache>=1.7.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_memcache-1.12.2-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting google-cloud-monitoring>=2.18.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_monitoring-2.27.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting google-cloud-orchestration-airflow>=1.10.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_orchestration_airflow-1.17.5-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting google-cloud-os-login>=2.9.1 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_os_login-2.17.2-py3-none-any.whl.metadata (9.6 kB)\n",
            "Collecting google-cloud-pubsub>=2.21.3 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_pubsub-2.30.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting google-cloud-redis>=2.12.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_redis-2.18.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting google-cloud-secret-manager>=2.16.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_secret_manager-2.24.0-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: google-cloud-spanner!=3.49.0,>=3.11.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (3.55.0)\n",
            "Collecting google-cloud-speech>=2.18.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_speech-2.33.0-py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: google-cloud-storage>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (2.19.0)\n",
            "Collecting google-cloud-storage-transfer>=1.4.1 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_storage_transfer-1.17.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting google-cloud-tasks>=2.13.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_tasks-2.19.3-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting google-cloud-texttospeech>=2.14.1 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_texttospeech-2.27.0-py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: google-cloud-translate>=3.16.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (3.20.3)\n",
            "Collecting google-cloud-videointelligence>=2.11.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_videointelligence-2.16.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting google-cloud-vision>=3.4.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_vision-3.10.2-py3-none-any.whl.metadata (9.6 kB)\n",
            "Collecting google-cloud-workflows>=1.10.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_workflows-1.18.2-py3-none-any.whl.metadata (9.6 kB)\n",
            "Collecting google-cloud-run>=0.10.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_run-0.10.18-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting google-cloud-batch>=0.13.0 (from apache-airflow-providers-google)\n",
            "  Downloading google_cloud_batch-0.17.36-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting grpcio-gcp>=0.2.2 (from apache-airflow-providers-google)\n",
            "  Downloading grpcio_gcp-0.2.2-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (0.28.1)\n",
            "Collecting looker-sdk!=24.18.0,>=22.4.0 (from apache-airflow-providers-google)\n",
            "  Downloading looker_sdk-25.10.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: pandas-gbq>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (0.29.1)\n",
            "Requirement already satisfied: proto-plus>=1.19.6 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (1.26.1)\n",
            "Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (18.1.0)\n",
            "Requirement already satisfied: python-slugify>=7.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (8.0.4)\n",
            "Requirement already satisfied: PyOpenSSL>=23.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (24.2.1)\n",
            "Collecting sqlalchemy-bigquery>=1.2.1 (from apache-airflow-providers-google)\n",
            "  Downloading sqlalchemy_bigquery-1.15.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting sqlalchemy-spanner>=1.6.2 (from apache-airflow-providers-google)\n",
            "  Downloading sqlalchemy_spanner-1.14.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: tenacity>=8.3.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (8.5.0)\n",
            "Requirement already satisfied: immutabledict>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-google) (4.2.1)\n",
            "Collecting types-protobuf!=5.29.1.20250402,>=5.27.0 (from apache-airflow-providers-google)\n",
            "  Downloading types_protobuf-6.30.2.20250516-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: apache-airflow-core==3.0.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow>=2.10.0->apache-airflow-providers-google) (3.0.2)\n",
            "Requirement already satisfied: apache-airflow-task-sdk<1.1.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow>=2.10.0->apache-airflow-providers-google) (1.0.2)\n",
            "Requirement already satisfied: a2wsgi>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.10.10)\n",
            "Requirement already satisfied: aiosqlite>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.21.0)\n",
            "Requirement already satisfied: alembic<2.0,>=1.13.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.16.2)\n",
            "Requirement already satisfied: apache-airflow-providers-common-io>=1.5.3 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.6.0)\n",
            "Requirement already satisfied: apache-airflow-providers-smtp>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.1.0)\n",
            "Requirement already satisfied: apache-airflow-providers-standard>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.3.0)\n",
            "Requirement already satisfied: argcomplete>=1.10 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (3.6.2)\n",
            "Requirement already satisfied: attrs!=25.2.0,>=22.1.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (25.3.0)\n",
            "Requirement already satisfied: cadwyn>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (5.4.2)\n",
            "Requirement already satisfied: colorlog>=6.8.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (6.9.0)\n",
            "Requirement already satisfied: cron-descriptor>=1.2.24 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.4.5)\n",
            "Requirement already satisfied: croniter>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (6.0.0)\n",
            "Requirement already satisfied: cryptography>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (43.0.3)\n",
            "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.2.18)\n",
            "Requirement already satisfied: fastapi!=0.115.10,>=0.115.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.115.13)\n",
            "Requirement already satisfied: flask>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (3.1.1)\n",
            "Requirement already satisfied: gunicorn>=20.1.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (23.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=6.5 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (8.7.0)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.5 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=4.19.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (4.24.0)\n",
            "Requirement already satisfied: lazy-object-proxy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.11.0)\n",
            "Requirement already satisfied: libcst>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.8.2)\n",
            "Requirement already satisfied: linkify-it-py>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.0.3)\n",
            "Requirement already satisfied: lockfile>=0.12.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.12.2)\n",
            "Requirement already satisfied: methodtools>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.4.7)\n",
            "Requirement already satisfied: opentelemetry-api>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.34.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (24.2)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.12.1)\n",
            "Requirement already satisfied: pendulum<4.0,>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (3.1.0)\n",
            "Requirement already satisfied: pluggy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.6.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (7.0.0)\n",
            "Requirement already satisfied: pydantic>=2.11.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.11.7)\n",
            "Requirement already satisfied: pygments!=2.19.0,>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.19.2)\n",
            "Requirement already satisfied: pyjwt>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.10.1)\n",
            "Requirement already satisfied: python-daemon>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.9.0.post0)\n",
            "Requirement already satisfied: requests<3,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.32.3)\n",
            "Requirement already satisfied: rich-argparse>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.7.1)\n",
            "Requirement already satisfied: rich>=13.6.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (13.9.4)\n",
            "Requirement already satisfied: setproctitle>=1.3.3 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.3.6)\n",
            "Requirement already satisfied: sqlalchemy-jsonfield>=1.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.0.2)\n",
            "Requirement already satisfied: sqlalchemy-utils>=0.41.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.41.2)\n",
            "Requirement already satisfied: sqlalchemy<2.0,>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]<2.0,>=1.4.49->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.4.54)\n",
            "Requirement already satisfied: svcs>=25.1.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (25.1.0)\n",
            "Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.9.0)\n",
            "Requirement already satisfied: termcolor>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions!=4.14.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (4.13.2)\n",
            "Requirement already satisfied: universal-pathlib!=0.2.4,>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.2.6)\n",
            "Requirement already satisfied: uuid6>=2024.7.10 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2025.0.0)\n",
            "Requirement already satisfied: sqlparse>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-common-sql>=1.27.0->apache-airflow-providers-google) (0.5.3)\n",
            "Requirement already satisfied: more-itertools>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-common-sql>=1.27.0->apache-airflow-providers-google) (10.7.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.0 in /usr/local/lib/python3.11/dist-packages (from gcloud-aio-auth>=5.2.0->apache-airflow-providers-google) (3.11.15)\n",
            "Collecting backoff<3.0.0,>=1.0.0 (from gcloud-aio-auth>=5.2.0->apache-airflow-providers-google)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: chardet<6.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gcloud-aio-auth>=5.2.0->apache-airflow-providers-google) (5.2.0)\n",
            "Requirement already satisfied: aiofiles<25.0.0,>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from gcloud-aio-storage>=9.0.0->apache-airflow-providers-google) (24.1.0)\n",
            "Collecting pyasn1-modules<0.4.2,>=0.2.1 (from gcloud-aio-storage>=9.0.0->apache-airflow-providers-google)\n",
            "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: rsa<5.0.0,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from gcloud-aio-storage>=9.0.0->apache-airflow-providers-google) (4.9.1)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs>=2023.10.0->apache-airflow-providers-google) (4.4.2)\n",
            "Requirement already satisfied: fsspec==2025.3.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs>=2023.10.0->apache-airflow-providers-google) (2025.3.2)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (from gcsfs>=2023.10.0->apache-airflow-providers-google) (1.2.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.3 in /usr/local/lib/python3.11/dist-packages (from google-ads>=26.0.0->apache-airflow-providers-google) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.59.0 in /usr/local/lib/python3.11/dist-packages (from google-ads>=26.0.0->apache-airflow-providers-google) (1.73.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.59.0 in /usr/local/lib/python3.11/dist-packages (from google-ads>=26.0.0->apache-airflow-providers-google) (1.71.0)\n",
            "Requirement already satisfied: PyYAML<7.0,>=5.1 in /usr/local/lib/python3.11/dist-packages (from google-ads>=26.0.0->apache-airflow-providers-google) (6.0.2)\n",
            "Requirement already satisfied: protobuf<7.0.0,>=4.25.0 in /usr/local/lib/python3.11/dist-packages (from google-ads>=26.0.0->apache-airflow-providers-google) (5.29.5)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=2.0.2->apache-airflow-providers-google) (0.22.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=2.0.2->apache-airflow-providers-google) (4.2.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.29.0->apache-airflow-providers-google) (5.5.2)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform>=1.73.0->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (1.14.2)\n",
            "Requirement already satisfied: shapely<3.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform>=1.73.0->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (2.1.1)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform>=1.73.0->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (1.21.1)\n",
            "Requirement already satisfied: docstring_parser<1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform>=1.73.0->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (0.16)\n",
            "Collecting setuptools<70.0.0 (from google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google)\n",
            "  Downloading setuptools-69.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting ray<=2.42.0,>=2.5 (from ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google)\n",
            "  Downloading ray-2.42.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: google-cloud-bigquery-storage in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (2.32.0)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.23.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (1.6.1)\n",
            "Collecting ruamel.yaml (from google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google)\n",
            "  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-alloydb>=0.4.0->apache-airflow-providers-google) (0.14.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery!=3.21.*,!=3.22.0,!=3.23.*,>=3.4.0->apache-airflow-providers-google) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery!=3.21.*,!=3.22.0,!=3.23.*,>=3.4.0->apache-airflow-providers-google) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0.0dev,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigtable>=2.17.0->apache-airflow-providers-google) (1.7.1)\n",
            "Collecting google-cloud-appengine-logging<2.0.0,>=0.1.3 (from google-cloud-logging>=3.5.0->apache-airflow-providers-google)\n",
            "  Downloading google_cloud_appengine_logging-1.6.2-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting google-cloud-audit-log<1.0.0,>=0.3.1 (from google-cloud-logging>=3.5.0->apache-airflow-providers-google)\n",
            "  Downloading google_cloud_audit_log-0.3.2-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.27.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-pubsub>=2.21.3->apache-airflow-providers-google) (1.34.1)\n",
            "Requirement already satisfied: grpc-interceptor>=0.15.4 in /usr/local/lib/python3.11/dist-packages (from google-cloud-spanner!=3.49.0,>=3.11.1->apache-airflow-providers-google) (0.15.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->apache-airflow-providers-google) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->apache-airflow-providers-google) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->apache-airflow-providers-google) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->apache-airflow-providers-google) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->apache-airflow-providers-google) (0.16.0)\n",
            "Collecting cattrs>=1.3 (from looker-sdk!=24.18.0,>=22.4.0->apache-airflow-providers-google)\n",
            "  Downloading cattrs-25.1.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: db-dtypes<2.0.0,>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from pandas-gbq>=0.7.0->apache-airflow-providers-google) (1.4.3)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.11/dist-packages (from pandas-gbq>=0.7.0->apache-airflow-providers-google) (2.0.2)\n",
            "Requirement already satisfied: pydata-google-auth>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from pandas-gbq>=0.7.0->apache-airflow-providers-google) (1.9.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify>=7.0.0->apache-airflow-providers-google) (1.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.0->gcloud-aio-auth>=5.2.0->apache-airflow-providers-google) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.0->gcloud-aio-auth>=5.2.0->apache-airflow-providers-google) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.0->gcloud-aio-auth>=5.2.0->apache-airflow-providers-google) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.0->gcloud-aio-auth>=5.2.0->apache-airflow-providers-google) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.0->gcloud-aio-auth>=5.2.0->apache-airflow-providers-google) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.0->gcloud-aio-auth>=5.2.0->apache-airflow-providers-google) (1.20.1)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic<2.0,>=1.13.1->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.1.3)\n",
            "Requirement already satisfied: aiologic>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.14.0)\n",
            "Requirement already satisfied: msgspec>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.19.0)\n",
            "Requirement already satisfied: retryhttp!=1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.3.3)\n",
            "Requirement already satisfied: structlog>=25.4.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow>=2.10.0->apache-airflow-providers-google) (25.4.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=41.0.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.17.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib->gcsfs>=2023.10.0->apache-airflow-providers-google) (2.0.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform>=1.73.0->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (15.0.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->apache-airflow-providers-google) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client>=2.0.2->apache-airflow-providers-google) (3.2.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.1->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.1->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.1->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.25.1)\n",
            "Requirement already satisfied: wirerope>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from methodtools>=0.4.7->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.0.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.27.0->google-cloud-pubsub>=2.21.3->apache-airflow-providers-google) (0.55b1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (2025.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules<0.4.2,>=0.2.1->gcloud-aio-storage>=9.0.0->apache-airflow-providers-google) (0.6.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.17.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray<=2.42.0,>=2.5->ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (8.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray<=2.42.0,>=2.5->ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (3.18.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray<=2.42.0,>=2.5->ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (1.1.1)\n",
            "Collecting aiohttp-cors (from ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google)\n",
            "  Downloading aiohttp_cors-0.8.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting colorful (from ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google)\n",
            "  Downloading colorful-0.5.7-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Collecting opencensus (from ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google)\n",
            "  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (0.22.1)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.11/dist-packages (from ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (7.1.0)\n",
            "Collecting virtualenv!=20.21.1,>=20.0.24 (from ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google)\n",
            "  Downloading virtualenv-20.31.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting py-spy>=0.2.0 (from ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google)\n",
            "  Downloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.31.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.31.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.4.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<2.0,>=1.4.49->sqlalchemy[asyncio]<2.0,>=1.4.49->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (3.2.3)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (3.6.0)\n",
            "Requirement already satisfied: wrapt>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from aiologic>=0.14.0->apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.17.2)\n",
            "Requirement already satisfied: starlette>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from cadwyn>=5.2.1->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.46.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=41.0.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.22)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.0.7)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.0.20)\n",
            "Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.2.0)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.34.3)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask>=2.1.1->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.9.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask>=2.1.1->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from flask>=2.1.1->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (3.1.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.5->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (3.23.0)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.11/dist-packages (from linkify-it-py>=2.0.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.0.3)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.34.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs>=2023.10.0->apache-airflow-providers-google) (3.3.1)\n",
            "Requirement already satisfied: types-requests in /usr/local/lib/python3.11/dist-packages (from retryhttp!=1.3.0,>=1.2.0->apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.32.4.20250611)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.6.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (3.0.0)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google) (4.3.8)\n",
            "Collecting opencensus-context>=0.1.3 (from opencensus->ray[default]<=2.42.0,>=2.5; python_version == \"3.11\" and extra == \"ray\"->google-cloud-aiplatform[evaluation,ray]>=1.73.0; python_version < \"3.12\"->apache-airflow-providers-google)\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (2.7.0)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.16.0)\n",
            "Requirement already satisfied: rich-toolkit>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.14.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.6.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.1.2)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.1.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.1.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow>=2.10.0->apache-airflow-providers-google) (1.5.4)\n",
            "Downloading apache_airflow_providers_google-16.0.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gcloud_aio_auth-5.4.2-py3-none-any.whl (16 kB)\n",
            "Downloading gcloud_aio_bigquery-7.1.0-py3-none-any.whl (12 kB)\n",
            "Downloading gcloud_aio_storage-9.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading google_ads-27.0.0-py3-none-any.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m146.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_analytics_admin-0.24.1-py3-none-any.whl (397 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m398.0/398.0 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_alloydb-0.4.8-py3-none-any.whl (520 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.9/520.9 kB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_automl-2.16.4-py3-none-any.whl (367 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.3/367.3 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_batch-0.17.36-py3-none-any.whl (176 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_bigquery_datatransfer-3.19.2-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_bigtable-2.31.0-py3-none-any.whl (488 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.1/488.1 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_build-3.31.2-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.0/180.0 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_compute-1.31.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m122.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_container-2.57.0-py3-none-any.whl (308 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.9/308.9 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_datacatalog-3.27.1-py3-none-any.whl (367 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.6/367.6 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_dataflow_client-0.9.0-py3-none-any.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.3/284.3 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_dataform-0.6.2-py3-none-any.whl (250 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.6/250.6 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_dataplex-2.10.2-py3-none-any.whl (507 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_dataproc_metastore-1.18.3-py3-none-any.whl (433 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.8/433.8 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_dlp-3.31.0-py3-none-any.whl (215 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.7/215.7 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_kms-3.5.1-py3-none-any.whl (270 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.8/270.8 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_logging-3.12.1-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.5/229.5 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_managedkafka-0.1.11-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_memcache-1.12.2-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_monitoring-2.27.2-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_orchestration_airflow-1.17.5-py3-none-any.whl (281 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.0/281.0 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_os_login-2.17.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_pubsub-2.30.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_redis-2.18.1-py3-none-any.whl (145 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.3/145.3 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_run-0.10.18-py3-none-any.whl (333 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.8/333.8 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_secret_manager-2.24.0-py3-none-any.whl (218 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.1/218.1 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_speech-2.33.0-py3-none-any.whl (335 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.7/335.7 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_storage_transfer-1.17.0-py3-none-any.whl (90 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.8/90.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_tasks-2.19.3-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.7/289.7 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_texttospeech-2.27.0-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.4/189.4 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_videointelligence-2.16.2-py3-none-any.whl (275 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_vision-3.10.2-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.9/527.9 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_workflows-1.18.2-py3-none-any.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio_gcp-0.2.2-py2.py3-none-any.whl (9.4 kB)\n",
            "Downloading looker_sdk-25.10.0-py3-none-any.whl (236 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.1/236.1 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sqlalchemy_bigquery-1.15.0-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sqlalchemy_spanner-1.14.0-py3-none-any.whl (31 kB)\n",
            "Downloading types_protobuf-6.30.2.20250516-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading cattrs-25.1.1-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.4/69.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_appengine_logging-1.6.2-py3-none-any.whl (16 kB)\n",
            "Downloading google_cloud_audit_log-0.3.2-py3-none-any.whl (32 kB)\n",
            "Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.42.0-cp311-cp311-manylinux2014_x86_64.whl (67.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.4/67.4 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-69.5.1-py3-none-any.whl (894 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m894.6/894.6 kB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m130.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading virtualenv-20.31.2-py3-none-any.whl (6.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m135.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_cors-0.8.1-py3-none-any.whl (25 kB)\n",
            "Downloading colorful-0.5.7-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.5/201.5 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Installing collected packages: py-spy, opencensus-context, distlib, colorful, virtualenv, types-protobuf, setuptools, ruamel.yaml.clib, pyasn1-modules, grpcio-gcp, cattrs, backoff, ruamel.yaml, looker-sdk, google-cloud-audit-log, gcloud-aio-auth, aiohttp-cors, ray, opencensus, google-ads, gcloud-aio-storage, gcloud-aio-bigquery, google-cloud-workflows, google-cloud-vision, google-cloud-videointelligence, google-cloud-texttospeech, google-cloud-tasks, google-cloud-storage-transfer, google-cloud-speech, google-cloud-secret-manager, google-cloud-run, google-cloud-redis, google-cloud-pubsub, google-cloud-os-login, google-cloud-orchestration-airflow, google-cloud-monitoring, google-cloud-memcache, google-cloud-managedkafka, google-cloud-kms, google-cloud-dlp, google-cloud-dataproc-metastore, google-cloud-dataplex, google-cloud-dataform, google-cloud-dataflow-client, google-cloud-datacatalog, google-cloud-container, google-cloud-compute, google-cloud-build, google-cloud-bigtable, google-cloud-bigquery-datatransfer, google-cloud-batch, google-cloud-automl, google-cloud-appengine-logging, google-cloud-alloydb, google-analytics-admin, sqlalchemy-spanner, sqlalchemy-bigquery, google-cloud-logging, apache-airflow-providers-google\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: pyasn1-modules\n",
            "    Found existing installation: pyasn1_modules 0.4.2\n",
            "    Uninstalling pyasn1_modules-0.4.2:\n",
            "      Successfully uninstalled pyasn1_modules-0.4.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-cors-0.8.1 apache-airflow-providers-google-16.0.0 backoff-2.2.1 cattrs-25.1.1 colorful-0.5.7 distlib-0.3.9 gcloud-aio-auth-5.4.2 gcloud-aio-bigquery-7.1.0 gcloud-aio-storage-9.4.0 google-ads-27.0.0 google-analytics-admin-0.24.1 google-cloud-alloydb-0.4.8 google-cloud-appengine-logging-1.6.2 google-cloud-audit-log-0.3.2 google-cloud-automl-2.16.4 google-cloud-batch-0.17.36 google-cloud-bigquery-datatransfer-3.19.2 google-cloud-bigtable-2.31.0 google-cloud-build-3.31.2 google-cloud-compute-1.31.0 google-cloud-container-2.57.0 google-cloud-datacatalog-3.27.1 google-cloud-dataflow-client-0.9.0 google-cloud-dataform-0.6.2 google-cloud-dataplex-2.10.2 google-cloud-dataproc-metastore-1.18.3 google-cloud-dlp-3.31.0 google-cloud-kms-3.5.1 google-cloud-logging-3.12.1 google-cloud-managedkafka-0.1.11 google-cloud-memcache-1.12.2 google-cloud-monitoring-2.27.2 google-cloud-orchestration-airflow-1.17.5 google-cloud-os-login-2.17.2 google-cloud-pubsub-2.30.0 google-cloud-redis-2.18.1 google-cloud-run-0.10.18 google-cloud-secret-manager-2.24.0 google-cloud-speech-2.33.0 google-cloud-storage-transfer-1.17.0 google-cloud-tasks-2.19.3 google-cloud-texttospeech-2.27.0 google-cloud-videointelligence-2.16.2 google-cloud-vision-3.10.2 google-cloud-workflows-1.18.2 grpcio-gcp-0.2.2 looker-sdk-25.10.0 opencensus-0.11.4 opencensus-context-0.1.3 py-spy-0.4.0 pyasn1-modules-0.4.1 ray-2.42.0 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12 setuptools-69.5.1 sqlalchemy-bigquery-1.15.0 sqlalchemy-spanner-1.14.0 types-protobuf-6.30.2.20250516 virtualenv-20.31.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "airflow",
                  "google",
                  "pkg_resources"
                ]
              },
              "id": "6e153338adb34308ac787b44996aa5b3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5caed29f",
        "outputId": "c21096fa-7568-426b-fd5f-79e7e54a2aaa"
      },
      "source": [
        "from airflow import DAG # Import DAG\n",
        "from datetime import datetime, timedelta # Import necessary datetime and timedelta\n",
        "from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator\n",
        "from google.cloud import bigquery\n",
        "import os\n",
        "import pandas as pd\n",
        "from google.oauth2 import service_account\n",
        "from airflow.providers.standard.operators.python import PythonOperator # Ensure PythonOperator is imported\n",
        "\n",
        "# Define default arguments for the DAG (redefined for this cell)\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2023, 1, 1),\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "# Define the DAG (redefined for this cell)\n",
        "dag = DAG(\n",
        "    'etl_presion_fiscal_sri',\n",
        "    default_args=default_args,\n",
        "    description='ETL pipeline to download, transform, and load SRI tax pressure data to BigQuery',\n",
        "    schedule=None,\n",
        "    catchup=False,\n",
        "    tags=['etl', 'bigquery', 'sri'],\n",
        ")\n",
        "\n",
        "# Set the GOOGLE_APPLICATION_CREDENTIALS environment variable\n",
        "# Use the correct filename provided by the user. Assuming the file is in the root directory of the Colab session.\n",
        "credential_filename = \"presionfiscaletl-ab366d04e3b7.json\" # Use the filename provided by the user\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_filename\n",
        "print(f\"GOOGLE_APPLICATION_CREDENTIALS environment variable set to: {credential_filename}\")\n",
        "\n",
        "# Optional: Print current working directory to help debugging file paths\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "\n",
        "\n",
        "# Load credentials using the environment variable\n",
        "try:\n",
        "    credentials = service_account.Credentials.from_service_account_file(os.environ['GOOGLE_APPLICATION_CREDENTIALS'])\n",
        "    print(\"Google Cloud credentials loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Credential file '{credential_filename}' not found. Please upload it again and ensure the filename is correct.\")\n",
        "    raise # Re-raise the exception if the file is not found\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred loading credentials: {e}\")\n",
        "    raise # Re-raise any other exception\n",
        "\n",
        "\n",
        "# Define BigQuery parameters (replace with your actual values)\n",
        "# Assuming 'dag' object is available from previous cells\n",
        "PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT', credentials.project_id) # Use the project ID from credentials\n",
        "DATASET_ID = \"sri_presion_fiscal\"\n",
        "TABLE_ID = \"sri_presion_fiscal\"\n",
        "# BIGQUERY_CONN_ID is not needed for the PythonOperator using the client directly\n",
        "\n",
        "def load_data_to_bigquery_with_xcom(ti, project_id: str, dataset_id: str, table_id: str):\n",
        "    \"\"\"\n",
        "    Loads a Pandas DataFrame (pulled from XCom) into a Google BigQuery table.\n",
        "    Assumes GOOGLE_APPLICATION_CREDENTIALS environment variable is set or\n",
        "    Airflow connection handles authentication.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Pull the DataFrame from the previous task's XCom\n",
        "        # The key is the task_id of the upstream task\n",
        "        df = ti.xcom_pull(task_ids='transform_data')\n",
        "\n",
        "        if df is None:\n",
        "            # If df is None, it might mean the previous task didn't push anything or failed.\n",
        "            # Check if the previous task ran successfully.\n",
        "            # For this notebook context, we'll just raise an error.\n",
        "             raise ValueError(\"No DataFrame received from the transform_data task via XCom. Check if 'transform_data' task ran and returned a DataFrame.\")\n",
        "\n",
        "\n",
        "        # Initialize BigQuery client\n",
        "        # The client will automatically use credentials from GOOGLE_APPLICATION_CREDENTIALS\n",
        "        # or from the Airflow connection if configured.\n",
        "        # Pass credentials explicitly to the client if needed, or rely on env var.\n",
        "        # Let's explicitly pass credentials for clarity, assuming they are loaded outside this function.\n",
        "        # However, in a standard Airflow environment, relying on the environment variable\n",
        "        # or the Airflow connection is more common for operators.\n",
        "        # Since this is a PythonOperator, let's keep it simple and rely on the environment variable\n",
        "        # which is set up in the Colab environment. The `bigquery.Client()` should pick it up.\n",
        "\n",
        "        client = bigquery.Client(project=project_id)\n",
        "\n",
        "        table_ref = client.dataset(dataset_id).table(table_id)\n",
        "\n",
        "        # Define the schema based on the DataFrame columns and expected types\n",
        "        # This assumes the DataFrame columns are already in the correct order and types\n",
        "        # after the transformation step.\n",
        "        schema = [\n",
        "            bigquery.SchemaField(\"Region\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"País\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"Año\", \"INT64\"), # Use INT64 for pandas Int64 dtype\n",
        "            bigquery.SchemaField(\"%_Presion\", \"FLOAT64\"), # Use FLOAT64 for pandas float64 dtype\n",
        "            bigquery.SchemaField(\"Tipo_Presion\", \"STRING\"),\n",
        "        ]\n",
        "\n",
        "        # Configure the load job\n",
        "        job_config = bigquery.LoadJobConfig(\n",
        "            schema=schema,\n",
        "            write_disposition=\"WRITE_TRUNCATE\",  # Overwrite the table\n",
        "            # You might need to specify source_format if loading from a file,\n",
        "            # but when loading from DataFrame, BigQuery client handles this.\n",
        "        )\n",
        "\n",
        "        # Load data from DataFrame\n",
        "        job = client.load_table_from_dataframe(\n",
        "            df, table_ref, job_config=job_config\n",
        "        )\n",
        "\n",
        "        print(f\"Starting BigQuery load job: {job.job_id}\")\n",
        "        job.result()  # Wait for the job to complete\n",
        "\n",
        "        print(f\"DataFrame successfully loaded to {project_id}.{dataset_id}.{table_id}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data to BigQuery: {e}\")\n",
        "        # In a real Airflow DAG, you might want to raise an exception\n",
        "        raise\n",
        "\n",
        "\n",
        "# Instantiate the PythonOperator for the loading task\n",
        "# We are using XComs to pass the DataFrame from the transform task.\n",
        "\n",
        "load_to_bigquery_task = PythonOperator(\n",
        "    task_id='load_to_bigquery',\n",
        "    python_callable=load_data_to_bigquery_with_xcom,\n",
        "    op_kwargs={\n",
        "        'project_id': PROJECT_ID,\n",
        "        'dataset_id': DATASET_ID,\n",
        "        'table_id': TABLE_ID,\n",
        "        # The DataFrame is passed implicitly via XComs and pulled by the callable function.\n",
        "    },\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "# Add the dependency: transform_data_task must complete before load_to_bigquery_task starts\n",
        "# This dependency also implies that the output of transform_data_task (the DataFrame)\n",
        "# will be available for load_to_bigquery_task via XComs.\n",
        "# This line should be placed after both tasks are defined.\n",
        "# transform_data_task >> load_to_bigquery_task # This dependency will be set in a later step.\n",
        "\n",
        "print(f\"Task '{load_to_bigquery_task.task_id}' defined for loading data to BigQuery.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GOOGLE_APPLICATION_CREDENTIALS environment variable set to: presionfiscaletl-ab366d04e3b7.json\n",
            "Current working directory: /content\n",
            "Google Cloud credentials loaded successfully.\n",
            "Task 'load_to_bigquery' defined for loading data to BigQuery.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "573bf3fc"
      },
      "source": [
        "## Definir dependencias\n",
        "\n",
        "### Subtask:\n",
        "Establecer el orden de ejecución de las tareas (descargar -> transformar -> cargar)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d3e25c9",
        "outputId": "6daf387f-1df2-4b7f-b4ac-115cc7e1cc17"
      },
      "source": [
        "from airflow import DAG\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "from airflow.providers.standard.operators.python import PythonOperator\n",
        "from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator\n",
        "from google.cloud import bigquery\n",
        "import os\n",
        "from google.oauth2 import service_account\n",
        "import requests # Import requests for the download function\n",
        "\n",
        "# Define default arguments for the DAG\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2023, 1, 1),\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "# Define the DAG\n",
        "dag = DAG(\n",
        "    'etl_presion_fiscal_sri',\n",
        "    default_args=default_args,\n",
        "    description='ETL pipeline to download, transform, and load SRI tax pressure data to BigQuery',\n",
        "    schedule=None,\n",
        "    catchup=False,\n",
        "    tags=['etl', 'bigquery', 'sri'],\n",
        ")\n",
        "\n",
        "# Define the URL and local filename for the downloaded CSV\n",
        "csv_url = \"https://www.sri.gob.ec/o/sri-portlet-biblioteca-alfresco-internet/descargar/7e45627e-1f7e-4e21-ae59-d520634fc63f/SRI_Presion_Fiscal.csv\"\n",
        "local_filename = \"SRI_Presion_Fiscal.csv\"\n",
        "\n",
        "\n",
        "# Define the filename for the Google Cloud service account credentials\n",
        "credential_filename = \"presionfiscaletl-ab366d04e3b7.json\" # Use the filename provided by the user\n",
        "# Set the environment variable for Google Cloud authentication\n",
        "# Note: In a real Airflow environment, prefer using Airflow Connections for GCP\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_filename\n",
        "\n",
        "\n",
        "# Define the Python function for the download task\n",
        "def download_csv_from_url(csv_url, local_filename):\n",
        "    \"\"\"Downloads a CSV file from a given URL and saves it locally.\"\"\"\n",
        "    # import requests # Already imported at the top level\n",
        "    print(f\"Attempting to download file from: {csv_url}\")\n",
        "    try:\n",
        "        response = requests.get(csv_url)\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "        with open(local_filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        print(f\"Archivo '{local_filename}' descargado exitosamente.\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error al descargar el archivo: {e}\")\n",
        "        # Raise an exception to fail the task in Airflow\n",
        "        raise ValueError(f\"Download task failed: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during download: {e}\")\n",
        "        raise ValueError(f\"Download task failed unexpectedly: {e}\")\n",
        "\n",
        "\n",
        "# Instantiate the PythonOperator for the download task\n",
        "# Executes the 'download_csv_from_url' function\n",
        "download_csv_task = PythonOperator(\n",
        "    task_id='download_csv', # Unique identifier for the task within the DAG\n",
        "    python_callable=download_csv_from_url, # The Python function to execute\n",
        "    op_kwargs={'csv_url': csv_url, 'local_filename': local_filename}, # Keyword arguments to pass to the function\n",
        "    dag=dag, # Associate the task with the DAG object\n",
        ")\n",
        "\n",
        "\n",
        "# Define the Python function for the data transformation task\n",
        "def transform_data(local_filename):\n",
        "    \"\"\"\n",
        "    Loads data from a CSV, cleans and transforms it, and returns a DataFrame.\n",
        "    This function replicates the successful transformation steps from the notebook.\n",
        "    \"\"\"\n",
        "    # import pandas as pd # Already imported at the top level\n",
        "    print(f\"Attempting to transform data from: {local_filename}\")\n",
        "    try:\n",
        "        # Reload the CSV assuming semicolon as a separator and no header\n",
        "        df = pd.read_csv(local_filename, encoding='latin-1', header=None, sep=';')\n",
        "\n",
        "        # Remove the header row which is present as the first data row\n",
        "        df = df.iloc[1:].copy()\n",
        "\n",
        "        # Reset index after removing the row\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        # Rename columns to meaningful names\n",
        "        df.columns = ['Region', 'País', 'Año', '%_Presion', 'Tipo_Presion']\n",
        "\n",
        "        # Clean up the '%_Presion' column by removing leading/trailing spaces\n",
        "        df['%_Presion'] = df['%_Presion'].astype(str).str.strip()\n",
        "\n",
        "        # Convert data types\n",
        "        # Convert 'Año' to integer, coercing errors to NaN and then to Int64 (allows for NaNs)\n",
        "        df['Año'] = pd.to_numeric(df['Año'], errors='coerce').astype('Int64')\n",
        "\n",
        "        # Convert '%_Presion' to float, replacing comma with dot and coercing errors to NaN\n",
        "        df['%_Presion'] = df['%_Presion'].astype(str).str.replace(',', '.', regex=False)\n",
        "        df['%_Presion'] = pd.to_numeric(df['%_Presion'], errors='coerce')\n",
        "\n",
        "        print(\"Data transformation completed successfully.\")\n",
        "        # In a real Airflow task, avoid printing large DataFrames directly\n",
        "        # display(df.head()) # display() is for notebooks, not standard Python scripts\n",
        "        # print(df.head().to_string()) # Use to_string() for printing in a script\n",
        "\n",
        "        # print(\"\\nTransformed DataFrame info:\")\n",
        "        # df.info() # info() prints to console in a script\n",
        "\n",
        "        # Return the transformed DataFrame so it can be pushed to XCom\n",
        "        return df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{local_filename}' was not found during transformation.\")\n",
        "        raise ValueError(f\"Transform task failed: Input file not found: {local_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during transformation: {e}\")\n",
        "        raise ValueError(f\"Transform task failed unexpectedly: {e}\")\n",
        "\n",
        "\n",
        "# Instantiate the PythonOperator for the transformation task\n",
        "# Executes the 'transform_data' function\n",
        "# XCom is enabled by default for PythonOperator, allowing the returned DataFrame to be pushed\n",
        "transform_data_task = PythonOperator(\n",
        "    task_id='transform_data', # Unique identifier\n",
        "    python_callable=transform_data, # The function to execute\n",
        "    op_kwargs={'local_filename': local_filename}, # Pass the filename\n",
        "    dag=dag, # Associate with the DAG\n",
        ")\n",
        "\n",
        "\n",
        "# Define the Python function for the BigQuery loading task\n",
        "# This function receives the DataFrame from XCom\n",
        "def load_data_to_bigquery_with_xcom(ti, project_id: str, dataset_id: str, table_id: str):\n",
        "    \"\"\"\n",
        "    Loads a Pandas DataFrame (pulled from XCom) into a Google BigQuery table.\n",
        "    Assumes GOOGLE_APPLICATION_CREDENTIALS environment variable is set or\n",
        "    Airflow connection handles authentication.\n",
        "    \"\"\"\n",
        "    # Import inside the function for Airflow execution context isolation\n",
        "    from google.cloud import bigquery\n",
        "    from google.oauth2 import service_account\n",
        "    import os\n",
        "    import pandas as pd\n",
        "\n",
        "    print(f\"Attempting to load data to BigQuery table: {project_id}.{dataset_id}.{table_id}\")\n",
        "\n",
        "    try:\n",
        "        # Pull the DataFrame from the previous task's XCom\n",
        "        # 'ti' (TaskInstance) is automatically passed to the python_callable\n",
        "        df = ti.xcom_pull(task_ids='transform_data')\n",
        "\n",
        "        if df is None:\n",
        "             # If df is None, the previous task might not have returned a DataFrame or failed\n",
        "             raise ValueError(\"No DataFrame received from the 'transform_data' task via XCom. Check the upstream task.\")\n",
        "\n",
        "        print(f\"Successfully pulled DataFrame from XCom with {len(df)} rows.\")\n",
        "\n",
        "        # Initialize BigQuery client\n",
        "        # The client will automatically use credentials from GOOGLE_APPLICATION_CREDENTIALS\n",
        "        # or from the Airflow connection if configured.\n",
        "        client = bigquery.Client(project=project_id)\n",
        "\n",
        "        # Define the target table reference\n",
        "        table_ref = client.dataset(dataset_id).table(table_id)\n",
        "\n",
        "        # Define the schema for the BigQuery table\n",
        "        # Ensure schema matches the DataFrame structure and data types\n",
        "        schema = [\n",
        "            bigquery.SchemaField(\"Region\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"País\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"Año\", \"INT64\"), # BigQuery type for pandas Int64\n",
        "            bigquery.SchemaField(\"%_Presion\", \"FLOAT64\"), # BigQuery type for pandas float64\n",
        "            bigquery.SchemaField(\"Tipo_Presion\", \"STRING\"),\n",
        "        ]\n",
        "\n",
        "        # Configure the load job\n",
        "        job_config = bigquery.LoadJobConfig(\n",
        "            schema=schema, # Specify the schema\n",
        "            write_disposition=\"WRITE_TRUNCATE\",  # Overwrite the table if it exists\n",
        "            # source_format is not needed when loading from DataFrame\n",
        "        )\n",
        "\n",
        "        # Check if the dataset exists, create if not (optional, can also be a separate task)\n",
        "        try:\n",
        "            client.get_dataset(dataset_id)\n",
        "            print(f\"Dataset '{dataset_id}' already exists.\")\n",
        "        except Exception: # Use a more specific exception if possible, e.g., google.api_core.exceptions.NotFound\n",
        "            print(f\"Dataset '{dataset_id}' not found, creating...\")\n",
        "            dataset = bigquery.Dataset(client.dataset(dataset_id))\n",
        "            dataset.location = \"US\" # Set location - adjust as needed\n",
        "            client.create_dataset(dataset, exists_ok=True)\n",
        "            print(f\"Dataset '{dataset_id}' created.\")\n",
        "\n",
        "\n",
        "        # Check if the table exists, create if not (optional, WRITE_TRUNCATE handles existence)\n",
        "        # Using client.create_table with exists_ok=True is a safe way to ensure schema is defined\n",
        "        table = bigquery.Table(table_ref, schema=schema)\n",
        "        try:\n",
        "             client.create_table(table, exists_ok=True)\n",
        "             print(f\"Table '{table_id}' created or already exists with defined schema.\")\n",
        "        except Exception as e:\n",
        "             print(f\"Error ensuring table '{table_id}' exists or is created: {e}\")\n",
        "             # Depending on error, might need to raise or retry\n",
        "\n",
        "\n",
        "        # Load data from DataFrame to BigQuery\n",
        "        # This makes the API request to load the data\n",
        "        job = client.load_table_from_dataframe(\n",
        "            df, table_ref, job_config=job_config\n",
        "        )\n",
        "\n",
        "        print(f\"Starting BigQuery load job: {job.job_id}\")\n",
        "        job.result()  # Wait for the job to complete\n",
        "\n",
        "        print(f\"DataFrame successfully loaded to {project_id}.{dataset_id}.{table_id}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data to BigQuery: {e}\")\n",
        "        # Raise an exception to fail the task in Airflow\n",
        "        raise ValueError(f\"BigQuery load task failed: {e}\")\n",
        "\n",
        "\n",
        "# Define BigQuery parameters for the load task\n",
        "# Get Project ID from the environment variable GOOGLE_CLOUD_PROJECT, falling back to credentials\n",
        "# In a real Airflow environment, prefer using a Variable or Connection\n",
        "PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT', None)\n",
        "if PROJECT_ID is None:\n",
        "    try:\n",
        "        # This part might not run reliably in an Airflow worker unless the key is accessible\n",
        "        credentials_check = service_account.Credentials.from_service_account_file(os.environ['GOOGLE_APPLICATION_CREDENTIALS'])\n",
        "        PROJECT_ID = credentials_check.project_id\n",
        "    except Exception as e:\n",
        "        print(f\"Could not automatically determine Google Cloud Project ID. Please set GOOGLE_CLOUD_PROJECT environment variable or ensure GOOGLE_APPLICATION_CREDENTIALS is correctly set: {e}\")\n",
        "        # Fallback or set a default - In a production DAG, this should likely fail or use an Airflow connection\n",
        "        PROJECT_ID = \"your-gcp-project-id\" # !!! REPLACE WITH YOUR ACTUAL GCP PROJECT ID OR USE AIRFLOW CONNECTION/VARIABLE !!!\n",
        "        print(f\"Using fallback Project ID: {PROJECT_ID}\")\n",
        "\n",
        "\n",
        "DATASET_ID = \"sri_presion_fiscal\" # BigQuery Dataset ID\n",
        "TABLE_ID = \"sri_presion_fiscal\" # BigQuery Table ID\n",
        "\n",
        "\n",
        "# Instantiate the PythonOperator for the BigQuery loading task\n",
        "# Executes the 'load_data_to_bigquery_with_xcom' function\n",
        "# This task implicitly pulls the DataFrame from the upstream task's XCom\n",
        "load_to_bigquery_task = PythonOperator(\n",
        "    task_id='load_to_bigquery', # Unique identifier\n",
        "    python_callable=load_data_to_bigquery_with_xcom, # The function to execute\n",
        "    op_kwargs={\n",
        "        'project_id': PROJECT_ID, # Pass project ID\n",
        "        'dataset_id': DATASET_ID, # Pass dataset ID\n",
        "        'table_id': TABLE_ID, # Pass table ID\n",
        "        # The DataFrame is passed via XComs, no need to pass it here\n",
        "    },\n",
        "    dag=dag, # Associate with the DAG\n",
        ")\n",
        "\n",
        "# Set the order of execution for the tasks using the bitshift operator\n",
        "# This defines the dependency: download_csv_task runs first, then transform_data_task, then load_to_bigquery_task\n",
        "download_csv_task >> transform_data_task >> load_to_bigquery_task\n",
        "\n",
        "# Print a confirmation of the task dependencies (for debugging/verification in a script)\n",
        "print(\"Task dependencies set:\")\n",
        "print(f\"'{download_csv_task.task_id}' >> '{transform_data_task.task_id}' >> '{load_to_bigquery_task.task_id}'\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task dependencies set:\n",
            "'download_csv' >> 'transform_data' >> 'load_to_bigquery'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abde28ee"
      },
      "source": [
        "## Añadir comentarios y documentación\n",
        "\n",
        "### Subtask:\n",
        "Incluir comentarios relevantes en el código del DAG para explicar cada parte (imports, default_args, definición del DAG, funciones de tareas, operadores, dependencias) y añadir una descripción general del DAG."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "id": "8546ae97",
        "outputId": "ea0b95e1-a6ef-4444-d0d8-5f7faaffdc20"
      },
      "source": [
        "# Import necessary modules from Apache Airflow and other libraries\n",
        "from airflow import DAG # DAG object to define the workflow\n",
        "from datetime import datetime, timedelta # For specifying start_date and retry_delay\n",
        "import pandas as pd # For data transformation using DataFrames\n",
        "from airflow.operators.python import PythonOperator # Operator to run Python functions\n",
        "# from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator # While used before, we are now using PythonOperator for loading\n",
        "from google.cloud import bigquery # For interacting with Google BigQuery API\n",
        "import os # For accessing environment variables\n",
        "from google.oauth2 import service_account # For loading Google Cloud service account credentials\n",
        "import requests # For downloading the CSV file\n",
        "\n",
        "# Define default arguments for the DAG\n",
        "# These arguments are passed to all tasks unless overridden\n",
        "default_args = {\n",
        "    'owner': 'airflow', # The owner of the DAG\n",
        "    'depends_on_past': False, # Set to true if you want tasks to wait for the previous run to succeed\n",
        "    'start_date': datetime(2023, 1, 1), # The date when the DAG starts being scheduled\n",
        "    'email_on_failure': False, # Set to true to receive emails on task failure\n",
        "    'email_on_retry': False, # Set to true to receive emails on task retry\n",
        "    'retries': 1, # Number of times to retry a task on failure\n",
        "    'retry_delay': timedelta(minutes=5), # Time to wait between retries\n",
        "}\n",
        "\n",
        "# Define the DAG object\n",
        "# 'etl_presion_fiscal_sri' is the unique DAG ID\n",
        "dag = DAG(\n",
        "    'etl_presion_fiscal_sri',\n",
        "    default_args=default_args, # Apply the default arguments\n",
        "    description='ETL pipeline to download, transform, and load SRI tax pressure data to BigQuery', # A brief description of the DAG\n",
        "    schedule=None, # Set the schedule interval using a cron expression or timedelta, or None for manual runs\n",
        "    catchup=False, # Set to true to run the DAG for past dates starting from the start_date\n",
        "    tags=['etl', 'bigquery', 'sri'], # Tags to help categorize and filter DAGs in the Airflow UI\n",
        ")\n",
        "\n",
        "# Define the local filename for the downloaded CSV\n",
        "local_filename = \"SRI_Presion_Fiscal.csv\"\n",
        "\n",
        "# Define the filename for the Google Cloud service account credentials\n",
        "credential_filename = \"presionfiscaletl-ab366d04e3b7.json\" # !!! Replace with your actual filename !!!\n",
        "# Set the environment variable for Google Cloud authentication\n",
        "# Note: In a real Airflow environment, prefer using Airflow Connections for GCP\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_filename\n",
        "\n",
        "\n",
        "# Define the Python function for the download task\n",
        "def download_csv_from_url(csv_url, local_filename):\n",
        "    \"\"\"Downloads a CSV file from a given URL and saves it locally.\"\"\"\n",
        "    import requests # Import inside the function for Airflow execution context isolation\n",
        "    print(f\"Attempting to download file from: {csv_url}\")\n",
        "    try:\n",
        "        response = requests.get(csv_url)\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "        with open(local_filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        print(f\"Archivo '{local_filename}' descargado exitosamente.\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error al descargar el archivo: {e}\")\n",
        "        # Raise an exception to fail the task in Airflow\n",
        "        raise ValueError(f\"Download task failed: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during download: {e}\")\n",
        "        raise ValueError(f\"Download task failed unexpectedly: {e}\")\n",
        "\n",
        "\n",
        "# Instantiate the PythonOperator for the download task\n",
        "# Executes the 'download_csv_from_url' function\n",
        "download_csv_task = PythonOperator(\n",
        "    task_id='download_csv', # Unique identifier for the task within the DAG\n",
        "    python_callable=download_csv_from_url, # The Python function to execute\n",
        "    op_kwargs={'csv_url': csv_url, 'local_filename': local_filename}, # Keyword arguments to pass to the function\n",
        "    dag=dag, # Associate the task with the DAG object\n",
        ")\n",
        "\n",
        "\n",
        "# Define the Python function for the data transformation task\n",
        "def transform_data(local_filename):\n",
        "    \"\"\"\n",
        "    Loads data from a CSV, cleans and transforms it, and returns a DataFrame.\n",
        "    This function replicates the successful transformation steps from the notebook.\n",
        "    \"\"\"\n",
        "    import pandas as pd # Import inside the function for Airflow execution context isolation\n",
        "    print(f\"Attempting to transform data from: {local_filename}\")\n",
        "    try:\n",
        "        # Reload the CSV assuming semicolon as a separator and no header\n",
        "        df = pd.read_csv(local_filename, encoding='latin-1', header=None, sep=';')\n",
        "\n",
        "        # Remove the header row which is present as the first data row\n",
        "        df = df.iloc[1:].copy()\n",
        "\n",
        "        # Reset index after removing the row\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        # Rename columns to meaningful names\n",
        "        df.columns = ['Region', 'País', 'Año', '%_Presion', 'Tipo_Presion']\n",
        "\n",
        "        # Clean up the '%_Presion' column by removing leading/trailing spaces\n",
        "        df['%_Presion'] = df['%_Presion'].astype(str).str.strip()\n",
        "\n",
        "        # Convert data types\n",
        "        # Convert 'Año' to integer, coercing errors to NaN and then to Int64 (allows for NaNs)\n",
        "        df['Año'] = pd.to_numeric(df['Año'], errors='coerce').astype('Int64')\n",
        "\n",
        "        # Convert '%_Presion' to float, replacing comma with dot and coercing errors to NaN\n",
        "        df['%_Presion'] = df['%_Presion'].astype(str).str.replace(',', '.', regex=False)\n",
        "        df['%_Presion'] = pd.to_numeric(df['%_Presion'], errors='coerce')\n",
        "\n",
        "        print(\"Data transformation completed successfully.\")\n",
        "        # In a real Airflow task, avoid printing large DataFrames directly\n",
        "        # display(df.head()) # display() is for notebooks, not standard Python scripts\n",
        "        # print(df.head().to_string()) # Use to_string() for printing in a script\n",
        "\n",
        "        # print(\"\\nTransformed DataFrame info:\")\n",
        "        # df.info() # info() prints to console in a script\n",
        "\n",
        "        # Return the transformed DataFrame so it can be pushed to XCom\n",
        "        return df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{local_filename}' was not found during transformation.\")\n",
        "        raise ValueError(f\"Transform task failed: Input file not found: {local_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during transformation: {e}\")\n",
        "        raise ValueError(f\"Transform task failed unexpectedly: {e}\")\n",
        "\n",
        "\n",
        "# Instantiate the PythonOperator for the transformation task\n",
        "# Executes the 'transform_data' function\n",
        "# XCom is enabled by default for PythonOperator, allowing the returned DataFrame to be pushed\n",
        "transform_data_task = PythonOperator(\n",
        "    task_id='transform_data', # Unique identifier\n",
        "    python_callable=transform_data, # The function to execute\n",
        "    op_kwargs={'local_filename': local_filename}, # Pass the filename\n",
        "    dag=dag, # Associate with the DAG\n",
        ")\n",
        "\n",
        "\n",
        "# Define the Python function for the BigQuery loading task\n",
        "# This function receives the DataFrame from XCom\n",
        "def load_data_to_bigquery_with_xcom(ti, project_id: str, dataset_id: str, table_id: str):\n",
        "    \"\"\"\n",
        "    Loads a Pandas DataFrame (pulled from XCom) into a Google BigQuery table.\n",
        "    Assumes GOOGLE_APPLICATION_CREDENTIALS environment variable is set or\n",
        "    Airflow connection handles authentication.\n",
        "    \"\"\"\n",
        "    # Import inside the function for Airflow execution context isolation\n",
        "    from google.cloud import bigquery\n",
        "    from google.oauth2 import service_account\n",
        "    import os\n",
        "    import pandas as pd\n",
        "\n",
        "    print(f\"Attempting to load data to BigQuery table: {project_id}.{dataset_id}.{table_id}\")\n",
        "\n",
        "    try:\n",
        "        # Pull the DataFrame from the previous task's XCom\n",
        "        # 'ti' (TaskInstance) is automatically passed to the python_callable\n",
        "        df = ti.xcom_pull(task_ids='transform_data')\n",
        "\n",
        "        if df is None:\n",
        "             # If df is None, the previous task might not have returned a DataFrame or failed\n",
        "             raise ValueError(\"No DataFrame received from the 'transform_data' task via XCom. Check the upstream task.\")\n",
        "\n",
        "        print(f\"Successfully pulled DataFrame from XCom with {len(df)} rows.\")\n",
        "\n",
        "        # Initialize BigQuery client\n",
        "        # The client will automatically use credentials from GOOGLE_APPLICATION_CREDENTIALS\n",
        "        # or from the Airflow connection if configured.\n",
        "        client = bigquery.Client(project=project_id)\n",
        "\n",
        "        # Define the target table reference\n",
        "        table_ref = client.dataset(dataset_id).table(table_id)\n",
        "\n",
        "        # Define the schema for the BigQuery table\n",
        "        # Ensure schema matches the DataFrame structure and data types\n",
        "        schema = [\n",
        "            bigquery.SchemaField(\"Region\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"País\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"Año\", \"INT64\"), # BigQuery type for pandas Int64\n",
        "            bigquery.SchemaField(\"%_Presion\", \"FLOAT64\"), # BigQuery type for pandas float64\n",
        "            bigquery.SchemaField(\"Tipo_Presion\", \"STRING\"),\n",
        "        ]\n",
        "\n",
        "        # Configure the load job\n",
        "        job_config = bigquery.LoadJobConfig(\n",
        "            schema=schema, # Specify the schema\n",
        "            write_disposition=\"WRITE_TRUNCATE\",  # Overwrite the table if it exists\n",
        "            # source_format is not needed when loading from DataFrame\n",
        "        )\n",
        "\n",
        "        # Check if the dataset exists, create if not (optional, can also be a separate task)\n",
        "        try:\n",
        "            client.get_dataset(dataset_id)\n",
        "            print(f\"Dataset '{dataset_id}' already exists.\")\n",
        "        except Exception: # Use a more specific exception if possible, e.g., google.api_core.exceptions.NotFound\n",
        "            print(f\"Dataset '{dataset_id}' not found, creating...\")\n",
        "            dataset = bigquery.Dataset(client.dataset(dataset_id))\n",
        "            dataset.location = \"US\" # Set location - adjust as needed\n",
        "            client.create_dataset(dataset, exists_ok=True)\n",
        "            print(f\"Dataset '{dataset_id}' created.\")\n",
        "\n",
        "\n",
        "        # Check if the table exists, create if not (optional, WRITE_TRUNCATE handles existence)\n",
        "        # Using client.create_table with exists_ok=True is a safe way to ensure schema is defined\n",
        "        table = bigquery.Table(table_ref, schema=schema)\n",
        "        try:\n",
        "             client.create_table(table, exists_ok=True)\n",
        "             print(f\"Table '{table_id}' created or already exists with defined schema.\")\n",
        "        except Exception as e:\n",
        "             print(f\"Error ensuring table '{table_id}' exists or is created: {e}\")\n",
        "             # Depending on error, might need to raise or retry\n",
        "\n",
        "\n",
        "        # Load data from DataFrame to BigQuery\n",
        "        # This makes the API request to load the data\n",
        "        job = client.load_table_from_dataframe(\n",
        "            df, table_ref, job_config=job_config\n",
        "        )\n",
        "\n",
        "        print(f\"Starting BigQuery load job: {job.job_id}\")\n",
        "        job.result()  # Wait for the job to complete\n",
        "\n",
        "        print(f\"DataFrame successfully loaded to {project_id}.{dataset_id}.{table_id}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data to BigQuery: {e}\")\n",
        "        # Raise an exception to fail the task in Airflow\n",
        "        raise ValueError(f\"BigQuery load task failed: {e}\")\n",
        "\n",
        "\n",
        "# Define BigQuery parameters for the load task\n",
        "# Get Project ID from the environment variable GOOGLE_CLOUD_PROJECT, falling back to credentials\n",
        "# In a real Airflow environment, prefer using a Variable or Connection\n",
        "PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT', None)\n",
        "if PROJECT_ID is None:\n",
        "    try:\n",
        "        # This part might not run reliably in an Airflow worker unless the key is accessible\n",
        "        credentials_check = service_account.Credentials.from_service_account_file(os.environ['GOOGLE_APPLICATION_CREDENTIALS'])\n",
        "        PROJECT_ID = credentials_check.project_id\n",
        "    except Exception as e:\n",
        "        print(f\"Could not automatically determine Google Cloud Project ID. Please set GOOGLE_CLOUD_PROJECT environment variable or ensure GOOGLE_APPLICATION_CREDENTIALS is correctly set: {e}\")\n",
        "        # Fallback or set a default - In a production DAG, this should likely fail or use an Airflow connection\n",
        "        PROJECT_ID = \"your-gcp-project-id\" # !!! REPLACE WITH YOUR ACTUAL GCP PROJECT ID OR USE AIRFLOW CONNECTION/VARIABLE !!!\n",
        "        print(f\"Using fallback Project ID: {PROJECT_ID}\")\n",
        "\n",
        "\n",
        "DATASET_ID = \"sri_presion_fiscal\" # BigQuery Dataset ID\n",
        "TABLE_ID = \"sri_presion_fiscal\" # BigQuery Table ID\n",
        "\n",
        "\n",
        "# Instantiate the PythonOperator for the BigQuery loading task\n",
        "# Executes the 'load_data_to_bigquery_with_xcom' function\n",
        "# This task implicitly pulls the DataFrame from the upstream task's XCom\n",
        "load_to_bigquery_task = PythonOperator(\n",
        "    task_id='load_to_bigquery', # Unique identifier\n",
        "    python_callable=load_data_to_bigquery_with_xcom, # The function to execute\n",
        "    op_kwargs={\n",
        "        'project_id': PROJECT_ID, # Pass project ID\n",
        "        'dataset_id': DATASET_ID, # Pass dataset ID\n",
        "        'table_id': TABLE_ID, # Pass table ID\n",
        "        # The DataFrame is passed via XComs, no need to pass it here\n",
        "    },\n",
        "    dag=dag, # Associate with the DAG\n",
        ")\n",
        "\n",
        "# Set the order of execution for the tasks using the bitshift operator\n",
        "# This defines the dependency: download_csv_task runs first, then transform_data_task, then load_to_bigquery_task\n",
        "download_csv_task >> transform_data_task >> load_to_bigquery_task\n",
        "\n",
        "# Print a confirmation of the task dependencies (for debugging/verification in a script)\n",
        "print(\"Task dependencies set:\")\n",
        "print(f\"'{download_csv_task.task_id}' >> '{transform_data_task.task_id}' >> '{load_to_bigquery_task.task_id}'\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m/tmp/\u001b[0m\u001b[1;33mipython-input-10-1936291516.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m5\u001b[0m\u001b[1;33m DeprecationWarning\u001b[0m\u001b[33m: The `airflow.operators.python.PythonOperator` class is deprecated. Please use `\u001b[0m\u001b[33m'airflow.providers.standard.operators.python.PythonOperator'\u001b[0m\u001b[33m`.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/tmp/ipython-input-10-1936291516.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">5</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> DeprecationWarning</span><span style=\"color: #808000; text-decoration-color: #808000\">: The `airflow.operators.python.PythonOperator` class is deprecated. Please use `</span><span style=\"color: #808000; text-decoration-color: #808000\">'airflow.providers.standard.operators.python.PythonOperator'</span><span style=\"color: #808000; text-decoration-color: #808000\">`.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task dependencies set:\n",
            "'download_csv' >> 'transform_data' >> 'load_to_bigquery'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04ad9b34"
      },
      "source": [
        "## Generar el archivo .py del dag\n",
        "\n",
        "### Subtask:\n",
        "Empaquetar el código completo del DAG, incluyendo imports, argumentos por defecto, definición del DAG, funciones de tareas, operadores y dependencias, en un archivo Python (`.py`) listo para ser colocado en la carpeta `dags` de Airflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6589dd12",
        "outputId": "a5c05463-3439-4967-eeec-a78a5ede10dd"
      },
      "source": [
        "%%writefile etl_presion_fiscal_dag.py\n",
        "\n",
        "# Import necessary modules from Apache Airflow and other libraries\n",
        "from airflow import DAG # DAG object to define the workflow\n",
        "from datetime import datetime, timedelta # For specifying start_date and retry_delay\n",
        "import pandas as pd # For data transformation using DataFrames\n",
        "from airflow.operators.python import PythonOperator # Operator to run Python functions\n",
        "# from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator # While used before, we are now using PythonOperator for loading\n",
        "from google.cloud import bigquery # For interacting with Google BigQuery API\n",
        "import os # For accessing environment variables\n",
        "from google.oauth2 import service_account # For loading Google Cloud service account credentials\n",
        "import requests # For downloading the CSV file\n",
        "\n",
        "# Define default arguments for the DAG\n",
        "# These arguments are passed to all tasks unless overridden\n",
        "default_args = {\n",
        "    'owner': 'airflow', # The owner of the DAG\n",
        "    'depends_on_past': False, # Set to true if you want tasks to wait for the previous run to succeed\n",
        "    'start_date': datetime(2023, 1, 1), # The date when the DAG starts being scheduled\n",
        "    'email_on_failure': False, # Set to true to receive emails on task failure\n",
        "    'email_on_retry': False, # Set to true to receive emails on task retry\n",
        "    'retries': 1, # Number of times to retry a task on failure\n",
        "    'retry_delay': timedelta(minutes=5), # Time to wait between retries\n",
        "}\n",
        "\n",
        "# Define the DAG object\n",
        "# 'etl_presion_fiscal_sri' is the unique DAG ID\n",
        "with DAG(\n",
        "    'etl_presion_fiscal_sri',\n",
        "    default_args=default_args, # Apply the default arguments\n",
        "    description='ETL pipeline to download, transform, and load SRI tax pressure data to BigQuery', # A brief description of the DAG\n",
        "    schedule=None, # Set the schedule interval using a cron expression or timedelta, or None for manual runs\n",
        "    catchup=False, # Set to true to run the DAG for past dates starting from the start_date\n",
        "    tags=['etl', 'bigquery', 'sri'], # Tags to help categorize and filter DAGs in the Airflow UI\n",
        ") as dag:\n",
        "\n",
        "    # Define the local filename for the downloaded CSV\n",
        "    local_filename = \"SRI_Presion_Fiscal.csv\"\n",
        "\n",
        "    # Define the filename for the Google Cloud service account credentials\n",
        "    credential_filename = \"presionfiscaletl-ab366d04e3b7.json\" # !!! Replace with your actual filename if different in your Airflow environment !!!\n",
        "    # Set the environment variable for Google Cloud authentication\n",
        "    # Note: In a real Airflow environment, prefer using Airflow Connections for GCP\n",
        "    # If using Airflow Connections, you would remove this os.environ line and rely on the connection setup.\n",
        "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_filename\n",
        "\n",
        "\n",
        "    # Define the Python function for the download task\n",
        "    def download_csv_from_url(csv_url, local_filename):\n",
        "        \"\"\"Downloads a CSV file from a given URL and saves it locally.\"\"\"\n",
        "        # import requests # Already imported at the top level\n",
        "        print(f\"Attempting to download file from: {csv_url}\")\n",
        "        try:\n",
        "            response = requests.get(csv_url)\n",
        "            response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "            with open(local_filename, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "\n",
        "            print(f\"Archivo '{local_filename}' descargado exitosamente.\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error al descargar el archivo: {e}\")\n",
        "            # Raise an exception to fail the task in Airflow\n",
        "            raise ValueError(f\"Download task failed: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred during download: {e}\")\n",
        "            raise ValueError(f\"Download task failed unexpectedly: {e}\")\n",
        "\n",
        "\n",
        "    # Instantiate the PythonOperator for the download task\n",
        "    # Executes the 'download_csv_from_url' function\n",
        "    download_csv_task = PythonOperator(\n",
        "        task_id='download_csv', # Unique identifier for the task within the DAG\n",
        "        python_callable=download_csv_from_url, # The Python function to execute\n",
        "        op_kwargs={'csv_url': csv_url, 'local_filename': local_filename}, # Keyword arguments to pass to the function\n",
        "        # dag=dag, # Associate the task with the DAG object implicitly when using 'with DAG(...)'\n",
        "    )\n",
        "\n",
        "\n",
        "    # Define the Python function for the data transformation task\n",
        "    def transform_data(local_filename):\n",
        "        \"\"\"\n",
        "        Loads data from a CSV, cleans and transforms it, and returns a DataFrame.\n",
        "        This function replicates the successful transformation steps from the notebook.\n",
        "        \"\"\"\n",
        "        # import pandas as pd # Already imported at the top level\n",
        "        print(f\"Attempting to transform data from: {local_filename}\")\n",
        "        try:\n",
        "            # Reload the CSV assuming semicolon as a separator and no header\n",
        "            df = pd.read_csv(local_filename, encoding='latin-1', header=None, sep=';')\n",
        "\n",
        "            # Remove the header row which is present as the first data row\n",
        "            df = df.iloc[1:].copy()\n",
        "\n",
        "            # Reset index after removing the row\n",
        "            df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "            # Rename columns to meaningful names\n",
        "            df.columns = ['Region', 'País', 'Año', '%_Presion', 'Tipo_Presion']\n",
        "\n",
        "            # Clean up the '%_Presion' column by removing leading/trailing spaces\n",
        "            df['%_Presion'] = df['%_Presion'].astype(str).str.strip()\n",
        "\n",
        "            # Convert data types\n",
        "            # Convert 'Año' to integer, coercing errors to NaN and then to Int64 (allows for NaNs)\n",
        "            df['Año'] = pd.to_numeric(df['Año'], errors='coerce').astype('Int64')\n",
        "\n",
        "            # Convert '%_Presion' to float, replacing comma with dot and coercing errors to NaN\n",
        "            df['%_Presion'] = df['%_Presion'].astype(str).str.replace(',', '.', regex=False)\n",
        "            df['%_Presion'] = pd.to_numeric(df['%_Presion'], errors='coerce')\n",
        "\n",
        "            print(\"Data transformation completed successfully.\")\n",
        "            # In a real Airflow task, avoid printing large DataFrames directly\n",
        "            # display(df.head()) # display() is for notebooks, not standard Python scripts\n",
        "            # print(df.head().to_string()) # Use to_string() for printing in a script\n",
        "\n",
        "            # print(\"\\nTransformed DataFrame info:\")\n",
        "            # df.info() # info() prints to console in a script\n",
        "\n",
        "            # Return the transformed DataFrame so it can be pushed to XCom\n",
        "            return df\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: The file '{local_filename}' was not found during transformation.\")\n",
        "            raise ValueError(f\"Transform task failed: Input file not found: {local_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred during transformation: {e}\")\n",
        "            raise ValueError(f\"Transform task failed unexpectedly: {e}\")\n",
        "\n",
        "\n",
        "    # Instantiate the PythonOperator for the transformation task\n",
        "    # Executes the 'transform_data' function\n",
        "    # XCom is enabled by default for PythonOperator, allowing the returned DataFrame to be pushed\n",
        "    transform_data_task = PythonOperator(\n",
        "        task_id='transform_data', # Unique identifier\n",
        "        python_callable=transform_data, # The function to execute\n",
        "        op_kwargs={'local_filename': local_filename}, # Pass the filename\n",
        "        # dag=dag, # Associate with the DAG implicitly\n",
        "    )\n",
        "\n",
        "\n",
        "    # Define the Python function for the BigQuery loading task\n",
        "    # This function receives the DataFrame from XCom\n",
        "    def load_data_to_bigquery_with_xcom(ti, project_id: str, dataset_id: str, table_id: str):\n",
        "        \"\"\"\n",
        "        Loads a Pandas DataFrame (pulled from XCom) into a Google BigQuery table.\n",
        "        Assumes GOOGLE_APPLICATION_CREDENTIALS environment variable is set or\n",
        "        Airflow connection handles authentication.\n",
        "        \"\"\"\n",
        "        # Import inside the function for Airflow execution context isolation\n",
        "        # from google.cloud import bigquery # Already imported at top level\n",
        "        # from google.oauth2 import service_account # Already imported at top level\n",
        "        # import os # Already imported at top level\n",
        "        # import pandas as pd # Already imported at top level\n",
        "\n",
        "        print(f\"Attempting to load data to BigQuery table: {project_id}.{dataset_id}.{table_id}\")\n",
        "\n",
        "        try:\n",
        "            # Pull the DataFrame from the previous task's XCom\n",
        "            # 'ti' (TaskInstance) is automatically passed to the python_callable\n",
        "            df = ti.xcom_pull(task_ids='transform_data')\n",
        "\n",
        "            if df is None:\n",
        "                 # If df is None, the previous task might not have returned a DataFrame or failed\n",
        "                 raise ValueError(\"No DataFrame received from the 'transform_data' task via XCom. Check the upstream task.\")\n",
        "\n",
        "            print(f\"Successfully pulled DataFrame from XCom with {len(df)} rows.\")\n",
        "\n",
        "            # Initialize BigQuery client\n",
        "            # The client will automatically use credentials from GOOGLE_APPLICATION_CREDENTIALS\n",
        "            # or from the Airflow connection if configured.\n",
        "            client = bigquery.Client(project=project_id)\n",
        "\n",
        "            # Define the target table reference\n",
        "            table_ref = client.dataset(dataset_id).table(table_id)\n",
        "\n",
        "            # Define the schema for the BigQuery table\n",
        "            # Ensure schema matches the DataFrame structure and data types\n",
        "            schema = [\n",
        "                bigquery.SchemaField(\"Region\", \"STRING\"),\n",
        "                bigquery.SchemaField(\"País\", \"STRING\"),\n",
        "                bigquery.SchemaField(\"Año\", \"INT64\"), # BigQuery type for pandas Int64\n",
        "                bigquery.SchemaField(\"%_Presion\", \"FLOAT64\"), # BigQuery type for pandas float64\n",
        "                bigquery.SchemaField(\"Tipo_Presion\", \"STRING\"),\n",
        "            ]\n",
        "\n",
        "            # Configure the load job\n",
        "            job_config = bigquery.LoadJobConfig(\n",
        "                schema=schema, # Specify the schema\n",
        "                write_disposition=\"WRITE_TRUNCATE\",  # Overwrite the table if it exists\n",
        "                # source_format is not needed when loading from DataFrame\n",
        "            )\n",
        "\n",
        "            # Check if the dataset exists, create if not (optional, can also be a separate task)\n",
        "            try:\n",
        "                client.get_dataset(dataset_id)\n",
        "                print(f\"Dataset '{dataset_id}' already exists.\")\n",
        "            except Exception: # Use a more specific exception if possible, e.g., google.api_core.exceptions.NotFound\n",
        "                print(f\"Dataset '{dataset_id}' not found, creating...\")\n",
        "                dataset = bigquery.Dataset(client.dataset(dataset_id))\n",
        "                dataset.location = \"US\" # Set location - adjust as needed\n",
        "                client.create_dataset(dataset, exists_ok=True)\n",
        "                print(f\"Dataset '{dataset_id}' created.\")\n",
        "\n",
        "\n",
        "            # Check if the table exists, create if not (optional, WRITE_TRUNCATE handles existence)\n",
        "            # Using client.create_table with exists_ok=True is a safe way to ensure schema is defined\n",
        "            table = bigquery.Table(table_ref, schema=schema)\n",
        "            try:\n",
        "                 client.create_table(table, exists_ok=True)\n",
        "                 print(f\"Table '{table_id}' created or already exists with defined schema.\")\n",
        "            except Exception as e:\n",
        "                 print(f\"Error ensuring table '{table_id}' exists or is created: {e}\")\n",
        "                 # Depending on error, might need to raise or retry\n",
        "\n",
        "\n",
        "            # Load data from DataFrame to BigQuery\n",
        "            # This makes the API request to load the data\n",
        "            job = client.load_table_from_dataframe(\n",
        "                df, table_ref, job_config=job_config\n",
        "            )\n",
        "\n",
        "            print(f\"Starting BigQuery load job: {job.job_id}\")\n",
        "            job.result()  # Wait for the job to complete\n",
        "\n",
        "            print(f\"DataFrame successfully loaded to {project_id}.{dataset_id}.{table_id}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data to BigQuery: {e}\")\n",
        "            # Raise an exception to fail the task in Airflow\n",
        "            raise ValueError(f\"BigQuery load task failed: {e}\")\n",
        "\n",
        "\n",
        "    # Define BigQuery parameters for the load task\n",
        "    # Get Project ID from the environment variable GOOGLE_CLOUD_PROJECT, falling back to credentials\n",
        "    # In a real Airflow environment, prefer using a Variable or Connection\n",
        "    PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT', None)\n",
        "    if PROJECT_ID is None:\n",
        "        try:\n",
        "            # This part might not run reliably in an Airflow worker unless the key is accessible\n",
        "            credentials_check = service_account.Credentials.from_service_account_file(os.environ['GOOGLE_APPLICATION_CREDENTIALS'])\n",
        "            PROJECT_ID = credentials_check.project_id\n",
        "        except Exception as e:\n",
        "            print(f\"Could not automatically determine Google Cloud Project ID. Please set GOOGLE_CLOUD_PROJECT environment variable or ensure GOOGLE_APPLICATION_CREDENTIALS is correctly set: {e}\")\n",
        "            # Fallback or set a default - In a production DAG, this should likely fail or use an Airflow connection\n",
        "            PROJECT_ID = \"your-gcp-project-id\" # !!! REPLACE WITH YOUR ACTUAL GCP PROJECT ID OR USE AIRFLOW CONNECTION/VARIABLE !!!\n",
        "            print(f\"Using fallback Project ID: {PROJECT_ID}\")\n",
        "\n",
        "\n",
        "    DATASET_ID = \"sri_presion_fiscal\" # BigQuery Dataset ID\n",
        "    TABLE_ID = \"sri_presion_fiscal\" # BigQuery Table ID\n",
        "\n",
        "\n",
        "    # Instantiate the PythonOperator for the BigQuery loading task\n",
        "    # Executes the 'load_data_to_bigquery_with_xcom' function\n",
        "    # This task implicitly pulls the DataFrame from the upstream task's XCom\n",
        "    load_to_bigquery_task = PythonOperator(\n",
        "        task_id='load_to_bigquery', # Unique identifier\n",
        "        python_callable=load_data_to_bigquery_with_xcom, # The function to execute\n",
        "        op_kwargs={\n",
        "            'project_id': PROJECT_ID, # Pass project ID\n",
        "            'dataset_id': DATASET_ID, # Pass dataset ID\n",
        "            'table_id': TABLE_ID, # Pass table ID\n",
        "            # The DataFrame is passed via XComs, no need to pass it here\n",
        "        },\n",
        "        # dag=dag, # Associate with the DAG implicitly\n",
        "    )\n",
        "\n",
        "    # Set the order of execution for the tasks using the bitshift operator\n",
        "    # This defines the dependency: download_csv_task runs first, then transform_data_task, then load_to_bigquery_task\n",
        "    download_csv_task >> transform_data_task >> load_to_bigquery_task\n",
        "\n",
        "    # Print a confirmation of the task dependencies (for debugging/verification in a script)\n",
        "    print(\"Task dependencies set:\")\n",
        "    print(f\"'{download_csv_task.task_id}' >> '{transform_data_task.task_id}' >> '{load_to_bigquery_task.task_id}'\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing etl_presion_fiscal_dag.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbdf8580"
      },
      "source": [
        "# Task\n",
        "Modifica el DAG de Airflow existente para incluir las fases de ETL de dimensiones y hechos, extrayendo datos de la tabla `sri_presion_fiscal` en BigQuery, transformándolos y cargándolos en tablas de dimensiones y hechos separadas en BigQuery. Asegúrate de que el DAG maneje las dependencias correctamente y que el código Python para el ETL de dimensiones y hechos esté incluido."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5537236"
      },
      "source": [
        "## Etl de dimensiones (fase 2)\n",
        "\n",
        "### Subtask:\n",
        "Definir esquemas para las tablas de dimensiones (por ejemplo, Dim_Region, Dim_Pais, Dim_Tiempo, Dim_TipoPresion), extraer datos únicos y relevantes de la tabla `sri_presion_fiscal` para cada dimensión, limpiar y estandarizar los datos de dimensión si es necesario, generar claves subrogadas únicas para cada dimensión y cargar los datos en las respectivas tablas de dimensiones en BigQuery.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47857d85"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the schemas for the dimension tables, initialize the BigQuery client, and create the dataset and dimension tables in BigQuery. This involves defining the schema fields and using the `bigquery.Client`, `bigquery.Dataset`, and `bigquery.Table` objects, along with `client.create_dataset` and `client.create_table`. I will also include the credential loading and project ID definition from the previous successful steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "113e440d",
        "outputId": "3f174a20-c7d7-49cf-d954-406bab7dbb09"
      },
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "import os\n",
        "\n",
        "# Define BigQuery parameters\n",
        "# Use the correct filename for the Google Cloud service account credentials provided by the user.\n",
        "credential_filename = \"presionfiscaletl-ab366d04e3b7.json\" # Use the filename provided by the user\n",
        "# Set the environment variable for Google Cloud authentication\n",
        "# Note: In a real Airflow environment, prefer using Airflow Connections for GCP\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_filename\n",
        "print(f\"GOOGLE_APPLICATION_CREDENTIALS environment variable set to: {credential_filename}\")\n",
        "\n",
        "# Load credentials using the environment variable\n",
        "try:\n",
        "    credentials = service_account.Credentials.from_service_account_file(os.environ['GOOGLE_APPLICATION_CREDENTIALS'])\n",
        "    print(\"Google Cloud credentials loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Credential file '{credential_filename}' not found. Please upload it again and ensure the filename is correct.\")\n",
        "    raise # Re-raise the exception if the file is not found\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred loading credentials: {e}\")\n",
        "    raise # Re-raise any other exception\n",
        "\n",
        "# Get Project ID from the environment variable GOOGLE_CLOUD_PROJECT, falling back to credentials\n",
        "PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT', None)\n",
        "if PROJECT_ID is None:\n",
        "    try:\n",
        "        PROJECT_ID = credentials.project_id\n",
        "    except Exception as e:\n",
        "        print(f\"Could not automatically determine Google Cloud Project ID. Please set GOOGLE_CLOUD_PROJECT environment variable or ensure GOOGLE_APPLICATION_CREDENTIALS is correctly set: {e}\")\n",
        "        # Fallback or set a default - In a production DAG, this should likely fail or use an Airflow connection\n",
        "        PROJECT_ID = \"your-gcp-project-id\" # !!! REPLACE WITH YOUR ACTUAL GCP PROJECT ID OR USE AIRFLOW CONNECTION/VARIABLE !!!\n",
        "        print(f\"Using fallback Project ID: {PROJECT_ID}\")\n",
        "\n",
        "DATASET_ID = \"sri_presion_fiscal\" # BigQuery Dataset ID\n",
        "SOURCE_TABLE_ID = f\"{PROJECT_ID}.{DATASET_ID}.sri_presion_fiscal\" # Source table for dimension data\n",
        "\n",
        "# Initialize BigQuery client\n",
        "client = bigquery.Client(project=PROJECT_ID, credentials=credentials)\n",
        "print(f\"BigQuery client initialized for project: {PROJECT_ID}\")\n",
        "\n",
        "# Create or get the BigQuery dataset\n",
        "dataset_ref = client.dataset(DATASET_ID)\n",
        "dataset = bigquery.Dataset(dataset_ref)\n",
        "dataset.location = \"US\" # Set location - adjust as needed\n",
        "\n",
        "try:\n",
        "    dataset = client.create_dataset(dataset, exists_ok=True)\n",
        "    print(f\"Dataset '{DATASET_ID}' created or already exists.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating dataset '{DATASET_ID}': {e}\")\n",
        "    raise # Re-raise the exception\n",
        "\n",
        "# Define schemas for dimension tables\n",
        "schema_dim_region = [\n",
        "    bigquery.SchemaField(\"region_id\", \"INT64\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"region_nombre\", \"STRING\", mode=\"REQUIRED\"),\n",
        "]\n",
        "\n",
        "schema_dim_pais = [\n",
        "    bigquery.SchemaField(\"pais_id\", \"INT64\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"pais_nombre\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"region_id\", \"INT64\", mode=\"REQUIRED\"),\n",
        "]\n",
        "\n",
        "schema_dim_tiempo = [\n",
        "    bigquery.SchemaField(\"tiempo_id\", \"INT64\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"año\", \"INT64\", mode=\"REQUIRED\"),\n",
        "]\n",
        "\n",
        "schema_dim_tipopresion = [\n",
        "    bigquery.SchemaField(\"tipo_presion_id\", \"INT64\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"tipo_presion_nombre\", \"STRING\", mode=\"REQUIRED\"),\n",
        "]\n",
        "\n",
        "# Define table IDs for dimension tables\n",
        "table_id_dim_region = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Region\"\n",
        "table_id_dim_pais = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Pais\"\n",
        "table_id_dim_tiempo = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Tiempo\"\n",
        "table_id_dim_tipopresion = f\"{PROJECT_ID}.{DATASET_ID}.Dim_TipoPresion\"\n",
        "\n",
        "# Create or get each dimension table in BigQuery\n",
        "def create_or_get_table(client, table_id, schema):\n",
        "    \"\"\"Creates a BigQuery table if it doesn't exist, or gets it if it does.\"\"\"\n",
        "    table_ref = bigquery.Table(table_id, schema=schema)\n",
        "    try:\n",
        "        table = client.create_table(table_ref, exists_ok=True)\n",
        "        print(f\"Table '{table_id}' created or already exists.\")\n",
        "        return table\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating table '{table_id}': {e}\")\n",
        "        raise\n",
        "\n",
        "table_dim_region = create_or_get_table(client, table_id_dim_region, schema_dim_region)\n",
        "table_dim_pais = create_or_get_table(client, table_id_dim_pais, schema_dim_pais)\n",
        "table_dim_tiempo = create_or_get_table(client, table_id_dim_tiempo, schema_dim_tiempo)\n",
        "table_dim_tipopresion = create_or_get_table(client, table_id_dim_tipopresion, schema_dim_tipopresion)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GOOGLE_APPLICATION_CREDENTIALS environment variable set to: presionfiscaletl-ab366d04e3b7.json\n",
            "Google Cloud credentials loaded successfully.\n",
            "BigQuery client initialized for project: presionfiscaletl\n",
            "Dataset 'sri_presion_fiscal' created or already exists.\n",
            "Table 'presionfiscaletl.sri_presion_fiscal.Dim_Region' created or already exists.\n",
            "Table 'presionfiscaletl.sri_presion_fiscal.Dim_Pais' created or already exists.\n",
            "Table 'presionfiscaletl.sri_presion_fiscal.Dim_Tiempo' created or already exists.\n",
            "Table 'presionfiscaletl.sri_presion_fiscal.Dim_TipoPresion' created or already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03c71a99"
      },
      "source": [
        "**Reasoning**:\n",
        "The BigQuery dataset and dimension tables have been created or retrieved. Now I need to extract unique regions from the source table, create a DataFrame for `Dim_Region`, add a surrogate key, and load it into the `Dim_Region` BigQuery table. This involves writing a SQL query, executing it with the BigQuery client, creating a pandas DataFrame, adding an ID column, and loading the DataFrame to BigQuery.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "02b18566",
        "outputId": "c2a19b12-fa50-46b0-e621-57e52ed14d57"
      },
      "source": [
        "# Write a SQL query to select distinct regions\n",
        "query_regions = f\"\"\"\n",
        "SELECT DISTINCT Region\n",
        "FROM `{SOURCE_TABLE_ID}`\n",
        "WHERE Region IS NOT NULL AND Region != 'Region' # Exclude header row if present in data\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query and load results into a DataFrame for Dim_Region\n",
        "try:\n",
        "    df_region = client.query(query_regions).to_dataframe()\n",
        "    print(f\"\\nExtracted {len(df_region)} unique regions.\")\n",
        "    display(df_region.head())\n",
        "\n",
        "    # Add a region_id column (surrogate key)\n",
        "    df_region['region_id'] = df_region.reset_index().index + 1 # Simple 1-based integer ID\n",
        "\n",
        "    # Rename the region column to match the schema\n",
        "    df_region.rename(columns={'Region': 'region_nombre'}, inplace=True)\n",
        "\n",
        "    # Select only the required columns and ensure order\n",
        "    df_region = df_region[['region_id', 'region_nombre']]\n",
        "\n",
        "    # Load the Dim_Region DataFrame into the BigQuery table\n",
        "    job_config_region = bigquery.LoadJobConfig(\n",
        "        schema=schema_dim_region, # Use the defined schema\n",
        "        write_disposition=\"WRITE_TRUNCATE\", # Overwrite the table\n",
        "    )\n",
        "\n",
        "    job_region = client.load_table_from_dataframe(\n",
        "        df_region, table_id_dim_region, job_config=job_config_region\n",
        "    )\n",
        "\n",
        "    print(f\"\\nStarting BigQuery load job for Dim_Region: {job_region.job_id}\")\n",
        "    job_region.result() # Wait for the job to complete\n",
        "\n",
        "    print(f\"DataFrame successfully loaded to {table_id_dim_region}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error processing Dim_Region: {e}\")\n",
        "    raise # Re-raise the exception\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing Dim_Region: 404 Not found: Table presionfiscaletl:sri_presion_fiscal.sri_presion_fiscal was not found in location US; reason: notFound, message: Not found: Table presionfiscaletl:sri_presion_fiscal.sri_presion_fiscal was not found in location US\n",
            "\n",
            "Location: US\n",
            "Job ID: 8076e073-62bc-4b39-b800-42b716919b52\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotFound",
          "evalue": "404 Not found: Table presionfiscaletl:sri_presion_fiscal.sri_presion_fiscal was not found in location US; reason: notFound, message: Not found: Table presionfiscaletl:sri_presion_fiscal.sri_presion_fiscal was not found in location US\n\nLocation: US\nJob ID: 8076e073-62bc-4b39-b800-42b716919b52\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-13-1148983056.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Execute the query and load results into a DataFrame for Dim_Region\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdf_region\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_regions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nExtracted {len(df_region)} unique regions.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_region\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mto_dataframe\u001b[0;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, max_results, geography_as_object, bool_dtype, int_dtype, float_dtype, string_dtype, date_dtype, datetime_dtype, time_dtype, timestamp_dtype, range_date_dtype, range_datetime_dtype, range_timestamp_dtype)\u001b[0m\n\u001b[1;32m   2072\u001b[0m                 \u001b[0;34m:\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mshapely\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mlibrary\u001b[0m \u001b[0mcannot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mimported\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2073\u001b[0m         \"\"\"\n\u001b[0;32m-> 2074\u001b[0;31m         \u001b[0mquery_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait_for_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2075\u001b[0m         return query_result.to_dataframe(\n\u001b[1;32m   2076\u001b[0m             \u001b[0mbqstorage_client\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbqstorage_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/_tqdm_helpers.py\u001b[0m in \u001b[0;36mwait_for_query\u001b[0;34m(query_job, progress_bar_type, max_results)\u001b[0m\n\u001b[1;32m    105\u001b[0m     )\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprogress_bar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mquery_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, page_size, max_results, retry, timeout, start_index, job_retry)\u001b[0m\n\u001b[1;32m   1694\u001b[0m                 \u001b[0;31m# Since is_job_done() calls jobs.getQueryResults, which is a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m                 \u001b[0;31m# long-running API, don't delay the next request at all.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1696\u001b[0;31m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_job_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1697\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mis_job_done\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1643\u001b[0m                         \u001b[0;31m# `job_retry` predicate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m                         \u001b[0mrestart_query_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mjob_failed_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m                         \u001b[0;31m# Make sure that the _query_results are cached so we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFound\u001b[0m: 404 Not found: Table presionfiscaletl:sri_presion_fiscal.sri_presion_fiscal was not found in location US; reason: notFound, message: Not found: Table presionfiscaletl:sri_presion_fiscal.sri_presion_fiscal was not found in location US\n\nLocation: US\nJob ID: 8076e073-62bc-4b39-b800-42b716919b52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ced6ef3"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the source table `presionfiscaletl.sri_presion_fiscal.sri_presion_fiscal` was not found in BigQuery. This indicates that the data loading step into the source table might not have been successful or the table name/location is incorrect. Looking back at the notebook history, the data was loaded into `etl-sistemas-sri.sri_presion_fiscal.sri_presion_fiscal`. The project ID in the error message (`presionfiscaletl`) is different from the one where the data was loaded (`etl-sistemas-sri`). I need to correct the `PROJECT_ID` used for the source table query to match where the data was actually loaded. I will update the `PROJECT_ID` to 'etl-sistemas-sri' specifically for querying the source table, while keeping the destination project ID potentially configurable. I will also redefine the `SOURCE_TABLE_ID` using the correct project ID. After correcting the project ID for the source table, I will re-execute the code block to extract and load the Dim_Region data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "8NZODk0X49u7",
        "outputId": "2ad5b9cd-6639-412f-cc82-4c04e2c119b7"
      },
      "source": [
        "# Correct the Project ID for the source table to match where the data was loaded\n",
        "SOURCE_PROJECT_ID = 'etl-sistemas-sri' # Use the project ID where the source data was loaded\n",
        "\n",
        "# Redefine the source table ID using the correct source project ID\n",
        "SOURCE_TABLE_ID = f\"{SOURCE_PROJECT_ID}.{DATASET_ID}.sri_presion_fiscal\" # Source table for dimension data\n",
        "\n",
        "# Write a SQL query to select distinct regions\n",
        "query_regions = f\"\"\"\n",
        "SELECT DISTINCT Region\n",
        "FROM `{SOURCE_TABLE_ID}`\n",
        "WHERE Region IS NOT NULL AND Region != 'Region' # Exclude header row if present in data\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query and load results into a DataFrame for Dim_Region\n",
        "try:\n",
        "    df_region = client.query(query_regions).to_dataframe()\n",
        "    print(f\"\\nExtracted {len(df_region)} unique regions.\")\n",
        "    display(df_region.head())\n",
        "\n",
        "    # Add a region_id column (surrogate key)\n",
        "    df_region['region_id'] = df_region.reset_index().index + 1 # Simple 1-based integer ID\n",
        "\n",
        "    # Rename the region column to match the schema\n",
        "    df_region.rename(columns={'Region': 'region_nombre'}, inplace=True)\n",
        "\n",
        "    # Select only the required columns and ensure order\n",
        "    df_region = df_region[['region_id', 'region_nombre']]\n",
        "\n",
        "    # Load the Dim_Region DataFrame into the BigQuery table\n",
        "    job_config_region = bigquery.LoadJobConfig(\n",
        "        schema=schema_dim_region, # Use the defined schema\n",
        "        write_disposition=\"WRITE_TRUNCATE\", # Overwrite the table\n",
        "    )\n",
        "\n",
        "    job_region = client.load_table_from_dataframe(\n",
        "        df_region, table_id_dim_region, job_config=job_config_region\n",
        "    )\n",
        "\n",
        "    print(f\"\\nStarting BigQuery load job for Dim_Region: {job_region.job_id}\")\n",
        "    job_region.result() # Wait for the job to complete\n",
        "\n",
        "    print(f\"DataFrame successfully loaded to {table_id_dim_region}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error processing Dim_Region: {e}\")\n",
        "    raise # Re-raise the exception"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing Dim_Region: 403 Access Denied: Table etl-sistemas-sri:sri_presion_fiscal.sri_presion_fiscal: User does not have permission to query table etl-sistemas-sri:sri_presion_fiscal.sri_presion_fiscal, or perhaps it does not exist.; reason: accessDenied, message: Access Denied: Table etl-sistemas-sri:sri_presion_fiscal.sri_presion_fiscal: User does not have permission to query table etl-sistemas-sri:sri_presion_fiscal.sri_presion_fiscal, or perhaps it does not exist.\n",
            "\n",
            "Location: US\n",
            "Job ID: d7aa6138-fd4f-4175-b2f8-c0acf7f5dec0\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Forbidden",
          "evalue": "403 Access Denied: Table etl-sistemas-sri:sri_presion_fiscal.sri_presion_fiscal: User does not have permission to query table etl-sistemas-sri:sri_presion_fiscal.sri_presion_fiscal, or perhaps it does not exist.; reason: accessDenied, message: Access Denied: Table etl-sistemas-sri:sri_presion_fiscal.sri_presion_fiscal: User does not have permission to query table etl-sistemas-sri:sri_presion_fiscal.sri_presion_fiscal, or perhaps it does not exist.\n\nLocation: US\nJob ID: d7aa6138-fd4f-4175-b2f8-c0acf7f5dec0\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-14-1622821769.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Execute the query and load results into a DataFrame for Dim_Region\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mdf_region\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_regions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nExtracted {len(df_region)} unique regions.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_region\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mto_dataframe\u001b[0;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, max_results, geography_as_object, bool_dtype, int_dtype, float_dtype, string_dtype, date_dtype, datetime_dtype, time_dtype, timestamp_dtype, range_date_dtype, range_datetime_dtype, range_timestamp_dtype)\u001b[0m\n\u001b[1;32m   2072\u001b[0m                 \u001b[0;34m:\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mshapely\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mlibrary\u001b[0m \u001b[0mcannot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mimported\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2073\u001b[0m         \"\"\"\n\u001b[0;32m-> 2074\u001b[0;31m         \u001b[0mquery_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait_for_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2075\u001b[0m         return query_result.to_dataframe(\n\u001b[1;32m   2076\u001b[0m             \u001b[0mbqstorage_client\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbqstorage_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/_tqdm_helpers.py\u001b[0m in \u001b[0;36mwait_for_query\u001b[0;34m(query_job, progress_bar_type, max_results)\u001b[0m\n\u001b[1;32m    105\u001b[0m     )\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprogress_bar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mquery_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, page_size, max_results, retry, timeout, start_index, job_retry)\u001b[0m\n\u001b[1;32m   1694\u001b[0m                 \u001b[0;31m# Since is_job_done() calls jobs.getQueryResults, which is a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m                 \u001b[0;31m# long-running API, don't delay the next request at all.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1696\u001b[0;31m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_job_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1697\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mis_job_done\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1643\u001b[0m                         \u001b[0;31m# `job_retry` predicate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m                         \u001b[0mrestart_query_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mjob_failed_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m                         \u001b[0;31m# Make sure that the _query_results are cached so we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mForbidden\u001b[0m: 403 Access Denied: Table etl-sistemas-sri:sri_presion_fiscal.sri_presion_fiscal: User does not have permission to query table etl-sistemas-sri:sri_presion_fiscal.sri_presion_fiscal, or perhaps it does not exist.; reason: accessDenied, message: Access Denied: Table etl-sistemas-sri:sri_presion_fiscal.sri_presion_fiscal: User does not have permission to query table etl-sistemas-sri:sri_presion_fiscal.sri_presion_fiscal, or perhaps it does not exist.\n\nLocation: US\nJob ID: d7aa6138-fd4f-4175-b2f8-c0acf7f5dec0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e098d96"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed with a `403 Access Denied` error. This indicates that the service account associated with the credentials used by the BigQuery client does not have permission to query the source table (`etl-sistemas-sri.sri_presion_fiscal.sri_presion_fiscal`) in the `etl-sistemas-sri` project. The credentials loaded (`presionfiscaletl-ab366d04e3b7.json`) are likely for a service account in a different project (`presionfiscaletl`) and do not have cross-project permissions to query tables in `etl-sistemas-sri`. To resolve this, I need to either: 1) Use credentials for a service account in the `etl-sistemas-sri` project that *does* have permission, or 2) Ensure the service account from `presionfiscaletl` is granted the necessary permissions (e.g., BigQuery Data Viewer role) on the `etl-sistemas-sri` project or the specific dataset/table.\n",
        "\n",
        "Since I cannot modify GCP permissions directly from this environment, the most feasible approach within the current context is to assume the credentials provided *should* have the necessary permissions for both loading data (which seemed to work in the previous steps) and querying. The issue might stem from using a different `PROJECT_ID` for the client initialization (`presionfiscaletl`) than the `SOURCE_PROJECT_ID` (`etl-sistemas-sri`) in the query.\n",
        "\n",
        "To fix this, I will re-initialize the BigQuery client using the `SOURCE_PROJECT_ID` (`etl-sistemas-sri`) to ensure the client operates within the context of the project where the source data resides. This way, the client will use the loaded credentials but target the correct project for the query. I will keep the destination `PROJECT_ID` potentially separate as defined previously if the dimension tables are intended for a different project. After re-initializing the client, I will retry the steps to extract, transform, and load the `Dim_Region` data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "qWn8lfHv5CAg",
        "outputId": "d51ee1fb-bfe0-4a69-c7c3-4357d8cec975"
      },
      "source": [
        "# Correct the Project ID for the BigQuery client to match the source project\n",
        "SOURCE_PROJECT_ID = 'etl-sistemas-sri' # Use the project ID where the source data was loaded\n",
        "\n",
        "# Re-initialize BigQuery client with the source project ID\n",
        "client = bigquery.Client(project=SOURCE_PROJECT_ID, credentials=credentials)\n",
        "print(f\"BigQuery client re-initialized for source project: {SOURCE_PROJECT_ID}\")\n",
        "\n",
        "# Redefine the source table ID using the correct source project ID\n",
        "SOURCE_TABLE_ID = f\"{SOURCE_PROJECT_ID}.{DATASET_ID}.sri_presion_fiscal\" # Source table for dimension data\n",
        "\n",
        "# Write a SQL query to select distinct regions\n",
        "query_regions = f\"\"\"\n",
        "SELECT DISTINCT Region\n",
        "FROM `{SOURCE_TABLE_ID}`\n",
        "WHERE Region IS NOT NULL AND Region != 'Region' # Exclude header row if present in data\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query and load results into a DataFrame for Dim_Region\n",
        "try:\n",
        "    df_region = client.query(query_regions).to_dataframe()\n",
        "    print(f\"\\nExtracted {len(df_region)} unique regions.\")\n",
        "    display(df_region.head())\n",
        "\n",
        "    # Add a region_id column (surrogate key)\n",
        "    df_region['region_id'] = df_region.reset_index().index + 1 # Simple 1-based integer ID\n",
        "\n",
        "    # Rename the region column to match the schema\n",
        "    df_region.rename(columns={'Region': 'region_nombre'}, inplace=True)\n",
        "\n",
        "    # Select only the required columns and ensure order\n",
        "    df_region = df_region[['region_id', 'region_nombre']]\n",
        "\n",
        "    # Load the Dim_Region DataFrame into the BigQuery table\n",
        "    # Use the original destination table ID, which might be in a different project\n",
        "    job_config_region = bigquery.LoadJobConfig(\n",
        "        schema=schema_dim_region, # Use the defined schema\n",
        "        write_disposition=\"WRITE_TRUNCATE\", # Overwrite the table\n",
        "    )\n",
        "\n",
        "    # The destination table ID includes the project ID (PROJECT_ID defined previously)\n",
        "    job_region = client.load_table_from_dataframe(\n",
        "        df_region, table_id_dim_region, job_config=job_config_region\n",
        "    )\n",
        "\n",
        "    print(f\"\\nStarting BigQuery load job for Dim_Region: {job_region.job_id}\")\n",
        "    job_region.result() # Wait for the job to complete\n",
        "\n",
        "    print(f\"DataFrame successfully loaded to {table_id_dim_region}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error processing Dim_Region: {e}\")\n",
        "    raise # Re-raise the exception\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BigQuery client re-initialized for source project: etl-sistemas-sri\n",
            "Error processing Dim_Region: 403 POST https://bigquery.googleapis.com/bigquery/v2/projects/etl-sistemas-sri/jobs?prettyPrint=false: Access Denied: Project etl-sistemas-sri: User does not have bigquery.jobs.create permission in project etl-sistemas-sri.\n",
            "\n",
            "Location: None\n",
            "Job ID: 8e5856d6-27df-4f14-bbd7-784ad3654f72\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Forbidden",
          "evalue": "403 POST https://bigquery.googleapis.com/bigquery/v2/projects/etl-sistemas-sri/jobs?prettyPrint=false: Access Denied: Project etl-sistemas-sri: User does not have bigquery.jobs.create permission in project etl-sistemas-sri.\n\nLocation: None\nJob ID: 8e5856d6-27df-4f14-bbd7-784ad3654f72\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-15-1902948165.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Execute the query and load results into a DataFrame for Dim_Region\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mdf_region\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_regions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nExtracted {len(df_region)} unique regions.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_region\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, query, job_config, job_id, job_id_prefix, location, project, retry, timeout, job_retry, api_method)\u001b[0m\n\u001b[1;32m   3517\u001b[0m             )\n\u001b[1;32m   3518\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mapi_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0menums\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQueryApiMethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINSERT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3519\u001b[0;31m             return _job_helpers.query_jobs_insert(\n\u001b[0m\u001b[1;32m   3520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3521\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/_job_helpers.py\u001b[0m in \u001b[0;36mquery_jobs_insert\u001b[0;34m(client, query, job_config, job_id, job_id_prefix, location, project, retry, timeout, job_retry)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mdo_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_DEFAULT_QUERY_JOB_INSERT_RETRY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# The future might be in a failed state now, but if it's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/_job_helpers.py\u001b[0m in \u001b[0;36mdo_query\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mquery_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mcore_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConflict\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcreate_exc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;31m# The thought is if someone is providing their own job IDs and they get\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36m_begin\u001b[0;34m(self, client, retry, timeout)\u001b[0m\n\u001b[1;32m   1396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1398\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQueryJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1399\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGoogleAPICallError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m             exc.message = _EXCEPTION_FOOTER_TEMPLATE.format(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/job/base.py\u001b[0m in \u001b[0;36m_begin\u001b[0;34m(self, client, retry, timeout)\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;31m# job has an ID.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mspan_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         api_response = client._call_api(\n\u001b[0m\u001b[1;32m    781\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0mspan_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"BigQuery.job.begin\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36m_call_api\u001b[0;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[1;32m    856\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspan_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspan_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m             ):\n\u001b[0;32m--> 858\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/_http/__init__.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexpect_json\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mForbidden\u001b[0m: 403 POST https://bigquery.googleapis.com/bigquery/v2/projects/etl-sistemas-sri/jobs?prettyPrint=false: Access Denied: Project etl-sistemas-sri: User does not have bigquery.jobs.create permission in project etl-sistemas-sri.\n\nLocation: None\nJob ID: 8e5856d6-27df-4f14-bbd7-784ad3654f72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65744af2"
      },
      "source": [
        "## Etl de dimensiones (fase 2)\n",
        "\n",
        "### Subtask:\n",
        "Definir esquemas para las tablas de dimensiones (por ejemplo, Dim_Region, Dim_Pais, Dim_Tiempo, Dim_TipoPresion), extraer datos únicos y relevantes de la tabla `sri_presion_fiscal` para cada dimensión, limpiar y estandarizar los datos de dimensión si es necesario, generar claves subrogadas únicas para cada dimensión y cargar los datos en las respectivas tablas de dimensiones en BigQuery."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dc2c416"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the schemas for the dimension tables, initialize the BigQuery client, and create the dataset and dimension tables in BigQuery. This involves defining the schema fields and using the `bigquery.Client`, `bigquery.Dataset`, and `bigquery.Table` objects, along with `client.create_dataset` and `client.create_table`. I will also include the credential loading and project ID definition from the previous successful steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0510c8a",
        "outputId": "f5094680-c6a1-4093-c1cb-b8cac72f410e"
      },
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "import os\n",
        "\n",
        "# Define BigQuery parameters\n",
        "# Use the correct filename for the Google Cloud service account credentials provided by the user.\n",
        "credential_filename = \"presionfiscaletl-ab366d04e3b7.json\" # Use the filename provided by the user\n",
        "# Set the environment variable for Google Cloud authentication\n",
        "# Note: In a real Airflow environment, prefer using Airflow Connections for GCP\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_filename\n",
        "print(f\"GOOGLE_APPLICATION_CREDENTIALS environment variable set to: {credential_filename}\")\n",
        "\n",
        "# Load credentials using the environment variable\n",
        "try:\n",
        "    credentials = service_account.Credentials.from_service_account_file(os.environ['GOOGLE_APPLICATION_CREDENTIALS'])\n",
        "    print(\"Google Cloud credentials loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Credential file '{credential_filename}' not found. Please upload it again and ensure the filename is correct.\")\n",
        "    raise # Re-raise the exception if the file is not found\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred loading credentials: {e}\")\n",
        "    raise # Re-raise any other exception\n",
        "\n",
        "# Get Project ID from the environment variable GOOGLE_CLOUD_PROJECT, falling back to credentials\n",
        "PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT', None)\n",
        "if PROJECT_ID is None:\n",
        "    try:\n",
        "        PROJECT_ID = credentials.project_id\n",
        "    except Exception as e:\n",
        "        print(f\"Could not automatically determine Google Cloud Project ID. Please set GOOGLE_CLOUD_PROJECT environment variable or ensure GOOGLE_APPLICATION_CREDENTIALS is correctly set: {e}\")\n",
        "        # Fallback or set a default - In a production DAG, this should likely fail or use an Airflow connection\n",
        "        PROJECT_ID = \"your-gcp-project-id\" # !!! REPLACE WITH YOUR ACTUAL GCP PROJECT ID OR USE AIRFLOW CONNECTION/VARIABLE !!!\n",
        "        print(f\"Using fallback Project ID: {PROJECT_ID}\")\n",
        "\n",
        "DATASET_ID = \"sri_presion_fiscal\" # BigQuery Dataset ID\n",
        "SOURCE_TABLE_ID = f\"{PROJECT_ID}.{DATASET_ID}.sri_presion_fiscal\" # Source table for dimension data\n",
        "\n",
        "# Initialize BigQuery client\n",
        "client = bigquery.Client(project=PROJECT_ID, credentials=credentials)\n",
        "print(f\"BigQuery client initialized for project: {PROJECT_ID}\")\n",
        "\n",
        "# Create or get the BigQuery dataset\n",
        "dataset_ref = client.dataset(DATASET_ID)\n",
        "dataset = bigquery.Dataset(dataset_ref)\n",
        "dataset.location = \"US\" # Set location - adjust as needed\n",
        "\n",
        "try:\n",
        "    dataset = client.create_dataset(dataset, exists_ok=True)\n",
        "    print(f\"Dataset '{DATASET_ID}' created or already exists.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating dataset '{DATASET_ID}': {e}\")\n",
        "    raise # Re-raise the exception\n",
        "\n",
        "# Define schemas for dimension tables\n",
        "schema_dim_region = [\n",
        "    bigquery.SchemaField(\"region_id\", \"INT64\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"region_nombre\", \"STRING\", mode=\"REQUIRED\"),\n",
        "]\n",
        "\n",
        "schema_dim_pais = [\n",
        "    bigquery.SchemaField(\"pais_id\", \"INT64\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"pais_nombre\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"region_id\", \"INT64\", mode=\"REQUIRED\"),\n",
        "]\n",
        "\n",
        "schema_dim_tiempo = [\n",
        "    bigquery.SchemaField(\"tiempo_id\", \"INT64\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"año\", \"INT64\", mode=\"REQUIRED\"),\n",
        "]\n",
        "\n",
        "schema_dim_tipopresion = [\n",
        "    bigquery.SchemaField(\"tipo_presion_id\", \"INT64\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"tipo_presion_nombre\", \"STRING\", mode=\"REQUIRED\"),\n",
        "]\n",
        "\n",
        "# Define table IDs for dimension tables\n",
        "table_id_dim_region = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Region\"\n",
        "table_id_dim_pais = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Pais\"\n",
        "table_id_dim_tiempo = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Tiempo\"\n",
        "table_id_dim_tipopresion = f\"{PROJECT_ID}.{DATASET_ID}.Dim_TipoPresion\"\n",
        "\n",
        "# Create or get each dimension table in BigQuery\n",
        "def create_or_get_table(client, table_id, schema):\n",
        "    \"\"\"Creates a BigQuery table if it doesn't exist, or gets it if it does.\"\"\"\n",
        "    table_ref = bigquery.Table(table_id, schema=schema)\n",
        "    try:\n",
        "        table = client.create_table(table_ref, exists_ok=True)\n",
        "        print(f\"Table '{table_id}' created or already exists.\")\n",
        "        return table\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating table '{table_id}': {e}\")\n",
        "        raise\n",
        "\n",
        "table_dim_region = create_or_get_table(client, table_id_dim_region, schema_dim_region)\n",
        "table_dim_pais = create_or_get_table(client, table_id_dim_pais, schema_dim_pais)\n",
        "table_dim_tiempo = create_or_get_table(client, table_id_dim_tiempo, schema_dim_tiempo)\n",
        "table_dim_tipopresion = create_or_get_table(client, table_id_dim_tipopresion, schema_dim_tipopresion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GOOGLE_APPLICATION_CREDENTIALS environment variable set to: presionfiscaletl-ab366d04e3b7.json\n",
            "Google Cloud credentials loaded successfully.\n",
            "BigQuery client initialized for project: presionfiscaletl\n",
            "Dataset 'sri_presion_fiscal' created or already exists.\n",
            "Table 'presionfiscaletl.sri_presion_fiscal.Dim_Region' created or already exists.\n",
            "Table 'presionfiscaletl.sri_presion_fiscal.Dim_Pais' created or already exists.\n",
            "Table 'presionfiscaletl.sri_presion_fiscal.Dim_Tiempo' created or already exists.\n",
            "Table 'presionfiscaletl.sri_presion_fiscal.Dim_TipoPresion' created or already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86e3259a"
      },
      "source": [
        "**Reasoning**:\n",
        "The BigQuery dataset and dimension tables have been created or retrieved. Now I need to extract unique regions from the source table, create a DataFrame for `Dim_Region`, add a surrogate key, and load it into the `Dim_Region` BigQuery table. This involves writing a SQL query, executing it with the BigQuery client, creating a pandas DataFrame, adding an ID column, and loading the DataFrame to BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "a4799e74",
        "outputId": "950afc52-37be-4dca-dfbf-acb791eed05f"
      },
      "source": [
        "# Correct the Project ID for the source table to match where the data was loaded\n",
        "SOURCE_PROJECT_ID = 'etl-sistemas-sri' # Use the project ID where the source data was loaded\n",
        "\n",
        "# Redefine the source table ID using the correct source project ID\n",
        "SOURCE_TABLE_ID = f\"{SOURCE_PROJECT_ID}.{DATASET_ID}.sri_presion_fiscal\" # Source table for dimension data\n",
        "\n",
        "# Write a SQL query to select distinct regions\n",
        "query_regions = f\"\"\"\n",
        "SELECT DISTINCT Region\n",
        "FROM `{SOURCE_TABLE_ID}`\n",
        "WHERE Region IS NOT NULL AND Region != 'Region' # Exclude header row if present in data\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query and load results into a DataFrame for Dim_Region\n",
        "try:\n",
        "    df_region = client.query(query_regions).to_dataframe()\n",
        "    print(f\"\\nExtracted {len(df_region)} unique regions.\")\n",
        "    display(df_region.head())\n",
        "\n",
        "    # Add a region_id column (surrogate key)\n",
        "    df_region['region_id'] = df_region.reset_index().index + 1 # Simple 1-based integer ID\n",
        "\n",
        "    # Rename the region column to match the schema\n",
        "    df_region.rename(columns={'Region': 'region_nombre'}, inplace=True)\n",
        "\n",
        "    # Select only the required columns and ensure order\n",
        "    df_region = df_region[['region_id', 'region_nombre']]\n",
        "\n",
        "    # Load the Dim_Region DataFrame into the BigQuery table\n",
        "    job_config_region = bigquery.LoadJobConfig(\n",
        "        schema=schema_dim_region, # Use the defined schema\n",
        "        write_disposition=\"WRITE_TRUNCATE\", # Overwrite the table\n",
        "    )\n",
        "\n",
        "    # The destination table ID includes the project ID (PROJECT_ID defined previously)\n",
        "    job_region = client.load_table_from_dataframe(\n",
        "        df_region, table_id_dim_region, job_config=job_config_region\n",
        "    )\n",
        "\n",
        "    print(f\"\\nStarting BigQuery load job for Dim_Region: {job_region.job_id}\")\n",
        "    job_region.result() # Wait for the job to complete\n",
        "\n",
        "    print(f\"DataFrame successfully loaded to {table_id_dim_region}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error processing Dim_Region: {e}\")\n",
        "    raise # Re-raise the exception"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing Dim_Region: 403 Access Denied: Table etl-sistemas-sri:sri_presion_fiscal.sri_presion_fiscal: User does not have permission to query table etl-sistemas-sri:sri_presion_fiscal.sri_presion_fiscal, or perhaps it does not exist.; reason: accessDenied, message: Access Denied: Table etl-sistemas-sri:sri_presion_fiscal.sri_presion_fiscal: User does not have permission to query table etl-sistemas-sri:sri_presion_fiscal.sri_presion_fiscal, or perhaps it does not exist.\n",
            "\n",
            "Location: US\n",
            "Job ID: 8db49a9f-064a-47b3-8bd7-f45c0492b9db\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Forbidden",
          "evalue": "403 Access Denied: Table etl-sistemas-sri:sri_presion_fiscal.sri_presion_fiscal: User does not have permission to query table etl-sistemas-sri:sri_presion_fiscal.sri_presion_fiscal, or perhaps it does not exist.; reason: accessDenied, message: Access Denied: Table etl-sistemas-sri:sri_presion_fiscal.sri_presion_fiscal: User does not have permission to query table etl-sistemas-sri:sri_presion_fiscal.sri_presion_fiscal, or perhaps it does not exist.\n\nLocation: US\nJob ID: 8db49a9f-064a-47b3-8bd7-f45c0492b9db\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-18-3361923662.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Execute the query and load results into a DataFrame for Dim_Region\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mdf_region\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_regions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nExtracted {len(df_region)} unique regions.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_region\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mto_dataframe\u001b[0;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, max_results, geography_as_object, bool_dtype, int_dtype, float_dtype, string_dtype, date_dtype, datetime_dtype, time_dtype, timestamp_dtype, range_date_dtype, range_datetime_dtype, range_timestamp_dtype)\u001b[0m\n\u001b[1;32m   2072\u001b[0m                 \u001b[0;34m:\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mshapely\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mlibrary\u001b[0m \u001b[0mcannot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mimported\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2073\u001b[0m         \"\"\"\n\u001b[0;32m-> 2074\u001b[0;31m         \u001b[0mquery_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait_for_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2075\u001b[0m         return query_result.to_dataframe(\n\u001b[1;32m   2076\u001b[0m             \u001b[0mbqstorage_client\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbqstorage_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/_tqdm_helpers.py\u001b[0m in \u001b[0;36mwait_for_query\u001b[0;34m(query_job, progress_bar_type, max_results)\u001b[0m\n\u001b[1;32m    105\u001b[0m     )\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprogress_bar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mquery_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, page_size, max_results, retry, timeout, start_index, job_retry)\u001b[0m\n\u001b[1;32m   1694\u001b[0m                 \u001b[0;31m# Since is_job_done() calls jobs.getQueryResults, which is a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m                 \u001b[0;31m# long-running API, don't delay the next request at all.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1696\u001b[0;31m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_job_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1697\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mis_job_done\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1643\u001b[0m                         \u001b[0;31m# `job_retry` predicate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m                         \u001b[0mrestart_query_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mjob_failed_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m                         \u001b[0;31m# Make sure that the _query_results are cached so we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mForbidden\u001b[0m: 403 Access Denied: Table etl-sistemas-sri:sri_presion_fiscal.sri_presion_fiscal: User does not have permission to query table etl-sistemas-sri:sri_presion_fiscal.sri_presion_fiscal, or perhaps it does not exist.; reason: accessDenied, message: Access Denied: Table etl-sistemas-sri:sri_presion_fiscal.sri_presion_fiscal: User does not have permission to query table etl-sistemas-sri:sri_presion_fiscal.sri_presion_fiscal, or perhaps it does not exist.\n\nLocation: US\nJob ID: 8db49a9f-064a-47b3-8bd7-f45c0492b9db\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4be8a0fd"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed with a `403 Access Denied` error. This indicates that the service account associated with the credentials used by the BigQuery client does not have permission to query the source table (`etl-sistemas-sri.sri_presion_fiscal.sri_presion_fiscal`) in the `etl-sistemas-sri` project. The credentials loaded (`presionfiscaletl-ab366d04e3b7.json`) are likely for a service account in a different project (`presionfiscaletl`) and do not have cross-project permissions to query tables in `etl-sistemas-sri`. To resolve this, I need to either: 1) Use credentials for a service account in the `etl-sistemas-sri` project that *does* have permission, or 2) Ensure the service account from `presionfiscaletl` is granted the necessary permissions (e.g., BigQuery Data Viewer role) on the `etl-sistemas-sri` project or the specific dataset/table.\n",
        "\n",
        "Since I cannot modify GCP permissions directly from this environment, the most feasible approach within the current context is to assume the credentials provided *should* have the necessary permissions for both loading data (which seemed to work in the previous steps) and querying. The issue might stem from using a different `PROJECT_ID` for the client initialization (`presionfiscaletl`) than the `SOURCE_PROJECT_ID` (`etl-sistemas-sri`) in the query.\n",
        "\n",
        "To fix this, I will re-initialize the BigQuery client using the `SOURCE_PROJECT_ID` (`etl-sistemas-sri`) to ensure the client operates within the context of the project where the source data resides. This way, the client will use the loaded credentials but target the correct project for the query. I will keep the destination `PROJECT_ID` potentially separate as defined previously if the dimension tables are intended for a different project. After re-initializing the client, I will retry the steps to extract, transform, and load the `Dim_Region` data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "9f3332c1",
        "outputId": "3c5be76e-9c05-4ed4-c017-4bec02f15571"
      },
      "source": [
        "# Correct the Project ID for the BigQuery client to match the source project\n",
        "SOURCE_PROJECT_ID = 'etl-sistemas-sri' # Use the project ID where the source data was loaded\n",
        "\n",
        "# Re-initialize BigQuery client with the source project ID\n",
        "client = bigquery.Client(project=SOURCE_PROJECT_ID, credentials=credentials)\n",
        "print(f\"BigQuery client re-initialized for source project: {SOURCE_PROJECT_ID}\")\n",
        "\n",
        "# Redefine the source table ID using the correct source project ID\n",
        "SOURCE_TABLE_ID = f\"{SOURCE_PROJECT_ID}.{DATASET_ID}.sri_presion_fiscal\" # Source table for dimension data\n",
        "\n",
        "# Write a SQL query to select distinct regions\n",
        "query_regions = f\"\"\"\n",
        "SELECT DISTINCT Region\n",
        "FROM `{SOURCE_TABLE_ID}`\n",
        "WHERE Region IS NOT NULL AND Region != 'Region' # Exclude header row if present in data\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query and load results into a DataFrame for Dim_Region\n",
        "try:\n",
        "    df_region = client.query(query_regions).to_dataframe()\n",
        "    print(f\"\\nExtracted {len(df_region)} unique regions.\")\n",
        "    display(df_region.head())\n",
        "\n",
        "    # Add a region_id column (surrogate key)\n",
        "    df_region['region_id'] = df_region.reset_index().index + 1 # Simple 1-based integer ID\n",
        "\n",
        "    # Rename the region column to match the schema\n",
        "    df_region.rename(columns={'Region': 'region_nombre'}, inplace=True)\n",
        "\n",
        "    # Select only the required columns and ensure order\n",
        "    df_region = df_region[['region_id', 'region_nombre']]\n",
        "\n",
        "    # Load the Dim_Region DataFrame into the BigQuery table\n",
        "    # Use the original destination table ID, which might be in a different project\n",
        "    job_config_region = bigquery.LoadJobConfig(\n",
        "        schema=schema_dim_region, # Use the defined schema\n",
        "        write_disposition=\"WRITE_TRUNCATE\", # Overwrite the table\n",
        "    )\n",
        "\n",
        "    # The destination table ID includes the project ID (PROJECT_ID defined previously)\n",
        "    job_region = client.load_table_from_dataframe(\n",
        "        df_region, table_id_dim_region, job_config=job_config_region\n",
        "    )\n",
        "\n",
        "    print(f\"\\nStarting BigQuery load job for Dim_Region: {job_region.job_id}\")\n",
        "    job_region.result() # Wait for the job to complete\n",
        "\n",
        "    print(f\"DataFrame successfully loaded to {table_id_dim_region}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error processing Dim_Region: {e}\")\n",
        "    raise # Re-raise the exception"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BigQuery client re-initialized for source project: etl-sistemas-sri\n",
            "Error processing Dim_Region: 403 POST https://bigquery.googleapis.com/bigquery/v2/projects/etl-sistemas-sri/jobs?prettyPrint=false: Access Denied: Project etl-sistemas-sri: User does not have bigquery.jobs.create permission in project etl-sistemas-sri.\n",
            "\n",
            "Location: None\n",
            "Job ID: aa003794-6255-45b3-9e2a-f13ae06e665b\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Forbidden",
          "evalue": "403 POST https://bigquery.googleapis.com/bigquery/v2/projects/etl-sistemas-sri/jobs?prettyPrint=false: Access Denied: Project etl-sistemas-sri: User does not have bigquery.jobs.create permission in project etl-sistemas-sri.\n\nLocation: None\nJob ID: aa003794-6255-45b3-9e2a-f13ae06e665b\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-19-3478464526.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Execute the query and load results into a DataFrame for Dim_Region\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mdf_region\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_regions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nExtracted {len(df_region)} unique regions.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_region\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, query, job_config, job_id, job_id_prefix, location, project, retry, timeout, job_retry, api_method)\u001b[0m\n\u001b[1;32m   3517\u001b[0m             )\n\u001b[1;32m   3518\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mapi_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0menums\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQueryApiMethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINSERT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3519\u001b[0;31m             return _job_helpers.query_jobs_insert(\n\u001b[0m\u001b[1;32m   3520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3521\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/_job_helpers.py\u001b[0m in \u001b[0;36mquery_jobs_insert\u001b[0;34m(client, query, job_config, job_id, job_id_prefix, location, project, retry, timeout, job_retry)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mdo_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_DEFAULT_QUERY_JOB_INSERT_RETRY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# The future might be in a failed state now, but if it's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/_job_helpers.py\u001b[0m in \u001b[0;36mdo_query\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mquery_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mcore_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConflict\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcreate_exc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;31m# The thought is if someone is providing their own job IDs and they get\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36m_begin\u001b[0;34m(self, client, retry, timeout)\u001b[0m\n\u001b[1;32m   1396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1398\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQueryJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1399\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGoogleAPICallError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m             exc.message = _EXCEPTION_FOOTER_TEMPLATE.format(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/job/base.py\u001b[0m in \u001b[0;36m_begin\u001b[0;34m(self, client, retry, timeout)\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;31m# job has an ID.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mspan_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         api_response = client._call_api(\n\u001b[0m\u001b[1;32m    781\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0mspan_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"BigQuery.job.begin\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36m_call_api\u001b[0;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[1;32m    856\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspan_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspan_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m             ):\n\u001b[0;32m--> 858\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/_http/__init__.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexpect_json\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mForbidden\u001b[0m: 403 POST https://bigquery.googleapis.com/bigquery/v2/projects/etl-sistemas-sri/jobs?prettyPrint=false: Access Denied: Project etl-sistemas-sri: User does not have bigquery.jobs.create permission in project etl-sistemas-sri.\n\nLocation: None\nJob ID: aa003794-6255-45b3-9e2a-f13ae06e665b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "babad4a4"
      },
      "source": [
        "**Reasoning**:\n",
        "The user has confirmed they have granted the necessary permissions in Google Cloud Console. Retrying the code to extract unique regions from the source table and load them into the `Dim_Region` table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "6e7c5b0a",
        "outputId": "b7c1e0df-e841-4568-857e-3454113dfcf1"
      },
      "source": [
        "# Correct the Project ID for the BigQuery client to match the source project\n",
        "SOURCE_PROJECT_ID = 'etl-sistemas-sri' # Use the project ID where the source data was loaded\n",
        "\n",
        "# Re-initialize BigQuery client with the source project ID\n",
        "client = bigquery.Client(project=SOURCE_PROJECT_ID, credentials=credentials)\n",
        "print(f\"BigQuery client re-initialized for source project: {SOURCE_PROJECT_ID}\")\n",
        "\n",
        "# Redefine the source table ID using the correct source project ID\n",
        "SOURCE_TABLE_ID = f\"{SOURCE_PROJECT_ID}.{DATASET_ID}.sri_presion_fiscal\" # Source table for dimension data\n",
        "\n",
        "# Write a SQL query to select distinct regions\n",
        "query_regions = f\"\"\"\n",
        "SELECT DISTINCT Region\n",
        "FROM `{SOURCE_TABLE_ID}`\n",
        "WHERE Region IS NOT NULL AND Region != 'Region' # Exclude header row if present in data\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query and load results into a DataFrame for Dim_Region\n",
        "try:\n",
        "    df_region = client.query(query_regions).to_dataframe()\n",
        "    print(f\"\\nExtracted {len(df_region)} unique regions.\")\n",
        "    display(df_region.head())\n",
        "\n",
        "    # Add a region_id column (surrogate key)\n",
        "    df_region['region_id'] = df_region.reset_index().index + 1 # Simple 1-based integer ID\n",
        "\n",
        "    # Rename the region column to match the schema\n",
        "    df_region.rename(columns={'Region': 'region_nombre'}, inplace=True)\n",
        "\n",
        "    # Select only the required columns and ensure order\n",
        "    df_region = df_region[['region_id', 'region_nombre']]\n",
        "\n",
        "    # Load the Dim_Region DataFrame into the BigQuery table\n",
        "    # Use the original destination table ID, which might be in a different project\n",
        "    job_config_region = bigquery.LoadJobConfig(\n",
        "        schema=schema_dim_region, # Use the defined schema\n",
        "        write_disposition=\"WRITE_TRUNCATE\", # Overwrite the table\n",
        "    )\n",
        "\n",
        "    # The destination table ID includes the project ID (PROJECT_ID defined previously)\n",
        "    job_region = client.load_table_from_dataframe(\n",
        "        df_region, table_id_dim_region, job_config=job_config_region\n",
        "    )\n",
        "\n",
        "    print(f\"\\nStarting BigQuery load job for Dim_Region: {job_region.job_id}\")\n",
        "    job_region.result() # Wait for the job to complete\n",
        "\n",
        "    print(f\"DataFrame successfully loaded to {table_id_dim_region}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error processing Dim_Region: {e}\")\n",
        "    raise # Re-raise the exception"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BigQuery client re-initialized for source project: etl-sistemas-sri\n",
            "Error processing Dim_Region: 403 POST https://bigquery.googleapis.com/bigquery/v2/projects/etl-sistemas-sri/jobs?prettyPrint=false: Access Denied: Project etl-sistemas-sri: User does not have bigquery.jobs.create permission in project etl-sistemas-sri.\n",
            "\n",
            "Location: None\n",
            "Job ID: 92f5aaff-6ac3-4eaf-9f7c-5ead44ed2590\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Forbidden",
          "evalue": "403 POST https://bigquery.googleapis.com/bigquery/v2/projects/etl-sistemas-sri/jobs?prettyPrint=false: Access Denied: Project etl-sistemas-sri: User does not have bigquery.jobs.create permission in project etl-sistemas-sri.\n\nLocation: None\nJob ID: 92f5aaff-6ac3-4eaf-9f7c-5ead44ed2590\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-20-3478464526.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Execute the query and load results into a DataFrame for Dim_Region\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mdf_region\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_regions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nExtracted {len(df_region)} unique regions.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_region\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, query, job_config, job_id, job_id_prefix, location, project, retry, timeout, job_retry, api_method)\u001b[0m\n\u001b[1;32m   3517\u001b[0m             )\n\u001b[1;32m   3518\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mapi_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0menums\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQueryApiMethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINSERT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3519\u001b[0;31m             return _job_helpers.query_jobs_insert(\n\u001b[0m\u001b[1;32m   3520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3521\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/_job_helpers.py\u001b[0m in \u001b[0;36mquery_jobs_insert\u001b[0;34m(client, query, job_config, job_id, job_id_prefix, location, project, retry, timeout, job_retry)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mdo_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_DEFAULT_QUERY_JOB_INSERT_RETRY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# The future might be in a failed state now, but if it's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/_job_helpers.py\u001b[0m in \u001b[0;36mdo_query\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mquery_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mcore_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConflict\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcreate_exc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;31m# The thought is if someone is providing their own job IDs and they get\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36m_begin\u001b[0;34m(self, client, retry, timeout)\u001b[0m\n\u001b[1;32m   1396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1398\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQueryJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1399\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGoogleAPICallError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m             exc.message = _EXCEPTION_FOOTER_TEMPLATE.format(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/job/base.py\u001b[0m in \u001b[0;36m_begin\u001b[0;34m(self, client, retry, timeout)\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;31m# job has an ID.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mspan_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         api_response = client._call_api(\n\u001b[0m\u001b[1;32m    781\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0mspan_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"BigQuery.job.begin\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36m_call_api\u001b[0;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[1;32m    856\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspan_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspan_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m             ):\n\u001b[0;32m--> 858\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/_http/__init__.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexpect_json\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mForbidden\u001b[0m: 403 POST https://bigquery.googleapis.com/bigquery/v2/projects/etl-sistemas-sri/jobs?prettyPrint=false: Access Denied: Project etl-sistemas-sri: User does not have bigquery.jobs.create permission in project etl-sistemas-sri.\n\nLocation: None\nJob ID: 92f5aaff-6ac3-4eaf-9f7c-5ead44ed2590\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f9426c1"
      },
      "source": [
        "**Reasoning**:\n",
        "The user has confirmed that the necessary permissions have been activated in Google Cloud. Retrying the code to extract unique regions from the source table and load them into the `Dim_Region` table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "7d47f470",
        "outputId": "c72f6261-88b6-4e07-9699-c12223879be6"
      },
      "source": [
        "# Define the Project ID for both source and destination to be the source project\n",
        "PROJECT_ID = 'etl-sistemas-sri' # Use the project ID where the source data is located and where dimension/fact tables will be created\n",
        "\n",
        "# Redefine the source table ID using the correct source project ID\n",
        "SOURCE_TABLE_ID = f\"{PROJECT_ID}.{DATASET_ID}.sri_presion_fiscal\" # Source table for dimension data (using the now unified PROJECT_ID)\n",
        "\n",
        "\n",
        "# Write a SQL query to select distinct regions\n",
        "query_regions = f\"\"\"\n",
        "SELECT DISTINCT Region\n",
        "FROM `{SOURCE_TABLE_ID}`\n",
        "WHERE Region IS NOT NULL AND Region != 'Region' # Exclude header row if present in data\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query and load results into a DataFrame for Dim_Region\n",
        "try:\n",
        "    # --- Update credential filename and path here ---\n",
        "    # Use the absolute path provided by the user\n",
        "    credential_filepath = \"/etl-sistemas-sri-cc64af72be76.json\" # Use the full path provided by the user\n",
        "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_filepath\n",
        "    print(f\"GOOGLE_APPLICATION_CREDENTIALS environment variable set to: {credential_filepath}\")\n",
        "\n",
        "    # Re-load credentials using the new environment variable setting\n",
        "    credentials = service_account.Credentials.from_service_account_file(os.environ['GOOGLE_APPLICATION_CREDENTIALS'])\n",
        "    print(\"Google Cloud credentials loaded successfully with new file and path.\")\n",
        "\n",
        "    # Re-initialize BigQuery client with the unified PROJECT_ID\n",
        "    # The client will now operate within the 'etl-sistemas-sri' project\n",
        "    client = bigquery.Client(project=PROJECT_ID, credentials=credentials)\n",
        "    print(f\"BigQuery client re-initialized for project: {PROJECT_ID} with new credentials.\")\n",
        "    # --- End of credential update ---\n",
        "\n",
        "    # --- Re-define dataset and table IDs using the unified PROJECT_ID ---\n",
        "    DATASET_ID = \"sri_presion_fiscal\" # BigQuery Dataset ID\n",
        "    table_id_dim_region = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Region\"\n",
        "    table_id_dim_pais = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Pais\"\n",
        "    table_id_dim_tiempo = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Tiempo\"\n",
        "    table_id_dim_tipopresion = f\"{PROJECT_ID}.{DATASET_ID}.Dim_TipoPresion\"\n",
        "    # --- End of re-definition ---\n",
        "\n",
        "    # Re-create or get the BigQuery dataset in the unified project\n",
        "    dataset_ref = client.dataset(DATASET_ID)\n",
        "    dataset = bigquery.Dataset(dataset_ref)\n",
        "    dataset.location = \"US\" # Set location - adjust as needed\n",
        "\n",
        "    try:\n",
        "        dataset = client.create_dataset(dataset, exists_ok=True)\n",
        "        print(f\"Dataset '{DATASET_ID}' created or already exists in project '{PROJECT_ID}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating dataset '{DATASET_ID}' in project '{PROJECT_ID}': {e}\")\n",
        "        raise # Re-raise the exception\n",
        "\n",
        "    # Re-create or get each dimension table in BigQuery in the unified project\n",
        "    def create_or_get_table(client, table_id, schema):\n",
        "        \"\"\"Creates a BigQuery table if it doesn't exist, or gets it if it does.\"\"\"\n",
        "        table_ref = bigquery.Table(table_id, schema=schema)\n",
        "        try:\n",
        "            table = client.create_table(table_ref, exists_ok=True)\n",
        "            print(f\"Table '{table_id}' created or already exists.\")\n",
        "            return table\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating table '{table_id}': {e}\")\n",
        "            raise\n",
        "\n",
        "    # Define schemas for dimension tables (assuming these were defined earlier or will be defined in this cell)\n",
        "    # For this modification, I will assume schemas are defined - ensure they are defined before this block in the final script\n",
        "    # schema_dim_region = [...] # Ensure schemas are defined!\n",
        "    # schema_dim_pais = [...]\n",
        "    # schema_dim_tiempo = [...]\n",
        "    # schema_dim_tipopresion = [...]\n",
        "\n",
        "    # Assuming schemas are defined, create or get tables\n",
        "    table_dim_region = create_or_get_table(client, table_id_dim_region, schema_dim_region)\n",
        "    table_dim_pais = create_or_get_table(client, table_id_dim_pais, schema_dim_pais)\n",
        "    table_dim_tiempo = create_or_get_table(client, table_id_dim_tiempo, schema_dim_tiempo)\n",
        "    table_dim_tipopresion = create_or_get_table(client, table_id_dim_tipopresion, schema_dim_tipopresion)\n",
        "\n",
        "\n",
        "    # --- Continue with extracting and loading Dim_Region ---\n",
        "    df_region = client.query(query_regions).to_dataframe()\n",
        "    print(f\"\\nExtracted {len(df_region)} unique regions.\")\n",
        "    display(df_region.head())\n",
        "\n",
        "    # Add a region_id column (surrogate key)\n",
        "    df_region['region_id'] = df_region.reset_index().index + 1 # Simple 1-based integer ID\n",
        "\n",
        "    # Rename the region column to match the schema\n",
        "    df_region.rename(columns={'Region': 'region_nombre'}, inplace=True)\n",
        "\n",
        "    # Select only the required columns and ensure order\n",
        "    df_region = df_region[['region_id', 'region_nombre']]\n",
        "\n",
        "    # Load the Dim_Region DataFrame into the BigQuery table (now in the unified project)\n",
        "    job_config_region = bigquery.LoadJobConfig(\n",
        "        schema=schema_dim_region, # Use the defined schema\n",
        "        write_disposition=\"WRITE_TRUNCATE\", # Overwrite the table\n",
        "    )\n",
        "\n",
        "    # The destination table ID now refers to the table in the unified PROJECT_ID\n",
        "    job_region = client.load_table_from_dataframe(\n",
        "        df_region, table_id_dim_region, job_config=job_config_region\n",
        "    )\n",
        "\n",
        "    print(f\"\\nStarting BigQuery load job for Dim_Region: {job_region.job_id}\")\n",
        "    job_region.result() # Wait for the job to complete\n",
        "\n",
        "    print(f\"DataFrame successfully loaded to {table_id_dim_region}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Credential file '{credential_filepath}' not found. Please upload the new file and ensure the path is correct.\")\n",
        "    raise # Re-raise the exception if the file is not found\n",
        "except Exception as e:\n",
        "    print(f\"Error processing Dim_Region: {e}\")\n",
        "    raise # Re-raise the exception"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GOOGLE_APPLICATION_CREDENTIALS environment variable set to: /etl-sistemas-sri-cc64af72be76.json\n",
            "Google Cloud credentials loaded successfully with new file and path.\n",
            "BigQuery client re-initialized for project: etl-sistemas-sri with new credentials.\n",
            "Dataset 'sri_presion_fiscal' created or already exists in project 'etl-sistemas-sri'.\n",
            "Table 'etl-sistemas-sri.sri_presion_fiscal.Dim_Region' created or already exists.\n",
            "Table 'etl-sistemas-sri.sri_presion_fiscal.Dim_Pais' created or already exists.\n",
            "Table 'etl-sistemas-sri.sri_presion_fiscal.Dim_Tiempo' created or already exists.\n",
            "Table 'etl-sistemas-sri.sri_presion_fiscal.Dim_TipoPresion' created or already exists.\n",
            "\n",
            "Extracted 2 unique regions.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "           Region\n",
              "0  America Latina\n",
              "1          Europa"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3b9cc960-55f2-43fc-984a-0c37dabc19a8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Region</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>America Latina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Europa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3b9cc960-55f2-43fc-984a-0c37dabc19a8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3b9cc960-55f2-43fc-984a-0c37dabc19a8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3b9cc960-55f2-43fc-984a-0c37dabc19a8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-058af3e7-6210-4104-ab67-463a4000cceb\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-058af3e7-6210-4104-ab67-463a4000cceb')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-058af3e7-6210-4104-ab67-463a4000cceb button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    raise # Re-raise the exception\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"Region\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Europa\",\n          \"America Latina\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting BigQuery load job for Dim_Region: bdc1302f-2150-4c73-ae49-c16a3e457a3e\n",
            "DataFrame successfully loaded to etl-sistemas-sri.sri_presion_fiscal.Dim_Region\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a248bbb"
      },
      "source": [
        "**Reasoning**:\n",
        "Extract unique countries from the source table, join with Dim_Region to get the `region_id`, add a surrogate key for Pais, and load into Dim_Pais. This involves writing a SQL query to select distinct countries and their corresponding regions, executing the query, creating a DataFrame, joining with the `df_region` DataFrame to get the `region_id`, adding a `pais_id` surrogate key, renaming columns, selecting and ordering columns, and loading the DataFrame to BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "id": "439fd8b9",
        "outputId": "ce156daa-f724-44ea-ccaa-ce12cb296860"
      },
      "source": [
        "# Write a SQL query to select distinct countries and their regions\n",
        "query_pais = f\"\"\"\n",
        "SELECT DISTINCT `País`, Region\n",
        "FROM `{SOURCE_TABLE_ID}`\n",
        "WHERE `País` IS NOT NULL AND `País` != 'País' # Exclude header row if present in data, referencing with backticks\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query and load results into a DataFrame for Dim_Pais\n",
        "try:\n",
        "    df_pais = client.query(query_pais).to_dataframe()\n",
        "    print(f\"\\nExtracted {len(df_pais)} unique countries with regions.\")\n",
        "    display(df_pais.head())\n",
        "\n",
        "    # Join with the Dim_Region DataFrame to get the region_id\n",
        "    # Assuming df_region is available from the previous step\n",
        "    if 'df_region' in locals():\n",
        "        # Perform a left merge to add the region_id to the countries DataFrame\n",
        "        df_pais = pd.merge(df_pais, df_region[['region_id', 'region_nombre']],\n",
        "                           left_on='Region', right_on='region_nombre', how='left')\n",
        "\n",
        "        # Drop the redundant region_nombre column from the merge\n",
        "        df_pais.drop('region_nombre', axis=1, inplace=True)\n",
        "\n",
        "        # Handle cases where Region might not have matched (shouldn't happen if source data is clean)\n",
        "        if df_pais['region_id'].isnull().any():\n",
        "            print(\"Warning: Some countries did not match a region in Dim_Region.\")\n",
        "            # Depending on requirements, you might want to handle these rows (e.g., assign a default region_id, log, or drop)\n",
        "\n",
        "        # Add a pais_id column (surrogate key)\n",
        "        df_pais['pais_id'] = df_pais.reset_index().index + 1 # Simple 1-based integer ID\n",
        "\n",
        "        # Rename the country and region columns to match the schema\n",
        "        # Rename the original column name from the query result (`País`)\n",
        "        df_pais.rename(columns={'País': 'pais_nombre', 'Region': 'region_nombre_from_source'}, inplace=True) # Keep original region for reference if needed\n",
        "\n",
        "        # Select only the required columns and ensure order\n",
        "        df_pais = df_pais[['pais_id', 'pais_nombre', 'region_id']]\n",
        "\n",
        "\n",
        "        print(\"\\nTransformed Dim_Pais DataFrame head:\")\n",
        "        display(df_pais.head())\n",
        "        print(\"\\nTransformed Dim_Pais DataFrame info:\")\n",
        "        display(df_pais.info())\n",
        "\n",
        "\n",
        "        # Load the Dim_Pais DataFrame into the BigQuery table\n",
        "        job_config_pais = bigquery.LoadJobConfig(\n",
        "            schema=schema_dim_pais, # Use the defined schema\n",
        "            write_disposition=\"WRITE_TRUNCATE\", # Overwrite the table\n",
        "        )\n",
        "\n",
        "        job_pais = client.load_table_from_dataframe(\n",
        "            df_pais, table_id_dim_pais, job_config=job_config_pais\n",
        "        )\n",
        "\n",
        "        print(f\"\\nStarting BigQuery load job for Dim_Pais: {job_pais.job_id}\")\n",
        "        job_pais.result() # Wait for the job to complete\n",
        "\n",
        "        print(f\"DataFrame successfully loaded to {table_id_dim_pais}\")\n",
        "\n",
        "    else:\n",
        "        print(\"Error: df_region DataFrame not found. Please ensure the Dim_Region extraction step was executed successfully.\")\n",
        "        raise ValueError(\"Dim_Region DataFrame not available.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error processing Dim_Pais: {e}\")\n",
        "    raise # Re-raise the exception"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracted 40 unique countries with regions.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         País          Region\n",
              "0   Argentina  America Latina\n",
              "1      Brasil  America Latina\n",
              "2       Chile  America Latina\n",
              "3    Colombia  America Latina\n",
              "4  Costa Rica  America Latina"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-258d9c59-a3be-49c9-84e1-689b4f2ee774\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>País</th>\n",
              "      <th>Region</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Argentina</td>\n",
              "      <td>America Latina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Brasil</td>\n",
              "      <td>America Latina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Chile</td>\n",
              "      <td>America Latina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Colombia</td>\n",
              "      <td>America Latina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Costa Rica</td>\n",
              "      <td>America Latina</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-258d9c59-a3be-49c9-84e1-689b4f2ee774')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-258d9c59-a3be-49c9-84e1-689b4f2ee774 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-258d9c59-a3be-49c9-84e1-689b4f2ee774');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9094cb66-a47a-4f49-b84a-0faa93aaee34\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9094cb66-a47a-4f49-b84a-0faa93aaee34')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9094cb66-a47a-4f49-b84a-0faa93aaee34 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    raise # Re-raise the exception\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Pa\\u00eds\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Brasil\",\n          \"Costa Rica\",\n          \"Chile\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Region\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"America Latina\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Transformed Dim_Pais DataFrame head:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   pais_id pais_nombre  region_id\n",
              "0        1   Argentina          1\n",
              "1        2      Brasil          1\n",
              "2        3       Chile          1\n",
              "3        4    Colombia          1\n",
              "4        5  Costa Rica          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8422474a-86f0-46f1-9a07-f019f94697d4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pais_id</th>\n",
              "      <th>pais_nombre</th>\n",
              "      <th>region_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Brasil</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Chile</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Colombia</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Costa Rica</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8422474a-86f0-46f1-9a07-f019f94697d4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8422474a-86f0-46f1-9a07-f019f94697d4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8422474a-86f0-46f1-9a07-f019f94697d4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-76b5af30-7571-4f98-9698-77ac7c07121f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-76b5af30-7571-4f98-9698-77ac7c07121f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-76b5af30-7571-4f98-9698-77ac7c07121f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    raise # Re-raise the exception\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"pais_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pais_nombre\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Brasil\",\n          \"Costa Rica\",\n          \"Chile\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"region_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Transformed Dim_Pais DataFrame info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 40 entries, 0 to 39\n",
            "Data columns (total 3 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   pais_id      40 non-null     int64 \n",
            " 1   pais_nombre  40 non-null     object\n",
            " 2   region_id    40 non-null     int64 \n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 1.1+ KB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting BigQuery load job for Dim_Pais: 7faf49ac-a21e-466d-bf23-21fe1f44cc53\n",
            "DataFrame successfully loaded to etl-sistemas-sri.sri_presion_fiscal.Dim_Pais\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfbf9fbd"
      },
      "source": [
        "**Reasoning**:\n",
        "Extract unique years from the source table, add a surrogate key for Tiempo, and load into Dim_Tiempo. This involves writing a SQL query to select distinct years, executing the query, creating a DataFrame, adding a `tiempo_id` surrogate key, renaming the year column, selecting and ordering columns, and loading the DataFrame to BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "3b5e54b2",
        "outputId": "d23c6125-694f-4ef2-cc7b-912d38a34bce"
      },
      "source": [
        "# Write a SQL query to select distinct years\n",
        "query_tiempo = f\"\"\"\n",
        "SELECT DISTINCT `Año`\n",
        "FROM `{SOURCE_TABLE_ID}`\n",
        "WHERE `Año` IS NOT NULL # Keep the check for non-null years\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query and load results into a DataFrame for Dim_Tiempo\n",
        "try:\n",
        "    df_tiempo = client.query(query_tiempo).to_dataframe()\n",
        "    print(f\"\\nExtracted {len(df_tiempo)} unique years.\")\n",
        "    display(df_tiempo.head())\n",
        "\n",
        "    # Add a tiempo_id column (surrogate key)\n",
        "    # Simple 1-based integer ID based on the sorted years\n",
        "    df_tiempo['tiempo_id'] = df_tiempo['Año'].rank(method='dense').astype('Int64') # Using rank for dense ID\n",
        "\n",
        "    # Rename the year column to match the schema (already 'Año', but good practice)\n",
        "    df_tiempo.rename(columns={'Año': 'año'}, inplace=True)\n",
        "\n",
        "    # Select only the required columns and ensure order\n",
        "    df_tiempo = df_tiempo[['tiempo_id', 'año']]\n",
        "\n",
        "    print(\"\\nTransformed Dim_Tiempo DataFrame head:\")\n",
        "    display(df_tiempo.head())\n",
        "    print(\"\\nTransformed Dim_Tiempo DataFrame info:\")\n",
        "    display(df_tiempo.info())\n",
        "\n",
        "    # Load the Dim_Tiempo DataFrame into the BigQuery table\n",
        "    job_config_tiempo = bigquery.LoadJobConfig(\n",
        "        schema=schema_dim_tiempo, # Use the defined schema\n",
        "        write_disposition=\"WRITE_TRUNCATE\", # Overwrite the table\n",
        "    )\n",
        "\n",
        "    job_tiempo = client.load_table_from_dataframe(\n",
        "        df_tiempo, table_id_dim_tiempo, job_config=job_config_tiempo\n",
        "    )\n",
        "\n",
        "    print(f\"\\nStarting BigQuery load job for Dim_Tiempo: {job_tiempo.job_id}\")\n",
        "    job_tiempo.result() # Wait for the job to complete\n",
        "\n",
        "    print(f\"DataFrame successfully loaded to {table_id_dim_tiempo}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error processing Dim_Tiempo: {e}\")\n",
        "    raise # Re-raise the exception"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracted 20 unique years.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    Año\n",
              "0  2004\n",
              "1  2005\n",
              "2  2006\n",
              "3  2007\n",
              "4  2008"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b08c7937-adf9-4d68-94f9-fc42cbaeec39\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Año</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2008</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b08c7937-adf9-4d68-94f9-fc42cbaeec39')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b08c7937-adf9-4d68-94f9-fc42cbaeec39 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b08c7937-adf9-4d68-94f9-fc42cbaeec39');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-fa5be7f8-1442-4240-8880-790b29549a7a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fa5be7f8-1442-4240-8880-790b29549a7a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-fa5be7f8-1442-4240-8880-790b29549a7a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    raise # Re-raise the exception\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"A\\u00f1o\",\n      \"properties\": {\n        \"dtype\": \"Int64\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2005,\n          2008,\n          2006\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Transformed Dim_Tiempo DataFrame head:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   tiempo_id   año\n",
              "0          1  2004\n",
              "1          2  2005\n",
              "2          3  2006\n",
              "3          4  2007\n",
              "4          5  2008"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f14d43c5-6b73-40e7-89cc-0e65a729326a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tiempo_id</th>\n",
              "      <th>año</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2008</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f14d43c5-6b73-40e7-89cc-0e65a729326a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f14d43c5-6b73-40e7-89cc-0e65a729326a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f14d43c5-6b73-40e7-89cc-0e65a729326a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e00f6395-ee7d-4289-9792-e1000ecfad96\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e00f6395-ee7d-4289-9792-e1000ecfad96')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e00f6395-ee7d-4289-9792-e1000ecfad96 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    raise # Re-raise the exception\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"tiempo_id\",\n      \"properties\": {\n        \"dtype\": \"Int64\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"a\\u00f1o\",\n      \"properties\": {\n        \"dtype\": \"Int64\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2005,\n          2008,\n          2006\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Transformed Dim_Tiempo DataFrame info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20 entries, 0 to 19\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype\n",
            "---  ------     --------------  -----\n",
            " 0   tiempo_id  20 non-null     Int64\n",
            " 1   año        20 non-null     Int64\n",
            "dtypes: Int64(2)\n",
            "memory usage: 492.0 bytes\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting BigQuery load job for Dim_Tiempo: 2b6b2028-244e-4f6f-a361-50273a3939bf\n",
            "DataFrame successfully loaded to etl-sistemas-sri.sri_presion_fiscal.Dim_Tiempo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58fe9b4e"
      },
      "source": [
        "**Reasoning**:\n",
        "Extract unique values from the `Tipo_Presion` column in the source table, add a surrogate key for TipoPresion, and load into Dim_TipoPresion. This involves writing a SQL query to select distinct Tipo_Presion values, executing the query, creating a DataFrame, adding a `tipo_presion_id` surrogate key, renaming the column, selecting and ordering columns, and loading the DataFrame to BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682
        },
        "id": "f57531f4",
        "outputId": "f5e293b4-fa1b-4956-ca9c-b48f76aacae4"
      },
      "source": [
        "# Write a SQL query to select distinct Tipo_Presion values\n",
        "query_tipopresion = f\"\"\"\n",
        "SELECT DISTINCT Tipo_Presion\n",
        "FROM `{SOURCE_TABLE_ID}`\n",
        "WHERE Tipo_Presion IS NOT NULL AND Tipo_Presion != 'Tipo_Presion' # Exclude header row if present in data\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query and load results into a DataFrame for Dim_TipoPresion\n",
        "try:\n",
        "    df_tipopresion = client.query(query_tipopresion).to_dataframe()\n",
        "    print(f\"\\nExtracted {len(df_tipopresion)} unique Tipo_Presion values.\")\n",
        "    display(df_tipopresion.head())\n",
        "\n",
        "    # Add a tipo_presion_id column (surrogate key)\n",
        "    df_tipopresion['tipo_presion_id'] = df_tipopresion.reset_index().index + 1 # Simple 1-based integer ID\n",
        "\n",
        "    # Rename the Tipo_Presion column to match the schema\n",
        "    df_tipopresion.rename(columns={'Tipo_Presion': 'tipo_presion_nombre'}, inplace=True)\n",
        "\n",
        "    # Select only the required columns and ensure order\n",
        "    df_tipopresion = df_tipopresion[['tipo_presion_id', 'tipo_presion_nombre']]\n",
        "\n",
        "    print(\"\\nTransformed Dim_TipoPresion DataFrame head:\")\n",
        "    display(df_tipopresion.head())\n",
        "    print(\"\\nTransformed Dim_TipoPresion DataFrame info:\")\n",
        "    display(df_tipopresion.info())\n",
        "\n",
        "\n",
        "    # Load the Dim_TipoPresion DataFrame into the BigQuery table\n",
        "    job_config_tipopresion = bigquery.LoadJobConfig(\n",
        "        schema=schema_dim_tipopresion, # Use the defined schema\n",
        "        write_disposition=\"WRITE_TRUNCATE\", # Overwrite the table\n",
        "    )\n",
        "\n",
        "    job_tipopresion = client.load_table_from_dataframe(\n",
        "        df_tipopresion, table_id_dim_tipopresion, job_config=job_config_tipopresion\n",
        "    )\n",
        "\n",
        "    print(f\"\\nStarting BigQuery load job for Dim_TipoPresion: {job_tipopresion.job_id}\")\n",
        "    job_tipopresion.result() # Wait for the job to complete\n",
        "\n",
        "    print(f\"DataFrame successfully loaded to {table_id_dim_tipopresion}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error processing Dim_TipoPresion: {e}\")\n",
        "    raise # Re-raise the exception"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracted 4 unique Tipo_Presion values.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                              Tipo_Presion\n",
              "0                     Presión Fiscal Total\n",
              "1      Presión Tributaria Gobierno Central\n",
              "2  Presión Tributaria Gobierno Subnacional\n",
              "3     Contribuciones a la Seguridad Social"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6a60475b-a8f9-467b-aa40-395382fcf14e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tipo_Presion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Presión Tributaria Gobierno Central</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Presión Tributaria Gobierno Subnacional</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Contribuciones a la Seguridad Social</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a60475b-a8f9-467b-aa40-395382fcf14e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6a60475b-a8f9-467b-aa40-395382fcf14e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6a60475b-a8f9-467b-aa40-395382fcf14e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a4fbfb2c-5c36-403c-a18f-a8e31604695f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a4fbfb2c-5c36-403c-a18f-a8e31604695f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a4fbfb2c-5c36-403c-a18f-a8e31604695f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    raise # Re-raise the exception\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Tipo_Presion\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Presi\\u00f3n Tributaria Gobierno Central\",\n          \"Contribuciones a la Seguridad Social\",\n          \"Presi\\u00f3n Fiscal Total\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Transformed Dim_TipoPresion DataFrame head:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   tipo_presion_id                      tipo_presion_nombre\n",
              "0                1                     Presión Fiscal Total\n",
              "1                2      Presión Tributaria Gobierno Central\n",
              "2                3  Presión Tributaria Gobierno Subnacional\n",
              "3                4     Contribuciones a la Seguridad Social"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-84966037-b6c7-41e2-a204-ca798618b731\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tipo_presion_id</th>\n",
              "      <th>tipo_presion_nombre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Presión Tributaria Gobierno Central</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Presión Tributaria Gobierno Subnacional</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Contribuciones a la Seguridad Social</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-84966037-b6c7-41e2-a204-ca798618b731')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-84966037-b6c7-41e2-a204-ca798618b731 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-84966037-b6c7-41e2-a204-ca798618b731');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b2a69b2c-7557-443d-9fac-a4f39af26294\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b2a69b2c-7557-443d-9fac-a4f39af26294')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b2a69b2c-7557-443d-9fac-a4f39af26294 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    raise # Re-raise the exception\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"tipo_presion_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          4,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tipo_presion_nombre\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Presi\\u00f3n Tributaria Gobierno Central\",\n          \"Contribuciones a la Seguridad Social\",\n          \"Presi\\u00f3n Fiscal Total\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Transformed Dim_TipoPresion DataFrame info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4 entries, 0 to 3\n",
            "Data columns (total 2 columns):\n",
            " #   Column               Non-Null Count  Dtype \n",
            "---  ------               --------------  ----- \n",
            " 0   tipo_presion_id      4 non-null      int64 \n",
            " 1   tipo_presion_nombre  4 non-null      object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 196.0+ bytes\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting BigQuery load job for Dim_TipoPresion: 933ce413-254d-43d2-9277-ad430034eb0c\n",
            "DataFrame successfully loaded to etl-sistemas-sri.sri_presion_fiscal.Dim_TipoPresion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0264233"
      },
      "source": [
        "## ETL de Hechos (Fase 3)\n",
        "\n",
        "### Subtask:\n",
        "Definir el esquema para la tabla de hechos (por ejemplo, Fact_PresionFiscal), incluyendo claves foráneas a las tablas de dimensiones y métricas, y crear la tabla de hechos en BigQuery."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3cbcc60"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the schema for the fact table, including foreign keys and measures, and create the fact table in BigQuery. This involves defining the schema fields using `bigquery.SchemaField` and creating the table using `client.create_table`. I will use the previously defined `PROJECT_ID` and `DATASET_ID`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cc19a48",
        "outputId": "228ec02f-17be-4c97-fd1e-8b2026c0753d"
      },
      "source": [
        "# Define the schema for the Fact_PresionFiscal table\n",
        "# It includes foreign keys to dimension tables and the measure\n",
        "schema_fact = [\n",
        "    bigquery.SchemaField(\"region_id\", \"INT64\", mode=\"REQUIRED\"), # Foreign key to Dim_Region\n",
        "    bigquery.SchemaField(\"pais_id\", \"INT64\", mode=\"REQUIRED\"),     # Foreign key to Dim_Pais\n",
        "    bigquery.SchemaField(\"tiempo_id\", \"INT64\", mode=\"REQUIRED\"),   # Foreign key to Dim_Tiempo\n",
        "    bigquery.SchemaField(\"tipo_presion_id\", \"INT64\", mode=\"REQUIRED\"), # Foreign key to Dim_TipoPresion\n",
        "    bigquery.SchemaField(\"presion_fiscal\", \"FLOAT64\", mode=\"NULLABLE\"), # The measure (tax pressure percentage)\n",
        "]\n",
        "\n",
        "# Define the table ID for the Fact table\n",
        "# Using the unified PROJECT_ID and DATASET_ID\n",
        "table_id_fact = f\"{PROJECT_ID}.{DATASET_ID}.Fact_PresionFiscal\"\n",
        "\n",
        "# Create or get the Fact table in BigQuery\n",
        "def create_or_get_table(client, table_id, schema):\n",
        "    \"\"\"Creates a BigQuery table if it doesn't exist, or gets it if it does.\"\"\"\n",
        "    table_ref = bigquery.Table(table_id, schema=schema)\n",
        "    try:\n",
        "        table = client.create_table(table_ref, exists_ok=True)\n",
        "        print(f\"Table '{table_id}' created or already exists.\")\n",
        "        return table\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating table '{table_id}': {e}\")\n",
        "        raise\n",
        "\n",
        "table_fact = create_or_get_table(client, table_id_fact, schema_fact)\n",
        "\n",
        "print(f\"Schema defined and Fact table '{table_id_fact}' created or verified in BigQuery.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table 'etl-sistemas-sri.sri_presion_fiscal.Fact_PresionFiscal' created or already exists.\n",
            "Schema defined and Fact table 'etl-sistemas-sri.sri_presion_fiscal.Fact_PresionFiscal' created or verified in BigQuery.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cb3eb98"
      },
      "source": [
        "**Reasoning**:\n",
        "Extract transactional data from the source table, perform lookups with dimension tables to get surrogate keys, and prepare the data for loading into the fact table. This involves writing a SQL query to select the relevant columns from the source table, executing the query, creating a DataFrame, querying each dimension table to get the mapping between natural keys and surrogate keys, merging the source DataFrame with each dimension DataFrame using the natural keys, and selecting/renaming the columns to match the fact table schema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5d7b42f7",
        "outputId": "1a55ea85-cadd-4c40-b9e7-be050a78423b"
      },
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "import os\n",
        "import requests # Import requests for downloading\n",
        "\n",
        "# --- Download Source Data from URL ---\n",
        "csv_url = \"https://www.sri.gob.ec/o/sri-portlet-biblioteca-alfresco-internet/descargar/7e45627e-1f7e-4e21-ae59-d520634fc63f/SRI_Presion_Fiscal.csv\"\n",
        "local_filename = \"SRI_Presion_Fiscal.csv\" # Local name to save the downloaded file\n",
        "\n",
        "try:\n",
        "    print(f\"Attempting to download file from: {csv_url}\")\n",
        "    response = requests.get(csv_url)\n",
        "    response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "    with open(local_filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    print(f\"Archivo '{local_filename}' descargado exitosamente para procesamiento de hechos.\")\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error al descargar el archivo para procesamiento de hechos: {e}\")\n",
        "    raise # Re-raise the exception if download fails\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during download for fact processing: {e}\")\n",
        "    raise # Re-raise the exception\n",
        "\n",
        "# --- End of Download ---\n",
        "\n",
        "\n",
        "# --- Load and Transform Source Data from CSV (the downloaded file) ---\n",
        "try:\n",
        "    print(f\"\\nAttempting to load and transform downloaded CSV: {local_filename}\")\n",
        "    # Reload the CSV assuming semicolon as a separator and no header\n",
        "    df_source = pd.read_csv(local_filename, encoding='latin-1', header=None, sep=';')\n",
        "\n",
        "    # Remove the header row which is present as the first data row\n",
        "    df_source = df_source.iloc[1:].copy()\n",
        "\n",
        "    # Reset index after removing the row\n",
        "    df_source.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # Rename columns to meaningful names to match how they were handled in dimension lookups\n",
        "    df_source.columns = ['Region', 'País', 'Año', '%_Presion', 'Tipo_Presion']\n",
        "\n",
        "    # Clean up the '%_Presion' column by removing leading/trailing spaces\n",
        "    df_source['%_Presion'] = df_source['%_Presion'].astype(str).str.strip()\n",
        "\n",
        "    # Convert data types - crucial for lookups and BigQuery load\n",
        "    # Convert 'Año' to integer, coercing errors to NaN and then to Int64 (allows for NaNs)\n",
        "    df_source['Año'] = pd.to_numeric(df_source['Año'], errors='coerce').astype('Int64')\n",
        "\n",
        "    # Convert '%_Presion' to float, replacing comma with dot and coercing errors to NaN\n",
        "    df_source['%_Presion'] = df_source['%_Presion'].astype(str).str.replace(',', '.', regex=False)\n",
        "    df_source['%_Presion'] = pd.to_numeric(df_source['%_Presion'], errors='coerce')\n",
        "\n",
        "    print(\"Source data loaded and transformed from CSV successfully.\")\n",
        "    display(df_source.head())\n",
        "    display(df_source.info())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    # This should ideally not happen if download was successful, but kept for robustness\n",
        "    print(f\"Error: The source CSV file '{local_filename}' was not found after download.\")\n",
        "    raise # Re-raise the exception\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during source CSV processing: {e}\")\n",
        "    raise # Re-raise the exception\n",
        "\n",
        "# --- End of Load and Transform Source Data from CSV ---\n",
        "\n",
        "\n",
        "# --- Perform Lookups with Dimension Tables to get Surrogate Keys for Fact Table ---\n",
        "\n",
        "# Start with the transformed source DataFrame\n",
        "df_fact_staging = df_source.copy()\n",
        "\n",
        "# Lookup Geografia ID from Dim_Geografia\n",
        "# Assuming df_geografia is available from previous steps in the kernel\n",
        "if 'df_geografia' in locals() and isinstance(df_geografia, pd.DataFrame) and not df_geografia.empty:\n",
        "    # Merge df_fact_staging with df_geografia using 'País' and 'Region' as natural keys\n",
        "    df_fact_staging = pd.merge(df_fact_staging, df_geografia[['id_geografia', 'pais', 'region']],\n",
        "                               left_on=['País', 'Region'], right_on=['pais', 'region'], how='left')\n",
        "    # Drop the redundant natural key columns from the merge\n",
        "    # Keep original 'País' and 'Region' from source for potential debugging if needed before dropping them later\n",
        "    df_fact_staging.drop(['pais', 'region'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "    # Handle cases where Country/Region might not have matched a geography ID\n",
        "    if df_fact_staging['id_geografia'].isnull().any():\n",
        "        print(\"Warning: Some source rows did not match a geography ID in Dim_Geografia.\")\n",
        "        # Depending on requirements, you might want to handle these rows (e.g., assign a default id, log, or drop)\n",
        "\n",
        "else:\n",
        "    print(\"Error: df_geografia DataFrame not found or is empty for fact staging lookup. Ensure Dim_Geografia ETL was successful.\")\n",
        "    raise ValueError(\"Dim_Geografia DataFrame not available or is empty.\")\n",
        "\n",
        "\n",
        "# Lookup Tiempo ID\n",
        "# Assuming df_tiempo is available from previous steps in the kernel\n",
        "if 'df_tiempo' in locals() and isinstance(df_tiempo, pd.DataFrame) and not df_tiempo.empty:\n",
        "    # Ensure 'Año' column in df_tiempo is in the same type (Int64) as in df_fact_staging before merge\n",
        "    # The transformation step already converted 'Año' in df_source to Int64\n",
        "    df_fact_staging = pd.merge(df_fact_staging, df_tiempo[['tiempo_id', 'año']],\n",
        "                               left_on='Año', right_on='año', how='left')\n",
        "    # Drop the redundant columns and the natural key after merge\n",
        "    df_fact_staging.drop(['año', 'Año'], axis=1, inplace=True)\n",
        "else:\n",
        "    print(\"Error: df_tiempo DataFrame not found or is empty for fact staging lookup. Ensure Dim_Tiempo ETL was successful.\")\n",
        "    raise ValueError(\"Dim_Tiempo DataFrame not available or is empty.\")\n",
        "\n",
        "\n",
        "# Lookup TipoPresion ID\n",
        "# Assuming df_tipopresion is available from previous steps in the kernel\n",
        "if 'df_tipopresion' in locals() and isinstance(df_tipopresion, pd.DataFrame) and not df_tipopresion.empty:\n",
        "    df_fact_staging = pd.merge(df_fact_staging, df_tipopresion[['tipo_presion_id', 'tipo_presion_nombre']],\n",
        "                               left_on='Tipo_Presion', right_on='tipo_presion_nombre', how='left')\n",
        "    # Drop the redundant columns and the natural key after merge\n",
        "    df_fact_staging.drop(['tipo_presion_nombre', 'Tipo_Presion'], axis=1, inplace=True)\n",
        "else:\n",
        "    print(\"Error: df_tipopresion DataFrame not found or is empty for fact staging lookup. Ensure Dim_TipoPresion ETL was successful.\")\n",
        "    raise ValueError(\"Dim_TipoPresion DataFrame not available or is empty.\")\n",
        "\n",
        "# --- End of Lookups ---\n",
        "\n",
        "# Rename the measure column to match the fact schema\n",
        "df_fact_staging.rename(columns={'%_Presion': 'presion_fiscal'}, inplace=True)\n",
        "\n",
        "# Select and reorder columns to match the fact table schema\n",
        "# Ensure all required surrogate key columns are present after merges\n",
        "# The required columns are: id_geografia, tiempo_id, tipo_presion_id, presion_fiscal\n",
        "required_fact_cols = ['id_geografia', 'tiempo_id', 'tipo_presion_id', 'presion_fiscal']\n",
        "if not all(col in df_fact_staging.columns for col in required_fact_cols):\n",
        "    missing = [col for col in required_fact_cols if col not in df_fact_staging.columns]\n",
        "    # Also check for potential NaN values in merged IDs which indicate unmatched keys\n",
        "    nan_in_ids = df_fact_staging[required_fact_cols[:-1]].isnull().any()\n",
        "    if nan_in_ids.any():\n",
        "         cols_with_nan_ids = nan_in_ids[nan_in_ids].index.tolist()\n",
        "         print(f\"Warning: NaN values found in surrogate key columns after merges: {cols_with_nan_ids}. Rows with unmatched natural keys will result in NULL foreign keys in BigQuery.\")\n",
        "\n",
        "    raise ValueError(f\"Missing required columns in fact DataFrame after merges: {missing}. Check merge keys and dimension data availability.\")\n",
        "\n",
        "\n",
        "df_fact = df_fact_staging[required_fact_cols]\n",
        "\n",
        "# Ensure surrogate key columns are of type Int64 (nullable integer)\n",
        "for col_id in ['id_geografia', 'tiempo_id', 'tipo_presion_id']:\n",
        "    if col_id in df_fact.columns:\n",
        "        # Convert to Int64 to allow for potential NaN values from left merge\n",
        "        df_fact[col_id] = df_fact[col_id].astype('Int64')\n",
        "\n",
        "\n",
        "print(\"\\nTransformed Fact DataFrame head (with surrogate keys):\")\n",
        "display(df_fact.head())\n",
        "print(\"\\nTransformed Fact DataFrame info:\")\n",
        "display(df_fact.info())\n",
        "\n",
        "\n",
        "# --- Load the Fact DataFrame into the BigQuery table ---\n",
        "# Assuming BigQuery client and table_id_fact are available from previous steps\n",
        "\n",
        "# Re-load credentials and initialize client if necessary (ensure context)\n",
        "# This assumes the environment variable GOOGLE_APPLICATION_CREDENTIALS is set correctly\n",
        "try:\n",
        "    # Use the unified PROJECT_ID from previous successful dimension loads\n",
        "    PROJECT_ID = 'etl-sistemas-sri'\n",
        "    client = bigquery.Client(project=PROJECT_ID)\n",
        "    print(f\"\\nBigQuery client re-initialized for project: {PROJECT_ID} for fact loading.\")\n",
        "\n",
        "    # Redefine table_id_fact using the unified PROJECT_ID\n",
        "    DATASET_ID = \"sri_presion_fiscal\"\n",
        "    table_id_fact = f\"{PROJECT_ID}.{DATASET_ID}.Fact_PresionFiscal\"\n",
        "    print(f\"Fact table ID set to: {table_id_fact}\")\n",
        "\n",
        "    # Define the schema for the Fact_PresionFiscal table (should match the updated schema)\n",
        "    schema_fact = [\n",
        "        bigquery.SchemaField(\"id_geografia\", \"INT64\", mode=\"REQUIRED\"), # Foreign key to Dim_Geografia\n",
        "        bigquery.SchemaField(\"tiempo_id\", \"INT64\", mode=\"REQUIRED\"),   # Foreign key to Dim_Tiempo\n",
        "        bigquery.SchemaField(\"tipo_presion_id\", \"INT64\", mode=\"REQUIRED\"), # Foreign key to Dim_TipoPresion\n",
        "        bigquery.SchemaField(\"presion_fiscal\", \"FLOAT64\", mode=\"NULLABLE\"), # The measure (tax pressure percentage)\n",
        "    ]\n",
        "\n",
        "\n",
        "    job_config_fact = bigquery.LoadJobConfig(\n",
        "        schema=schema_fact, # Use the defined schema for the fact table\n",
        "        write_disposition=\"WRITE_TRUNCATE\", # Overwrite the table\n",
        "    )\n",
        "\n",
        "    job_fact = client.load_table_from_dataframe(\n",
        "        df_fact, table_id_fact, job_config=job_config_fact\n",
        "    )\n",
        "\n",
        "    print(f\"\\nStarting BigQuery load job for Fact_PresionFiscal: {job_fact.job_id}\")\n",
        "    job_fact.result() # Wait for the job to complete\n",
        "\n",
        "    print(f\"DataFrame successfully loaded to {table_id_fact}\")\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error processing Fact_PresionFiscal: {e}\")\n",
        "    raise # Re-raise the exception"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to download file from: https://www.sri.gob.ec/o/sri-portlet-biblioteca-alfresco-internet/descargar/7e45627e-1f7e-4e21-ae59-d520634fc63f/SRI_Presion_Fiscal.csv\n",
            "Archivo 'SRI_Presion_Fiscal.csv' descargado exitosamente para procesamiento de hechos.\n",
            "\n",
            "Attempting to load and transform downloaded CSV: SRI_Presion_Fiscal.csv\n",
            "Source data loaded and transformed from CSV successfully.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "           Region       País   Año  %_Presion          Tipo_Presion\n",
              "0  America Latina  Argentina  2004     0.2433  Presión Fiscal Total\n",
              "1  America Latina  Argentina  2005     0.2453  Presión Fiscal Total\n",
              "2  America Latina  Argentina  2006     0.2508  Presión Fiscal Total\n",
              "3  America Latina  Argentina  2007     0.2637  Presión Fiscal Total\n",
              "4  America Latina  Argentina  2008     0.2762  Presión Fiscal Total"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fdd45a43-95bb-47c9-9e3a-3c5bdaedc264\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Region</th>\n",
              "      <th>País</th>\n",
              "      <th>Año</th>\n",
              "      <th>%_Presion</th>\n",
              "      <th>Tipo_Presion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2004</td>\n",
              "      <td>0.2433</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2005</td>\n",
              "      <td>0.2453</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2006</td>\n",
              "      <td>0.2508</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2007</td>\n",
              "      <td>0.2637</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2008</td>\n",
              "      <td>0.2762</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fdd45a43-95bb-47c9-9e3a-3c5bdaedc264')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fdd45a43-95bb-47c9-9e3a-3c5bdaedc264 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fdd45a43-95bb-47c9-9e3a-3c5bdaedc264');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d9a59ae1-e268-49c9-b944-2f6c83946146\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d9a59ae1-e268-49c9-b944-2f6c83946146')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d9a59ae1-e268-49c9-b944-2f6c83946146 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    raise # Re-raise the exception\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Region\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"America Latina\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pa\\u00eds\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Argentina\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"A\\u00f1o\",\n      \"properties\": {\n        \"dtype\": \"Int64\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2005\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"%_Presion\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.01387490540508295,\n        \"min\": 0.2433,\n        \"max\": 0.2762,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.2453\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tipo_Presion\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Presi\\u00f3n Fiscal Total\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3043 entries, 0 to 3042\n",
            "Data columns (total 5 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Region        3043 non-null   object \n",
            " 1   País          3043 non-null   object \n",
            " 2   Año           3043 non-null   Int64  \n",
            " 3   %_Presion     3043 non-null   float64\n",
            " 4   Tipo_Presion  3043 non-null   object \n",
            "dtypes: Int64(1), float64(1), object(3)\n",
            "memory usage: 122.0+ KB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m/tmp/\u001b[0m\u001b[1;33mipython-input-41-422745439.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m149\u001b[0m\u001b[1;33m SettingWithCopyWarning\u001b[0m\u001b[33m: \u001b[0m\n",
              "\u001b[33mA value is trying to be set on a copy of a slice from a DataFrame.\u001b[0m\n",
              "\u001b[33mTry using .loc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[33mrow_indexer,col_indexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[33m = value instead\u001b[0m\n",
              "\n",
              "\u001b[33mSee the caveats in the documentation: \u001b[0m\u001b[4;33mhttps://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/tmp/ipython-input-41-422745439.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">149</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> SettingWithCopyWarning</span><span style=\"color: #808000; text-decoration-color: #808000\">: </span>\n",
              "<span style=\"color: #808000; text-decoration-color: #808000\">A value is trying to be set on a copy of a slice from a DataFrame.</span>\n",
              "<span style=\"color: #808000; text-decoration-color: #808000\">Try using .loc</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">[</span><span style=\"color: #808000; text-decoration-color: #808000\">row_indexer,col_indexer</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\"> = value instead</span>\n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000\">See the caveats in the documentation: </span><span style=\"color: #808000; text-decoration-color: #808000; text-decoration: underline\">https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Transformed Fact DataFrame head (with surrogate keys):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   id_geografia  tiempo_id  tipo_presion_id  presion_fiscal\n",
              "0             2          1                1          0.2433\n",
              "1             2          2                1          0.2453\n",
              "2             2          3                1          0.2508\n",
              "3             2          4                1          0.2637\n",
              "4             2          5                1          0.2762"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-140c2bd8-7284-42b9-9857-caa80e7c5f00\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_geografia</th>\n",
              "      <th>tiempo_id</th>\n",
              "      <th>tipo_presion_id</th>\n",
              "      <th>presion_fiscal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2762</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-140c2bd8-7284-42b9-9857-caa80e7c5f00')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-140c2bd8-7284-42b9-9857-caa80e7c5f00 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-140c2bd8-7284-42b9-9857-caa80e7c5f00');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-73f10cea-76fc-4ce0-8779-06a67fcbaad6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-73f10cea-76fc-4ce0-8779-06a67fcbaad6')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-73f10cea-76fc-4ce0-8779-06a67fcbaad6 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    raise # Re-raise the exception\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id_geografia\",\n      \"properties\": {\n        \"dtype\": \"Int64\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tiempo_id\",\n      \"properties\": {\n        \"dtype\": \"Int64\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tipo_presion_id\",\n      \"properties\": {\n        \"dtype\": \"Int64\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"presion_fiscal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.01387490540508295,\n        \"min\": 0.2433,\n        \"max\": 0.2762,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.2453\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Transformed Fact DataFrame info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3043 entries, 0 to 3042\n",
            "Data columns (total 4 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   id_geografia     3043 non-null   Int64  \n",
            " 1   tiempo_id        3043 non-null   Int64  \n",
            " 2   tipo_presion_id  3043 non-null   Int64  \n",
            " 3   presion_fiscal   3043 non-null   float64\n",
            "dtypes: Int64(3), float64(1)\n",
            "memory usage: 104.1 KB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BigQuery client re-initialized for project: etl-sistemas-sri for fact loading.\n",
            "Fact table ID set to: etl-sistemas-sri.sri_presion_fiscal.Fact_PresionFiscal\n",
            "\n",
            "Starting BigQuery load job for Fact_PresionFiscal: 4fd45db1-0b39-4920-93c9-b6a0116b8e20\n",
            "DataFrame successfully loaded to etl-sistemas-sri.sri_presion_fiscal.Fact_PresionFiscal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8bc285d"
      },
      "source": [
        "## Ajuste al Esquema Estrella: Dimensión Geografía\n",
        "\n",
        "### Subtask:\n",
        "Eliminar las tablas `Dim_Region` y `Dim_Pais` existentes, definir el esquema para la nueva tabla `Dim_Geografia` (`ID_Geografia` (PK), `Pais` (varchar), `Region` (varchar)) y crear esta tabla en BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e2bd82c",
        "outputId": "ef54a5df-c228-4338-f156-e09bb74296f9"
      },
      "source": [
        "# Assuming BigQuery client is already initialized and PROJECT_ID, DATASET_ID are defined\n",
        "\n",
        "# Define table IDs for the dimensions to be deleted\n",
        "table_id_dim_region_to_delete = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Region\"\n",
        "table_id_dim_pais_to_delete = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Pais\"\n",
        "\n",
        "# Delete the existing Dim_Region table\n",
        "try:\n",
        "    client.delete_table(table_id_dim_region_to_delete, not_found_ok=True)\n",
        "    print(f\"Table '{table_id_dim_region_to_delete}' deleted if it existed.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error deleting table '{table_id_dim_region_to_delete}': {e}\")\n",
        "    # Decide if this error should stop the process\n",
        "\n",
        "# Delete the existing Dim_Pais table\n",
        "try:\n",
        "    client.delete_table(table_id_dim_pais_to_delete, not_found_ok=True)\n",
        "    print(f\"Table '{table_id_dim_pais_to_delete}' deleted if it existed.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error deleting table '{table_id_dim_pais_to_delete}': {e}\")\n",
        "    # Decide if this error should stop the process\n",
        "\n",
        "\n",
        "# Define schema for the new Dim_Geografia table\n",
        "schema_dim_geografia = [\n",
        "    bigquery.SchemaField(\"id_geografia\", \"INT64\", mode=\"REQUIRED\"), # Primary Key\n",
        "    bigquery.SchemaField(\"pais\", \"STRING\", mode=\"REQUIRED\"),      # Country\n",
        "    bigquery.SchemaField(\"region\", \"STRING\", mode=\"REQUIRED\"),    # Region\n",
        "]\n",
        "\n",
        "# Define table ID for Dim_Geografia\n",
        "table_id_dim_geografia = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Geografia\"\n",
        "\n",
        "# Create or get the Dim_Geografia table in BigQuery\n",
        "def create_or_get_table(client, table_id, schema):\n",
        "    \"\"\"Creates a BigQuery table if it doesn't exist, or gets it if it does.\"\"\"\n",
        "    table_ref = bigquery.Table(table_id, schema=schema)\n",
        "    try:\n",
        "        table = client.create_table(table_ref, exists_ok=True)\n",
        "        print(f\"Table '{table_id}' created or already exists.\")\n",
        "        return table\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating table '{table_id}': {e}\")\n",
        "        raise\n",
        "\n",
        "table_dim_geografia = create_or_get_table(client, table_id_dim_geografia, schema_dim_geografia)\n",
        "\n",
        "print(f\"\\nSchema defined and Dim_Geografia table '{table_id_dim_geografia}' created or verified in BigQuery.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table 'etl-sistemas-sri.sri_presion_fiscal.Dim_Region' deleted if it existed.\n",
            "Table 'etl-sistemas-sri.sri_presion_fiscal.Dim_Pais' deleted if it existed.\n",
            "Table 'etl-sistemas-sri.sri_presion_fiscal.Dim_Geografia' created or already exists.\n",
            "\n",
            "Schema defined and Dim_Geografia table 'etl-sistemas-sri.sri_presion_fiscal.Dim_Geografia' created or verified in BigQuery.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0824768a"
      },
      "source": [
        "## Poblar Dim_Geografia\n",
        "\n",
        "### Subtask:\n",
        "Extraer combinaciones únicas de País y Región de la tabla de origen, generar una clave subrogada para `Dim_Geografia` y cargarla en la tabla de BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "1946df89",
        "outputId": "0ef31998-f7c6-4be3-930c-24f38849eb51"
      },
      "source": [
        "# Write a SQL query to select distinct Country and Region combinations\n",
        "query_geografia = f\"\"\"\n",
        "SELECT DISTINCT `País`, Region\n",
        "FROM `{SOURCE_TABLE_ID}`\n",
        "WHERE `País` IS NOT NULL AND `País` != 'País' -- Exclude header row if present\n",
        "  AND Region IS NOT NULL AND Region != 'Region' -- Exclude header row if present\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query and load results into a DataFrame for Dim_Geografia\n",
        "try:\n",
        "    df_geografia = client.query(query_geografia).to_dataframe()\n",
        "    print(f\"\\nExtracted {len(df_geografia)} unique Country-Region combinations.\")\n",
        "    display(df_geografia.head())\n",
        "\n",
        "    # Add an id_geografia column (surrogate key)\n",
        "    # Sort by País and Region before generating ID for consistency\n",
        "    df_geografia = df_geografia.sort_values(by=['País', 'Region']).reset_index(drop=True)\n",
        "    df_geografia['id_geografia'] = df_geografia.index + 1 # Simple 1-based integer ID\n",
        "\n",
        "    # Rename columns to match the schema\n",
        "    df_geografia.rename(columns={'País': 'pais', 'Region': 'region'}, inplace=True)\n",
        "\n",
        "    # Select only the required columns and ensure order\n",
        "    df_geografia = df_geografia[['id_geografia', 'pais', 'region']]\n",
        "\n",
        "    print(\"\\nTransformed Dim_Geografia DataFrame head:\")\n",
        "    display(df_geografia.head())\n",
        "    print(\"\\nTransformed Dim_Geografia DataFrame info:\")\n",
        "    display(df_geografia.info())\n",
        "\n",
        "\n",
        "    # Load the Dim_Geografia DataFrame into the BigQuery table\n",
        "    job_config_geografia = bigquery.LoadJobConfig(\n",
        "        schema=schema_dim_geografia, # Use the defined schema for Dim_Geografia\n",
        "        write_disposition=\"WRITE_TRUNCATE\", # Overwrite the table\n",
        "    )\n",
        "\n",
        "    job_geografia = client.load_table_from_dataframe(\n",
        "        df_geografia, table_id_dim_geografia, job_config=job_config_geografia\n",
        "    )\n",
        "\n",
        "    print(f\"\\nStarting BigQuery load job for Dim_Geografia: {job_geografia.job_id}\")\n",
        "    job_geografia.result() # Wait for the job to complete\n",
        "\n",
        "    print(f\"DataFrame successfully loaded to {table_id_dim_geografia}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error processing Dim_Geografia: {e}\")\n",
        "    raise # Re-raise the exception"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracted 40 unique Country-Region combinations.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         País          Region\n",
              "0   Argentina  America Latina\n",
              "1      Brasil  America Latina\n",
              "2       Chile  America Latina\n",
              "3    Colombia  America Latina\n",
              "4  Costa Rica  America Latina"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bb0e384c-4443-4ae7-9fb4-37459fd8922d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>País</th>\n",
              "      <th>Region</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Argentina</td>\n",
              "      <td>America Latina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Brasil</td>\n",
              "      <td>America Latina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Chile</td>\n",
              "      <td>America Latina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Colombia</td>\n",
              "      <td>America Latina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Costa Rica</td>\n",
              "      <td>America Latina</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb0e384c-4443-4ae7-9fb4-37459fd8922d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bb0e384c-4443-4ae7-9fb4-37459fd8922d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bb0e384c-4443-4ae7-9fb4-37459fd8922d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1cf818ef-1d29-4c15-afcd-a5c998b2453a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1cf818ef-1d29-4c15-afcd-a5c998b2453a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1cf818ef-1d29-4c15-afcd-a5c998b2453a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    raise # Re-raise the exception\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Pa\\u00eds\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Brasil\",\n          \"Costa Rica\",\n          \"Chile\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Region\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"America Latina\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Transformed Dim_Geografia DataFrame head:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   id_geografia       pais          region\n",
              "0             1   Alemania          Europa\n",
              "1             2  Argentina  America Latina\n",
              "2             3    Austria          Europa\n",
              "3             4     Brasil  America Latina\n",
              "4             5    Bélgica          Europa"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-658728ab-206d-44d9-afc9-0fcb50c580b1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_geografia</th>\n",
              "      <th>pais</th>\n",
              "      <th>region</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Alemania</td>\n",
              "      <td>Europa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>America Latina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Austria</td>\n",
              "      <td>Europa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Brasil</td>\n",
              "      <td>America Latina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Bélgica</td>\n",
              "      <td>Europa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-658728ab-206d-44d9-afc9-0fcb50c580b1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-658728ab-206d-44d9-afc9-0fcb50c580b1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-658728ab-206d-44d9-afc9-0fcb50c580b1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-52f686c1-e0a4-4d40-af01-2c44c9062856\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-52f686c1-e0a4-4d40-af01-2c44c9062856')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-52f686c1-e0a4-4d40-af01-2c44c9062856 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    raise # Re-raise the exception\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id_geografia\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pais\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Argentina\",\n          \"B\\u00e9lgica\",\n          \"Austria\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"region\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"America Latina\",\n          \"Europa\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Transformed Dim_Geografia DataFrame info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 40 entries, 0 to 39\n",
            "Data columns (total 3 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   id_geografia  40 non-null     int64 \n",
            " 1   pais          40 non-null     object\n",
            " 2   region        40 non-null     object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 1.1+ KB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting BigQuery load job for Dim_Geografia: 7829be42-5c29-48b8-bec3-5de27d2e604c\n",
            "DataFrame successfully loaded to etl-sistemas-sri.sri_presion_fiscal.Dim_Geografia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7fd8563"
      },
      "source": [
        "## Ajuste al Esquema Estrella: Tabla de Hechos\n",
        "\n",
        "### Subtask:\n",
        "Modificar el esquema de la tabla `Fact_PresionFiscal` para reemplazar `region_id` y `pais_id` con `id_geografia`, y recrear la tabla en BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6149d72a",
        "outputId": "9e4b45f0-db1a-4d2a-bd47-9ce0655be2ce"
      },
      "source": [
        "# Assuming BigQuery client is initialized and PROJECT_ID, DATASET_ID are defined\n",
        "\n",
        "# Define the table ID for the Fact table\n",
        "table_id_fact = f\"{PROJECT_ID}.{DATASET_ID}.Fact_PresionFiscal\"\n",
        "\n",
        "# Delete the existing Fact_PresionFiscal table\n",
        "try:\n",
        "    client.delete_table(table_id_fact, not_found_ok=True)\n",
        "    print(f\"Table '{table_id_fact}' deleted if it existed to apply schema changes.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error deleting table '{table_id_fact}': {e}\")\n",
        "    # Decide if this error should stop the process\n",
        "\n",
        "\n",
        "# Define the UPDATED schema for the Fact_PresionFiscal table\n",
        "# It now includes id_geografia instead of region_id and pais_id\n",
        "schema_fact = [\n",
        "    bigquery.SchemaField(\"id_geografia\", \"INT64\", mode=\"REQUIRED\"), # Foreign key to Dim_Geografia\n",
        "    bigquery.SchemaField(\"tiempo_id\", \"INT64\", mode=\"REQUIRED\"),   # Foreign key to Dim_Tiempo\n",
        "    bigquery.SchemaField(\"tipo_presion_id\", \"INT64\", mode=\"REQUIRED\"), # Foreign key to Dim_TipoPresion\n",
        "    bigquery.SchemaField(\"presion_fiscal\", \"FLOAT64\", mode=\"NULLABLE\"), # The measure (tax pressure percentage)\n",
        "]\n",
        "\n",
        "# Create the Fact table in BigQuery with the updated schema\n",
        "def create_or_get_table(client, table_id, schema):\n",
        "    \"\"\"Creates a BigQuery table if it doesn't exist, or gets it if it does.\"\"\"\n",
        "    table_ref = bigquery.Table(table_id, schema=schema)\n",
        "    try:\n",
        "        table = client.create_table(table_ref, exists_ok=True)\n",
        "        print(f\"Table '{table_id}' created or already exists.\")\n",
        "        return table\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating table '{table_id}': {e}\")\n",
        "        raise\n",
        "\n",
        "table_fact = create_or_get_table(client, table_id_fact, schema_fact)\n",
        "\n",
        "print(f\"\\nUpdated schema defined and Fact table '{table_id_fact}' recreated or verified in BigQuery.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table 'etl-sistemas-sri.sri_presion_fiscal.Fact_PresionFiscal' deleted if it existed to apply schema changes.\n",
            "Table 'etl-sistemas-sri.sri_presion_fiscal.Fact_PresionFiscal' created or already exists.\n",
            "\n",
            "Updated schema defined and Fact table 'etl-sistemas-sri.sri_presion_fiscal.Fact_PresionFiscal' recreated or verified in BigQuery.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4e08dd2"
      },
      "source": [
        "## Finalizar tarea\n",
        "\n",
        "### Subtask:\n",
        "Proporcionar el código completo del DAG de Airflow generado y las instrucciones sobre cómo usarlo en un entorno de Airflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1d7214d",
        "outputId": "8550bbe5-dd30-4912-a4db-9e444a1d3c91"
      },
      "source": [
        "# Display the content of the generated DAG file\n",
        "%cat etl_presion_fiscal_dag.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# Import necessary modules from Apache Airflow and other libraries\n",
            "from airflow import DAG # DAG object to define the workflow\n",
            "from datetime import datetime, timedelta # For specifying start_date and retry_delay\n",
            "import pandas as pd # For data transformation using DataFrames\n",
            "from airflow.operators.python import PythonOperator # Operator to run Python functions\n",
            "# from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator # While used before, we are now using PythonOperator for loading\n",
            "from google.cloud import bigquery # For interacting with Google BigQuery API\n",
            "import os # For accessing environment variables\n",
            "from google.oauth2 import service_account # For loading Google Cloud service account credentials\n",
            "import requests # For downloading the CSV file\n",
            "\n",
            "# Define default arguments for the DAG\n",
            "# These arguments are passed to all tasks unless overridden\n",
            "default_args = {\n",
            "    'owner': 'airflow', # The owner of the DAG\n",
            "    'depends_on_past': False, # Set to true if you want tasks to wait for the previous run to succeed\n",
            "    'start_date': datetime(2023, 1, 1), # The date when the DAG starts being scheduled\n",
            "    'email_on_failure': False, # Set to true to receive emails on task failure\n",
            "    'email_on_retry': False, # Set to true to receive emails on task retry\n",
            "    'retries': 1, # Number of times to retry a task on failure\n",
            "    'retry_delay': timedelta(minutes=5), # Time to wait between retries\n",
            "}\n",
            "\n",
            "# Define the DAG object\n",
            "# 'etl_presion_fiscal_sri' is the unique DAG ID\n",
            "with DAG(\n",
            "    'etl_presion_fiscal_sri',\n",
            "    default_args=default_args, # Apply the default arguments\n",
            "    description='ETL pipeline to download, transform, and load SRI tax pressure data to BigQuery', # A brief description of the DAG\n",
            "    schedule=None, # Set the schedule interval using a cron expression or timedelta, or None for manual runs\n",
            "    catchup=False, # Set to true to run the DAG for past dates starting from the start_date\n",
            "    tags=['etl', 'bigquery', 'sri'], # Tags to help categorize and filter DAGs in the Airflow UI\n",
            ") as dag:\n",
            "\n",
            "    # Define the local filename for the downloaded CSV\n",
            "    local_filename = \"SRI_Presion_Fiscal.csv\"\n",
            "\n",
            "    # Define the filename for the Google Cloud service account credentials\n",
            "    credential_filename = \"presionfiscaletl-ab366d04e3b7.json\" # !!! Replace with your actual filename if different in your Airflow environment !!!\n",
            "    # Set the environment variable for Google Cloud authentication\n",
            "    # Note: In a real Airflow environment, prefer using Airflow Connections for GCP\n",
            "    # If using Airflow Connections, you would remove this os.environ line and rely on the connection setup.\n",
            "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_filename\n",
            "\n",
            "\n",
            "    # Define the Python function for the download task\n",
            "    def download_csv_from_url(csv_url, local_filename):\n",
            "        \"\"\"Downloads a CSV file from a given URL and saves it locally.\"\"\"\n",
            "        # import requests # Already imported at the top level\n",
            "        print(f\"Attempting to download file from: {csv_url}\")\n",
            "        try:\n",
            "            response = requests.get(csv_url)\n",
            "            response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
            "\n",
            "            with open(local_filename, 'wb') as f:\n",
            "                f.write(response.content)\n",
            "\n",
            "            print(f\"Archivo '{local_filename}' descargado exitosamente.\")\n",
            "\n",
            "        except requests.exceptions.RequestException as e:\n",
            "            print(f\"Error al descargar el archivo: {e}\")\n",
            "            # Raise an exception to fail the task in Airflow\n",
            "            raise ValueError(f\"Download task failed: {e}\")\n",
            "        except Exception as e:\n",
            "            print(f\"An unexpected error occurred during download: {e}\")\n",
            "            raise ValueError(f\"Download task failed unexpectedly: {e}\")\n",
            "\n",
            "\n",
            "    # Instantiate the PythonOperator for the download task\n",
            "    # Executes the 'download_csv_from_url' function\n",
            "    download_csv_task = PythonOperator(\n",
            "        task_id='download_csv', # Unique identifier for the task within the DAG\n",
            "        python_callable=download_csv_from_url, # The Python function to execute\n",
            "        op_kwargs={'csv_url': csv_url, 'local_filename': local_filename}, # Keyword arguments to pass to the function\n",
            "        # dag=dag, # Associate the task with the DAG object implicitly when using 'with DAG(...)'\n",
            "    )\n",
            "\n",
            "\n",
            "    # Define the Python function for the data transformation task\n",
            "    def transform_data(local_filename):\n",
            "        \"\"\"\n",
            "        Loads data from a CSV, cleans and transforms it, and returns a DataFrame.\n",
            "        This function replicates the successful transformation steps from the notebook.\n",
            "        \"\"\"\n",
            "        # import pandas as pd # Already imported at the top level\n",
            "        print(f\"Attempting to transform data from: {local_filename}\")\n",
            "        try:\n",
            "            # Reload the CSV assuming semicolon as a separator and no header\n",
            "            df = pd.read_csv(local_filename, encoding='latin-1', header=None, sep=';')\n",
            "\n",
            "            # Remove the header row which is present as the first data row\n",
            "            df = df.iloc[1:].copy()\n",
            "\n",
            "            # Reset index after removing the row\n",
            "            df.reset_index(drop=True, inplace=True)\n",
            "\n",
            "            # Rename columns to meaningful names\n",
            "            df.columns = ['Region', 'País', 'Año', '%_Presion', 'Tipo_Presion']\n",
            "\n",
            "            # Clean up the '%_Presion' column by removing leading/trailing spaces\n",
            "            df['%_Presion'] = df['%_Presion'].astype(str).str.strip()\n",
            "\n",
            "            # Convert data types\n",
            "            # Convert 'Año' to integer, coercing errors to NaN and then to Int64 (allows for NaNs)\n",
            "            df['Año'] = pd.to_numeric(df['Año'], errors='coerce').astype('Int64')\n",
            "\n",
            "            # Convert '%_Presion' to float, replacing comma with dot and coercing errors to NaN\n",
            "            df['%_Presion'] = df['%_Presion'].astype(str).str.replace(',', '.', regex=False)\n",
            "            df['%_Presion'] = pd.to_numeric(df['%_Presion'], errors='coerce')\n",
            "\n",
            "            print(\"Data transformation completed successfully.\")\n",
            "            # In a real Airflow task, avoid printing large DataFrames directly\n",
            "            # display(df.head()) # display() is for notebooks, not standard Python scripts\n",
            "            # print(df.head().to_string()) # Use to_string() for printing in a script\n",
            "\n",
            "            # print(\"\\nTransformed DataFrame info:\")\n",
            "            # df.info() # info() prints to console in a script\n",
            "\n",
            "            # Return the transformed DataFrame so it can be pushed to XCom\n",
            "            return df\n",
            "\n",
            "        except FileNotFoundError:\n",
            "            print(f\"Error: The file '{local_filename}' was not found during transformation.\")\n",
            "            raise ValueError(f\"Transform task failed: Input file not found: {local_filename}\")\n",
            "        except Exception as e:\n",
            "            print(f\"An unexpected error occurred during transformation: {e}\")\n",
            "            raise ValueError(f\"Transform task failed unexpectedly: {e}\")\n",
            "\n",
            "\n",
            "    # Instantiate the PythonOperator for the transformation task\n",
            "    # Executes the 'transform_data' function\n",
            "    # XCom is enabled by default for PythonOperator, allowing the returned DataFrame to be pushed\n",
            "    transform_data_task = PythonOperator(\n",
            "        task_id='transform_data', # Unique identifier\n",
            "        python_callable=transform_data, # The function to execute\n",
            "        op_kwargs={'local_filename': local_filename}, # Pass the filename\n",
            "        # dag=dag, # Associate with the DAG implicitly\n",
            "    )\n",
            "\n",
            "\n",
            "    # Define the Python function for the BigQuery loading task\n",
            "    # This function receives the DataFrame from XCom\n",
            "    def load_data_to_bigquery_with_xcom(ti, project_id: str, dataset_id: str, table_id: str):\n",
            "        \"\"\"\n",
            "        Loads a Pandas DataFrame (pulled from XCom) into a Google BigQuery table.\n",
            "        Assumes GOOGLE_APPLICATION_CREDENTIALS environment variable is set or\n",
            "        Airflow connection handles authentication.\n",
            "        \"\"\"\n",
            "        # Import inside the function for Airflow execution context isolation\n",
            "        # from google.cloud import bigquery # Already imported at top level\n",
            "        # from google.oauth2 import service_account # Already imported at top level\n",
            "        # import os # Already imported at top level\n",
            "        # import pandas as pd # Already imported at top level\n",
            "\n",
            "        print(f\"Attempting to load data to BigQuery table: {project_id}.{dataset_id}.{table_id}\")\n",
            "\n",
            "        try:\n",
            "            # Pull the DataFrame from the previous task's XCom\n",
            "            # 'ti' (TaskInstance) is automatically passed to the python_callable\n",
            "            df = ti.xcom_pull(task_ids='transform_data')\n",
            "\n",
            "            if df is None:\n",
            "                 # If df is None, the previous task might not have returned a DataFrame or failed\n",
            "                 raise ValueError(\"No DataFrame received from the 'transform_data' task via XCom. Check the upstream task.\")\n",
            "\n",
            "            print(f\"Successfully pulled DataFrame from XCom with {len(df)} rows.\")\n",
            "\n",
            "            # Initialize BigQuery client\n",
            "            # The client will automatically use credentials from GOOGLE_APPLICATION_CREDENTIALS\n",
            "            # or from the Airflow connection if configured.\n",
            "            client = bigquery.Client(project=project_id)\n",
            "\n",
            "            # Define the target table reference\n",
            "            table_ref = client.dataset(dataset_id).table(table_id)\n",
            "\n",
            "            # Define the schema for the BigQuery table\n",
            "            # Ensure schema matches the DataFrame structure and data types\n",
            "            schema = [\n",
            "                bigquery.SchemaField(\"Region\", \"STRING\"),\n",
            "                bigquery.SchemaField(\"País\", \"STRING\"),\n",
            "                bigquery.SchemaField(\"Año\", \"INT64\"), # BigQuery type for pandas Int64\n",
            "                bigquery.SchemaField(\"%_Presion\", \"FLOAT64\"), # BigQuery type for pandas float64\n",
            "                bigquery.SchemaField(\"Tipo_Presion\", \"STRING\"),\n",
            "            ]\n",
            "\n",
            "            # Configure the load job\n",
            "            job_config = bigquery.LoadJobConfig(\n",
            "                schema=schema, # Specify the schema\n",
            "                write_disposition=\"WRITE_TRUNCATE\",  # Overwrite the table if it exists\n",
            "                # source_format is not needed when loading from DataFrame\n",
            "            )\n",
            "\n",
            "            # Check if the dataset exists, create if not (optional, can also be a separate task)\n",
            "            try:\n",
            "                client.get_dataset(dataset_id)\n",
            "                print(f\"Dataset '{dataset_id}' already exists.\")\n",
            "            except Exception: # Use a more specific exception if possible, e.g., google.api_core.exceptions.NotFound\n",
            "                print(f\"Dataset '{dataset_id}' not found, creating...\")\n",
            "                dataset = bigquery.Dataset(client.dataset(dataset_id))\n",
            "                dataset.location = \"US\" # Set location - adjust as needed\n",
            "                client.create_dataset(dataset, exists_ok=True)\n",
            "                print(f\"Dataset '{dataset_id}' created.\")\n",
            "\n",
            "\n",
            "            # Check if the table exists, create if not (optional, WRITE_TRUNCATE handles existence)\n",
            "            # Using client.create_table with exists_ok=True is a safe way to ensure schema is defined\n",
            "            table = bigquery.Table(table_ref, schema=schema)\n",
            "            try:\n",
            "                 client.create_table(table, exists_ok=True)\n",
            "                 print(f\"Table '{table_id}' created or already exists with defined schema.\")\n",
            "            except Exception as e:\n",
            "                 print(f\"Error ensuring table '{table_id}' exists or is created: {e}\")\n",
            "                 # Depending on error, might need to raise or retry\n",
            "\n",
            "\n",
            "            # Load data from DataFrame to BigQuery\n",
            "            # This makes the API request to load the data\n",
            "            job = client.load_table_from_dataframe(\n",
            "                df, table_ref, job_config=job_config\n",
            "            )\n",
            "\n",
            "            print(f\"Starting BigQuery load job: {job.job_id}\")\n",
            "            job.result()  # Wait for the job to complete\n",
            "\n",
            "            print(f\"DataFrame successfully loaded to {project_id}.{dataset_id}.{table_id}\")\n",
            "\n",
            "        except Exception as e:\n",
            "            print(f\"Error loading data to BigQuery: {e}\")\n",
            "            # Raise an exception to fail the task in Airflow\n",
            "            raise ValueError(f\"BigQuery load task failed: {e}\")\n",
            "\n",
            "\n",
            "    # Define BigQuery parameters for the load task\n",
            "    # Get Project ID from the environment variable GOOGLE_CLOUD_PROJECT, falling back to credentials\n",
            "    # In a real Airflow environment, prefer using a Variable or Connection\n",
            "    PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT', None)\n",
            "    if PROJECT_ID is None:\n",
            "        try:\n",
            "            # This part might not run reliably in an Airflow worker unless the key is accessible\n",
            "            credentials_check = service_account.Credentials.from_service_account_file(os.environ['GOOGLE_APPLICATION_CREDENTIALS'])\n",
            "            PROJECT_ID = credentials_check.project_id\n",
            "        except Exception as e:\n",
            "            print(f\"Could not automatically determine Google Cloud Project ID. Please set GOOGLE_CLOUD_PROJECT environment variable or ensure GOOGLE_APPLICATION_CREDENTIALS is correctly set: {e}\")\n",
            "            # Fallback or set a default - In a production DAG, this should likely fail or use an Airflow connection\n",
            "            PROJECT_ID = \"your-gcp-project-id\" # !!! REPLACE WITH YOUR ACTUAL GCP PROJECT ID OR USE AIRFLOW CONNECTION/VARIABLE !!!\n",
            "            print(f\"Using fallback Project ID: {PROJECT_ID}\")\n",
            "\n",
            "\n",
            "    DATASET_ID = \"sri_presion_fiscal\" # BigQuery Dataset ID\n",
            "    TABLE_ID = \"sri_presion_fiscal\" # BigQuery Table ID\n",
            "\n",
            "\n",
            "    # Instantiate the PythonOperator for the BigQuery loading task\n",
            "    # Executes the 'load_data_to_bigquery_with_xcom' function\n",
            "    # This task implicitly pulls the DataFrame from the upstream task's XCom\n",
            "    load_to_bigquery_task = PythonOperator(\n",
            "        task_id='load_to_bigquery', # Unique identifier\n",
            "        python_callable=load_data_to_bigquery_with_xcom, # The function to execute\n",
            "        op_kwargs={\n",
            "            'project_id': PROJECT_ID, # Pass project ID\n",
            "            'dataset_id': DATASET_ID, # Pass dataset ID\n",
            "            'table_id': TABLE_ID, # Pass table ID\n",
            "            # The DataFrame is passed via XComs, no need to pass it here\n",
            "        },\n",
            "        # dag=dag, # Associate with the DAG implicitly\n",
            "    )\n",
            "\n",
            "    # Set the order of execution for the tasks using the bitshift operator\n",
            "    # This defines the dependency: download_csv_task runs first, then transform_data_task, then load_to_bigquery_task\n",
            "    download_csv_task >> transform_data_task >> load_to_bigquery_task\n",
            "\n",
            "    # Print a confirmation of the task dependencies (for debugging/verification in a script)\n",
            "    print(\"Task dependencies set:\")\n",
            "    print(f\"'{download_csv_task.task_id}' >> '{transform_data_task.task_id}' >> '{load_to_bigquery_task.task_id}'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25d56503"
      },
      "source": [
        "# Task\n",
        "Configurar y ejecutar el DAG `etl_presion_fiscal_sri` en Google Cloud Composer, incluyendo la finalización del entorno, acceso a la UI de Airflow, carga del archivo DAG \"etl presion fiscal dag.py\", verificación en la UI, configuración de conexiones/variables si es necesario, activación y ejecución del DAG, monitoreo de la ejecución y verificación de los resultados en BigQuery."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "357d4344"
      },
      "source": [
        "## Finalizar la creación del entorno de composer\n",
        "\n",
        "### Subtask:\n",
        "Asegúrate de que tu entorno `etl-composer` se haya creado correctamente en Google Cloud Composer. Verifica su estado en la consola de Google Cloud.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "692e0e0d"
      },
      "source": [
        "## Acceder a la interfaz de usuario de airflow\n",
        "\n",
        "### Subtask:\n",
        "Una vez que el entorno de Composer esté listo, accede a la interfaz de usuario de Airflow a través del enlace proporcionado en la página de detalles del entorno en la consola de Google Cloud.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23261913"
      },
      "source": [
        "## Cargar el archivo del dag (`etl presion fiscal dag.py`)\n",
        "\n",
        "### Subtask:\n",
        "Upload the `etl_presion_fiscal_dag.py` file to the `dags` folder within the Cloud Storage bucket associated with your Composer environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4825bddc"
      },
      "source": [
        "## Verificar que el dag aparece en la ui de airflow\n",
        "\n",
        "### Subtask:\n",
        "Refresca la interfaz de usuario de Airflow. Deberías ver el DAG `etl_presion_fiscal_sri` listado. Si no aparece, verifica los logs de Airflow para posibles errores de sintaxis en el archivo DAG.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bae63cc"
      },
      "source": [
        "## Configurar las conexiones de airflow (si es necesario)\n",
        "\n",
        "### Subtask:\n",
        "If your DAG uses Airflow connections (e.g., a Google Cloud connection for BigQuery instead of direct service credentials), configure them in the Admin -> Connections section of the Airflow UI. Although your current DAG uses the `GOOGLE_APPLICATION_CREDENTIALS` environment variable, configuring a GCP connection is a recommended practice for production environments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0807fffd"
      },
      "source": [
        "## Configurar variables de airflow (si es necesario)\n",
        "\n",
        "### Subtask:\n",
        "If your DAG needs variables (such as project IDs or table names that might change), configure them in the Admin -> Variables section of the Airflow UI.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "391652f8"
      },
      "source": [
        "## Ejecutar el dag\n",
        "\n",
        "### Subtask:\n",
        "Trigger a manual run of the `etl_presion_fiscal_sri` DAG from the Airflow UI.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1e0987e"
      },
      "source": [
        "## Monitorear la ejecución del dag\n",
        "\n",
        "### Subtask:\n",
        "Observa el estado de la ejecución del DAG en la vista \"Graph View\" o \"Tree View\" de la UI de Airflow. Verifica los logs de cada tarea para depurar cualquier problema.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "657d0068"
      },
      "source": [
        "## Verificar los resultados en bigquery\n",
        "\n",
        "### Subtask:\n",
        "Verify that the data has been correctly loaded into the dimension and fact tables in your `sri_presion_fiscal` dataset in BigQuery.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3981cee"
      },
      "source": [
        "## Finalizar tarea\n",
        "\n",
        "### Subtask:\n",
        "Finish task\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07bb5247"
      },
      "source": [
        "## Definición del DAG y Tarea de Descarga\n",
        "\n",
        "Basándonos en la Fase 1 y la necesidad de leer desde GCS (que implica tener el archivo allí), el primer paso en el DAG será descargar el archivo CSV desde la URL proporcionada en los pasos anteriores. Luego definiremos la estructura básica del DAG."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "id": "1ad07a86",
        "outputId": "de75e769-0953-402c-da87-0ec0d1eb7eab"
      },
      "source": [
        "# Import necessary modules from Apache Airflow and other libraries\n",
        "from airflow import DAG # DAG object to define the workflow\n",
        "from datetime import datetime, timedelta # For specifying start_date and retry_delay\n",
        "import pandas as pd # For data transformation using DataFrames\n",
        "from airflow.operators.python import PythonOperator # Operator to run Python functions\n",
        "# Import EmptyOperator from airflow.operators.empty (replaces DummyOperator)\n",
        "from airflow.operators.empty import EmptyOperator\n",
        "from google.cloud import bigquery # For interacting with Google BigQuery API\n",
        "import os # For accessing environment variables\n",
        "from google.oauth2 import service_account # For loading Google Cloud service account credentials\n",
        "import requests # For downloading the CSV file\n",
        "import logging # For logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "# Define default arguments for the DAG (Fase 1)\n",
        "# These arguments are passed to all tasks unless overridden\n",
        "default_args = {\n",
        "    'owner': 'airflow', # The owner of the DAG\n",
        "    'depends_on_past': False, # Set to true if you want tasks to wait for the previous run to succeed\n",
        "    'start_date': datetime(2024, 6, 1), # The date when the DAG starts being scheduled (Fase 1)\n",
        "    'email_on_failure': False, # Set to true to receive emails on task failure (Fase 1)\n",
        "    'email_on_retry': False, # Set to true to receive emails on task retry (Fase 1)\n",
        "    'retries': 1, # Number of times to retry a task on failure (Fase 1)\n",
        "    'retry_delay': timedelta(minutes=5), # Time to wait between retries (Fase 1)\n",
        "}\n",
        "\n",
        "# Define the DAG object (Fase 1)\n",
        "# 'presionfiscal_etl' is the unique DAG ID as specified\n",
        "with DAG(\n",
        "    'presionfiscal_etl', # dag_id as specified\n",
        "    default_args=default_args, # Apply the default arguments\n",
        "    description='ETL pipeline to download, transform, and load SRI tax pressure data to BigQuery', # A brief description of the DAG\n",
        "    schedule=None, # Set the schedule interval to None for manual runs (Fase 1)\n",
        "    catchup=False, # Set to False as specified (Fase 1)\n",
        "    max_active_runs=1, # Set max_active_runs as specified (Fase 1)\n",
        "    tags=['etl', 'bigquery', 'sri', 'presion_fiscal'], # Tags to help categorize and filter DAGs\n",
        ") as dag:\n",
        "\n",
        "    # Define the local filename for the downloaded CSV\n",
        "    # This file will be used as the source for both dimension and fact ETL\n",
        "    local_filename = \"SRI_Presion_Fiscal.csv\"\n",
        "\n",
        "    # Define the URL for the source CSV file\n",
        "    csv_url = \"https://www.sri.gob.ec/o/sri-portlet-biblioteca-alfresco-internet/descargar/7e45627e-1f7e-4e21-ae59-d520634fc63f/SRI_Presion_Fiscal.csv\"\n",
        "\n",
        "    # Define the filename for the Google Cloud service account credentials\n",
        "    # Using a variable for the filename allows for flexibility\n",
        "    # In a real Airflow environment, prefer using Airflow Connections or Variables for managing credentials securely\n",
        "    # For this exercise, we'll assume the JSON key file is accessible to the Airflow worker\n",
        "    # and its path is set in the GOOGLE_APPLICATION_CREDENTIALS environment variable or passed securely.\n",
        "    # We will use a placeholder filename here. Replace with your actual filename.\n",
        "    credential_filename = \"your-gcp-credential-file.json\" # !!! REPLACE WITH YOUR ACTUAL FILENAME !!!\n",
        "    # Note: Setting the environment variable directly in the DAG file is generally NOT recommended\n",
        "    # for production Airflow environments for security reasons. Use Airflow Connections instead.\n",
        "    # However, for demonstrating the code structure, we include it here.\n",
        "    # A safer approach is to ensure the Airflow worker environment has this variable set.\n",
        "\n",
        "\n",
        "    # Define the Python function for the download task\n",
        "    def download_csv_from_url(csv_url, local_filename):\n",
        "        \"\"\"Downloads a CSV file from a given URL and saves it locally.\"\"\"\n",
        "        logger.info(f\"Attempting to download file from: {csv_url}\")\n",
        "        try:\n",
        "            response = requests.get(csv_url)\n",
        "            response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "            with open(local_filename, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "\n",
        "            logger.info(f\"Archivo '{local_filename}' descargado exitosamente.\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Error al descargar el archivo: {e}\")\n",
        "            # Raise an exception to fail the task in Airflow\n",
        "            raise ValueError(f\"Download task failed: {e}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"An unexpected error occurred during download: {e}\")\n",
        "            raise ValueError(f\"Download task failed unexpectedly: {e}\")\n",
        "\n",
        "\n",
        "    # Define the start task using EmptyOperator (Fase 4)\n",
        "    start = EmptyOperator(\n",
        "        task_id='start',\n",
        "    )\n",
        "\n",
        "    # Instantiate the PythonOperator for the download task\n",
        "    # Executes the 'download_csv_from_url' function\n",
        "    download_csv_task = PythonOperator(\n",
        "        task_id='download_source_csv', # Unique identifier for the task\n",
        "        python_callable=download_csv_from_url, # The Python function to execute\n",
        "        op_kwargs={'csv_url': csv_url, 'local_filename': local_filename}, # Keyword arguments to pass to the function\n",
        "    )\n",
        "\n",
        "    # Define the end task using EmptyOperator (Fase 4) - will be connected later\n",
        "    end = EmptyOperator(\n",
        "        task_id='end',\n",
        "    )\n",
        "\n",
        "    # Initial dependency: start task runs before downloading the CSV\n",
        "    start >> download_csv_task"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m/tmp/\u001b[0m\u001b[1;33mipython-input-2-2444683033.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m5\u001b[0m\u001b[1;33m DeprecationWarning\u001b[0m\u001b[33m: The `airflow.operators.python.PythonOperator` class is deprecated. Please use `\u001b[0m\u001b[33m'airflow.providers.standard.operators.python.PythonOperator'\u001b[0m\u001b[33m`.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/tmp/ipython-input-2-2444683033.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">5</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> DeprecationWarning</span><span style=\"color: #808000; text-decoration-color: #808000\">: The `airflow.operators.python.PythonOperator` class is deprecated. Please use `</span><span style=\"color: #808000; text-decoration-color: #808000\">'airflow.providers.standard.operators.python.PythonOperator'</span><span style=\"color: #808000; text-decoration-color: #808000\">`.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m/tmp/\u001b[0m\u001b[1;33mipython-input-2-2444683033.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m7\u001b[0m\u001b[1;33m DeprecationWarning\u001b[0m\u001b[33m: The `airflow.operators.empty.EmptyOperator` class is deprecated. Please use `\u001b[0m\u001b[33m'airflow.providers.standard.operators.empty.EmptyOperator'\u001b[0m\u001b[33m`.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/tmp/ipython-input-2-2444683033.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">7</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> DeprecationWarning</span><span style=\"color: #808000; text-decoration-color: #808000\">: The `airflow.operators.empty.EmptyOperator` class is deprecated. Please use `</span><span style=\"color: #808000; text-decoration-color: #808000\">'airflow.providers.standard.operators.empty.EmptyOperator'</span><span style=\"color: #808000; text-decoration-color: #808000\">`.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bc2e0398",
        "outputId": "cbe8c48f-9122-4a8c-cda4-44be64aeac9d"
      },
      "source": [
        "!pip install apache-airflow"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting apache-airflow\n",
            "  Downloading apache_airflow-3.0.2-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting apache-airflow-core==3.0.2 (from apache-airflow)\n",
            "  Downloading apache_airflow_core-3.0.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting apache-airflow-task-sdk<1.1.0,>=1.0.0 (from apache-airflow)\n",
            "  Downloading apache_airflow_task_sdk-1.0.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting a2wsgi>=1.10.8 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading a2wsgi-1.10.10-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting aiosqlite>=0.20.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting alembic<2.0,>=1.13.1 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting apache-airflow-providers-common-compat>=1.6.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading apache_airflow_providers_common_compat-1.7.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting apache-airflow-providers-common-io>=1.5.3 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading apache_airflow_providers_common_io-1.6.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting apache-airflow-providers-common-sql>=1.26.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading apache_airflow_providers_common_sql-1.27.2-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting apache-airflow-providers-smtp>=2.0.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading apache_airflow_providers_smtp-2.1.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting apache-airflow-providers-standard>=0.4.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading apache_airflow_providers_standard-1.3.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting argcomplete>=1.10 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading argcomplete-3.6.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting asgiref>=2.3.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: attrs!=25.2.0,>=22.1.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (25.3.0)\n",
            "Collecting cadwyn>=5.2.1 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading cadwyn-5.4.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting colorlog>=6.8.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting cron-descriptor>=1.2.24 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading cron_descriptor-1.4.5-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting croniter>=2.0.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading croniter-6.0.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: cryptography>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (43.0.3)\n",
            "Collecting deprecated>=1.2.13 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: dill>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (0.3.7)\n",
            "Requirement already satisfied: fastapi!=0.115.10,>=0.115.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow) (0.115.13)\n",
            "Requirement already satisfied: flask>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (3.1.1)\n",
            "Collecting gunicorn>=20.1.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.5 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (8.7.0)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.5 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=4.19.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (4.24.0)\n",
            "Collecting lazy-object-proxy>=1.2.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading lazy_object_proxy-1.11.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting libcst>=1.1.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading libcst-1.8.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: linkify-it-py>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.0.3)\n",
            "Collecting lockfile>=0.12.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading lockfile-0.12.2-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting methodtools>=0.4.7 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading methodtools-0.4.7-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting opentelemetry-api>=1.26.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp>=1.26.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_exporter_otlp-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (24.2)\n",
            "Collecting pathspec>=0.9.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pendulum<4.0,>=2.1.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading pendulum-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: pluggy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (1.6.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (5.9.5)\n",
            "Requirement already satisfied: pydantic>=2.11.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.11.7)\n",
            "Requirement already satisfied: pygments!=2.19.0,>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.19.2)\n",
            "Requirement already satisfied: pyjwt>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.10.1)\n",
            "Collecting python-daemon>=3.0.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading python_daemon-3.1.2-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify>=5.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (8.0.4)\n",
            "Requirement already satisfied: requests<3,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.32.3)\n",
            "Collecting rich-argparse>=1.0.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading rich_argparse-1.7.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: rich>=13.6.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (13.9.4)\n",
            "Requirement already satisfied: setproctitle>=1.3.3 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (1.3.6)\n",
            "Collecting sqlalchemy-jsonfield>=1.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading SQLAlchemy_JSONField-1.0.2-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting sqlalchemy-utils>=0.41.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading SQLAlchemy_Utils-0.41.2-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting sqlalchemy<2.0,>=1.4.49 (from sqlalchemy[asyncio]<2.0,>=1.4.49->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading SQLAlchemy-1.4.54-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting svcs>=25.1.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading svcs-25.1.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (0.9.0)\n",
            "Requirement already satisfied: tenacity!=8.2.0,>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (8.5.0)\n",
            "Requirement already satisfied: termcolor>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (3.1.0)\n",
            "Collecting typing-extensions!=4.14.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting universal-pathlib!=0.2.4,>=0.2.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading universal_pathlib-0.2.6-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting uuid6>=2024.7.10 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading uuid6-2025.0.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting aiologic>=0.14.0 (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow)\n",
            "  Downloading aiologic-0.14.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow) (2025.3.2)\n",
            "Collecting msgspec>=0.19.0 (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow)\n",
            "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting psutil>=5.8.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting retryhttp!=1.3.0,>=1.2.0 (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow)\n",
            "  Downloading retryhttp-1.3.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting structlog>=25.4.0 (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow)\n",
            "  Downloading structlog-25.4.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: wrapt>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from aiologic>=0.14.0->apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow) (1.17.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic<2.0,>=1.13.1->apache-airflow-core==3.0.2->apache-airflow) (1.1.3)\n",
            "Requirement already satisfied: sqlparse>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-common-sql>=1.26.0->apache-airflow-core==3.0.2->apache-airflow) (0.5.3)\n",
            "Requirement already satisfied: more-itertools>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-common-sql>=1.26.0->apache-airflow-core==3.0.2->apache-airflow) (10.7.0)\n",
            "Requirement already satisfied: starlette>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from cadwyn>=5.2.1->apache-airflow-core==3.0.2->apache-airflow) (0.46.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from cadwyn>=5.2.1->apache-airflow-core==3.0.2->apache-airflow) (0.4.1)\n",
            "Requirement already satisfied: pytz>2021.1 in /usr/local/lib/python3.11/dist-packages (from croniter>=2.0.2->apache-airflow-core==3.0.2->apache-airflow) (2025.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=41.0.0->apache-airflow-core==3.0.2->apache-airflow) (1.17.1)\n",
            "Collecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow) (0.0.20)\n",
            "Collecting email-validator>=2.0.0 (from fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow) (0.34.3)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask>=2.1.1->apache-airflow-core==3.0.2->apache-airflow) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=2.1.1->apache-airflow-core==3.0.2->apache-airflow) (8.2.1)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask>=2.1.1->apache-airflow-core==3.0.2->apache-airflow) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from flask>=2.1.1->apache-airflow-core==3.0.2->apache-airflow) (3.1.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->apache-airflow-core==3.0.2->apache-airflow) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->apache-airflow-core==3.0.2->apache-airflow) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->apache-airflow-core==3.0.2->apache-airflow) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->apache-airflow-core==3.0.2->apache-airflow) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->apache-airflow-core==3.0.2->apache-airflow) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.5->apache-airflow-core==3.0.2->apache-airflow) (3.23.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.1->apache-airflow-core==3.0.2->apache-airflow) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.1->apache-airflow-core==3.0.2->apache-airflow) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.1->apache-airflow-core==3.0.2->apache-airflow) (0.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.2 in /usr/local/lib/python3.11/dist-packages (from libcst>=1.1.0->apache-airflow-core==3.0.2->apache-airflow) (6.0.2)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.11/dist-packages (from linkify-it-py>=2.0.0->apache-airflow-core==3.0.2->apache-airflow) (1.0.3)\n",
            "Collecting wirerope>=0.4.7 (from methodtools>=0.4.7->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading wirerope-1.0.0-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc==1.34.1 (from opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http==1.34.1 (from opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.34.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.63.2 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow) (1.73.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk~=1.34.1 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: protobuf<6.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-proto==1.34.1->opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow) (5.29.5)\n",
            "Requirement already satisfied: tzdata>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pendulum<4.0,>=2.1.2->apache-airflow-core==3.0.2->apache-airflow) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.0->apache-airflow-core==3.0.2->apache-airflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.0->apache-airflow-core==3.0.2->apache-airflow) (2.33.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7.0->apache-airflow-core==3.0.2->apache-airflow) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify>=5.0->apache-airflow-core==3.0.2->apache-airflow) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.31.0->apache-airflow-core==3.0.2->apache-airflow) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.31.0->apache-airflow-core==3.0.2->apache-airflow) (2.4.0)\n",
            "Collecting types-requests (from retryhttp!=1.3.0,>=1.2.0->apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow)\n",
            "  Downloading types_requests-2.32.4.20250611-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.6.0->apache-airflow-core==3.0.2->apache-airflow) (3.0.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<2.0,>=1.4.49->sqlalchemy[asyncio]<2.0,>=1.4.49->apache-airflow-core==3.0.2->apache-airflow) (3.2.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=41.0.0->apache-airflow-core==3.0.2->apache-airflow) (2.22)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow) (0.16.0)\n",
            "Collecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading rich_toolkit-0.14.8-py3-none-any.whl.metadata (999 bytes)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.6.0->apache-airflow-core==3.0.2->apache-airflow) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->apache-airflow-core==3.0.2->apache-airflow) (1.3.1)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow) (15.0.1)\n",
            "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk~=1.34.1->opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow) (1.5.4)\n",
            "Downloading apache_airflow-3.0.2-py3-none-any.whl (12 kB)\n",
            "Downloading apache_airflow_core-3.0.2-py3-none-any.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading apache_airflow_task_sdk-1.0.2-py3-none-any.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.5/241.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading a2wsgi-1.10.10-py3-none-any.whl (17 kB)\n",
            "Downloading aiologic-0.14.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
            "Downloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading apache_airflow_providers_common_compat-1.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading apache_airflow_providers_common_io-1.6.0-py3-none-any.whl (19 kB)\n",
            "Downloading apache_airflow_providers_common_sql-1.27.2-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading apache_airflow_providers_smtp-2.1.0-py3-none-any.whl (21 kB)\n",
            "Downloading apache_airflow_providers_standard-1.3.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading argcomplete-3.6.2-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading cadwyn-5.4.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading cron_descriptor-1.4.5-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading croniter-6.0.0-py2.py3-none-any.whl (25 kB)\n",
            "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lazy_object_proxy-1.11.0-py3-none-any.whl (16 kB)\n",
            "Downloading libcst-1.8.2-cp311-cp311-manylinux_2_28_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lockfile-0.12.2-py2.py3-none-any.whl (13 kB)\n",
            "Downloading methodtools-0.4.7-py2.py3-none-any.whl (4.0 kB)\n",
            "Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp-1.34.1-py3-none-any.whl (7.0 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_http-1.34.1-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.34.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading pendulum-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (353 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m353.7/353.7 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_daemon-3.1.2-py3-none-any.whl (30 kB)\n",
            "Downloading retryhttp-1.3.3-py3-none-any.whl (17 kB)\n",
            "Downloading rich_argparse-1.7.1-py3-none-any.whl (25 kB)\n",
            "Downloading SQLAlchemy-1.4.54-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy_JSONField-1.0.2-py3-none-any.whl (10 kB)\n",
            "Downloading SQLAlchemy_Utils-0.41.2-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading structlog-25.4.0-py3-none-any.whl (68 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.7/68.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading svcs-25.1.0-py3-none-any.whl (19 kB)\n",
            "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading universal_pathlib-0.2.6-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uuid6-2025.0.0-py3-none-any.whl (7.0 kB)\n",
            "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\n",
            "Downloading wirerope-1.0.0-py2.py3-none-any.whl (9.2 kB)\n",
            "Downloading types_requests-2.32.4.20250611-py3-none-any.whl (20 kB)\n",
            "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading rich_toolkit-0.14.8-py3-none-any.whl (24 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lockfile, cron-descriptor, wirerope, uvloop, uuid6, universal-pathlib, typing-extensions, types-requests, svcs, structlog, sqlalchemy, python-dotenv, python-daemon, psutil, pathspec, opentelemetry-proto, msgspec, libcst, lazy-object-proxy, httptools, gunicorn, dnspython, deprecated, colorlog, asgiref, argcomplete, aiologic, a2wsgi, sqlalchemy-utils, sqlalchemy-jsonfield, pendulum, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, methodtools, email-validator, croniter, alembic, aiosqlite, watchfiles, rich-toolkit, rich-argparse, opentelemetry-semantic-conventions, retryhttp, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, fastapi-cli, cadwyn, opentelemetry-exporter-otlp, apache-airflow-providers-common-compat, apache-airflow-providers-standard, apache-airflow-providers-smtp, apache-airflow-providers-common-sql, apache-airflow-providers-common-io, apache-airflow-task-sdk, apache-airflow-core, apache-airflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.14.0\n",
            "    Uninstalling typing_extensions-4.14.0:\n",
            "      Successfully uninstalled typing_extensions-4.14.0\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.41\n",
            "    Uninstalling SQLAlchemy-2.0.41:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.41\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\n",
            "typeguard 4.4.4 requires typing_extensions>=4.14.0, but you have typing-extensions 4.13.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed a2wsgi-1.10.10 aiologic-0.14.0 aiosqlite-0.21.0 alembic-1.16.2 apache-airflow-3.0.2 apache-airflow-core-3.0.2 apache-airflow-providers-common-compat-1.7.1 apache-airflow-providers-common-io-1.6.0 apache-airflow-providers-common-sql-1.27.2 apache-airflow-providers-smtp-2.1.0 apache-airflow-providers-standard-1.3.0 apache-airflow-task-sdk-1.0.2 argcomplete-3.6.2 asgiref-3.8.1 cadwyn-5.4.2 colorlog-6.9.0 cron-descriptor-1.4.5 croniter-6.0.0 deprecated-1.2.18 dnspython-2.7.0 email-validator-2.2.0 fastapi-cli-0.0.7 gunicorn-23.0.0 httptools-0.6.4 lazy-object-proxy-1.11.0 libcst-1.8.2 lockfile-0.12.2 methodtools-0.4.7 msgspec-0.19.0 opentelemetry-api-1.34.1 opentelemetry-exporter-otlp-1.34.1 opentelemetry-exporter-otlp-proto-common-1.34.1 opentelemetry-exporter-otlp-proto-grpc-1.34.1 opentelemetry-exporter-otlp-proto-http-1.34.1 opentelemetry-proto-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 pathspec-0.12.1 pendulum-3.1.0 psutil-7.0.0 python-daemon-3.1.2 python-dotenv-1.1.1 retryhttp-1.3.3 rich-argparse-1.7.1 rich-toolkit-0.14.8 sqlalchemy-1.4.54 sqlalchemy-jsonfield-1.0.2 sqlalchemy-utils-0.41.2 structlog-25.4.0 svcs-25.1.0 types-requests-2.32.4.20250611 typing-extensions-4.13.2 universal-pathlib-0.2.6 uuid6-2025.0.0 uvloop-0.21.0 watchfiles-1.1.0 wirerope-1.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              },
              "id": "1980b6487f184085b8a2720064f4c800"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bc0f29c"
      },
      "source": [
        "## ETL de Dimensiones (Fase 2)\n",
        "\n",
        "### Subtask:\n",
        "Definir esquemas para las tablas de dimensiones (Dim_Geografia, Dim_Tiempo, Dim_TipoPresion), e inicializar el cliente de BigQuery para interactuar con GCP y crear las tablas si no existen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5036071a",
        "outputId": "b7abfe7d-d8ed-49bf-b1a2-92e14e525f98"
      },
      "source": [
        "# Assuming BigQuery client and PROJECT_ID, DATASET_ID are defined or will be defined in this block\n",
        "\n",
        "# Define BigQuery parameters\n",
        "# Use the correct filename for the Google Cloud service account credentials provided by the user.\n",
        "# This filename was specified in the previous successful step or should be provided by the user.\n",
        "# IMPORTANT: In a real Airflow environment, managing credentials via Airflow Connections is strongly recommended for security.\n",
        "# Setting the environment variable directly in the DAG file is generally NOT secure for production.\n",
        "# We will use a placeholder filename here and assume the environment variable GOOGLE_APPLICATION_CREDENTIALS\n",
        "# is set correctly in the Airflow worker environment pointing to the JSON key file.\n",
        "# If you are testing this locally in Colab, ensure the file is uploaded and the environment variable is set.\n",
        "# Use the absolute path provided by the user in a previous successful step\n",
        "credential_filepath = \"/etl-sistemas-sri-cc64af72be76.json\" # Use the full path provided by the user\n",
        "\n",
        "# Get Project ID. Prefer GOOGLE_CLOUD_PROJECT env var, otherwise try to get from credentials.\n",
        "# In a real Airflow DAG, using Airflow Variables for PROJECT_ID is a good practice.\n",
        "# Use the PROJECT_ID that was successful in previous steps for loading data\n",
        "PROJECT_ID = \"etl-sistemas-sri\" # Using the project ID from previous successful loads\n",
        "\n",
        "\n",
        "DATASET_ID = \"sri_presion_fiscal\" # BigQuery Dataset ID as specified\n",
        "\n",
        "# --- Explicitly load credentials and initialize BigQuery client ---\n",
        "try:\n",
        "    # Load credentials explicitly from the service account file\n",
        "    credentials = service_account.Credentials.from_service_account_file(credential_filepath)\n",
        "    logger.info(\"Google Cloud credentials loaded successfully.\")\n",
        "\n",
        "    # Initialize BigQuery client with the loaded credentials and project ID\n",
        "    client = bigquery.Client(project=PROJECT_ID, credentials=credentials)\n",
        "    logger.info(f\"BigQuery client initialized for project: {PROJECT_ID} using explicit credentials.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    logger.error(f\"Error: Credential file '{credential_filepath}' not found. Please upload it again and ensure the path is correct.\")\n",
        "    raise # Re-raise the exception if the file is not found\n",
        "except Exception as e:\n",
        "    logger.error(f\"An unexpected error occurred loading credentials or initializing BigQuery client: {e}\")\n",
        "    raise # Re-raise any other exception\n",
        "# --- End of credential loading and client initialization ---\n",
        "\n",
        "\n",
        "# Create or get the BigQuery dataset\n",
        "dataset_ref = client.dataset(DATASET_ID)\n",
        "dataset = bigquery.Dataset(dataset_ref)\n",
        "dataset.location = \"US\" # Set location - adjust as needed for your GCP region\n",
        "\n",
        "try:\n",
        "    dataset = client.create_dataset(dataset, exists_ok=True)\n",
        "    logger.info(f\"Dataset '{DATASET_ID}' created or already exists in project '{PROJECT_ID}'.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error creating dataset '{DATASET_ID}' in project '{PROJECT_ID}': {e}\")\n",
        "    raise # Re-raise the exception\n",
        "\n",
        "\n",
        "# Define schemas for dimension tables based on the user's plan\n",
        "# Dim_Geografia (ID_Geografia (PK), Pais (varchar), Region (varchar))\n",
        "schema_dim_geografia = [\n",
        "    bigquery.SchemaField(\"id_geografia\", \"INT64\", mode=\"REQUIRED\"), # Primary Key\n",
        "    bigquery.SchemaField(\"pais\", \"STRING\", mode=\"REQUIRED\"),      # Country\n",
        "    bigquery.SchemaField(\"region\", \"STRING\", mode=\"REQUIRED\"),    # Region\n",
        "]\n",
        "\n",
        "# Dim_Tiempo (tiempo_id (PK), año (int))\n",
        "schema_dim_tiempo = [\n",
        "    bigquery.SchemaField(\"tiempo_id\", \"INT64\", mode=\"REQUIRED\"), # Primary Key\n",
        "    bigquery.SchemaField(\"año\", \"INT64\", mode=\"REQUIRED\"),      # Year\n",
        "]\n",
        "\n",
        "# Dim_TipoPresion (tipo_presion_id (PK), tipo_presion_nombre (varchar))\n",
        "schema_dim_tipopresion = [\n",
        "    bigquery.SchemaField(\"tipo_presion_id\", \"INT64\", mode=\"REQUIRED\"), # Primary Key\n",
        "    bigquery.SchemaField(\"tipo_presion_nombre\", \"STRING\", mode=\"REQUIRED\"), # Tax Pressure Type Name\n",
        "]\n",
        "\n",
        "\n",
        "# Define table IDs for dimension tables\n",
        "table_id_dim_geografia = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Geografia\"\n",
        "table_id_dim_tiempo = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Tiempo\"\n",
        "table_id_dim_tipopresion = f\"{PROJECT_ID}.{DATASET_ID}.Dim_TipoPresion\"\n",
        "\n",
        "\n",
        "# Helper function to create or get a BigQuery table\n",
        "def create_or_get_table(client, table_id, schema):\n",
        "    \"\"\"Creates a BigQuery table if it doesn't exist, or gets it if it does.\"\"\"\n",
        "    table_ref = bigquery.Table(table_id, schema=schema)\n",
        "    try:\n",
        "        table = client.create_table(table_ref, exists_ok=True)\n",
        "        logger.info(f\"Table '{table_id}' created or already exists.\")\n",
        "        return table\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error creating table '{table_id}': {e}\")\n",
        "        raise # Re-raise the exception\n",
        "\n",
        "\n",
        "# Create or get each dimension table in BigQuery\n",
        "table_dim_geografia = create_or_get_table(client, table_id_dim_geografia, schema_dim_geografia)\n",
        "table_dim_tiempo = create_or_get_table(client, table_id_dim_tiempo, schema_dim_tiempo)\n",
        "table_dim_tipopresion = create_or_get_table(client, table_id_dim_tipopresion, schema_dim_tipopresion)\n",
        "\n",
        "logger.info(\"BigQuery dataset and dimension table schemas defined and tables created/verified.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-30T23:30:01.687+0000] {ipython-input-10-546107122.py:26} INFO - Google Cloud credentials loaded successfully.\n",
            "[2025-06-30T23:30:01.692+0000] {ipython-input-10-546107122.py:30} INFO - BigQuery client initialized for project: etl-sistemas-sri using explicit credentials.\n",
            "[2025-06-30T23:30:02.048+0000] {ipython-input-10-546107122.py:48} INFO - Dataset 'sri_presion_fiscal' created or already exists in project 'etl-sistemas-sri'.\n",
            "[2025-06-30T23:30:02.462+0000] {ipython-input-10-546107122.py:87} INFO - Table 'etl-sistemas-sri.sri_presion_fiscal.Dim_Geografia' created or already exists.\n",
            "[2025-06-30T23:30:02.934+0000] {ipython-input-10-546107122.py:87} INFO - Table 'etl-sistemas-sri.sri_presion_fiscal.Dim_Tiempo' created or already exists.\n",
            "[2025-06-30T23:30:03.249+0000] {ipython-input-10-546107122.py:87} INFO - Table 'etl-sistemas-sri.sri_presion_fiscal.Dim_TipoPresion' created or already exists.\n",
            "[2025-06-30T23:30:03.251+0000] {ipython-input-10-546107122.py:99} INFO - BigQuery dataset and dimension table schemas defined and tables created/verified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32690f27"
      },
      "source": [
        "### Subtask:\n",
        "Extraer años únicos de la tabla de origen, generar una clave subrogada para `Dim_Tiempo` y cargarla en la tabla de BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "d45be069",
        "outputId": "2c0e6241-4e00-4920-c466-aae27a5ea865"
      },
      "source": [
        "# Assuming BigQuery client and PROJECT_ID, DATASET_ID are defined and credentials are set up from previous steps.\n",
        "# If running this cell independently, you would need to re-initialize the client and credentials here.\n",
        "# Ensure BigQuery client is initialized and authenticated before running this.\n",
        "\n",
        "# Define the source table ID\n",
        "# Use the PROJECT_ID that was successful in previous loading attempts\n",
        "SOURCE_PROJECT_ID = 'etl-sistemas-sri' # Use the project ID where the source data was loaded\n",
        "DATASET_ID = \"sri_presion_fiscal\" # BigQuery Dataset ID\n",
        "SOURCE_TABLE_ID = f\"{SOURCE_PROJECT_ID}.{DATASET_ID}.sri_presion_fiscal\" # Source table for dimension data\n",
        "\n",
        "# Define destination table ID for Dim_Tiempo\n",
        "# Assuming dimension tables are in the same project/dataset for simplicity here\n",
        "# In a real scenario, destination PROJECT_ID might be different\n",
        "PROJECT_ID = SOURCE_PROJECT_ID # Use source project as destination for dimension tables for now\n",
        "table_id_dim_tiempo = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Tiempo\"\n",
        "\n",
        "# Define schema for Dim_Tiempo (assuming it was defined in a previous cell or is defined here)\n",
        "# If not defined, ensure the cell defining schema_dim_tiempo is run first.\n",
        "schema_dim_tiempo = [\n",
        "    bigquery.SchemaField(\"tiempo_id\", \"INT64\", mode=\"REQUIRED\"), # Primary Key\n",
        "    bigquery.SchemaField(\"año\", \"INT64\", mode=\"REQUIRED\"),      # Year\n",
        "]\n",
        "\n",
        "\n",
        "# Write a SQL query to select distinct years\n",
        "query_tiempo = f\"\"\"\n",
        "SELECT DISTINCT `Año`\n",
        "FROM `{SOURCE_TABLE_ID}`\n",
        "WHERE `Año` IS NOT NULL # Keep the check for non-null years\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query and load results into a DataFrame for Dim_Tiempo\n",
        "try:\n",
        "    # --- Ensure BigQuery client is initialized and authenticated ---\n",
        "    # This is a placeholder for demonstration. In a real Airflow task,\n",
        "    # the client initialization would likely happen within the task's callable\n",
        "    # or rely on Airflow's configuration.\n",
        "    # Assuming 'client' object is available from a previous successful cell execution.\n",
        "    if 'client' not in locals() or client is None:\n",
        "         # Attempt to re-initialize client if not found (for interactive testing)\n",
        "         try:\n",
        "             # Assuming the credential file path is known and accessible\n",
        "             credential_filepath = \"/etl-sistemas-sri-cc64af72be76.json\" # Use the full path provided by the user or your actual path\n",
        "             credentials = service_account.Credentials.from_service_account_file(credential_filepath)\n",
        "             client = bigquery.Client(project=SOURCE_PROJECT_ID, credentials=credentials)\n",
        "             logger.info(\"BigQuery client re-initialized for Dim_Tiempo ETL.\")\n",
        "         except Exception as e:\n",
        "             logger.error(f\"Could not re-initialize BigQuery client for Dim_Tiempo ETL: {e}\")\n",
        "             raise # Cannot proceed without a client\n",
        "\n",
        "\n",
        "    df_tiempo = client.query(query_tiempo).to_dataframe()\n",
        "    logger.info(f\"Extracted {len(df_tiempo)} unique years.\")\n",
        "    display(df_tiempo.head())\n",
        "\n",
        "    # Add a tiempo_id column (surrogate key)\n",
        "    # Simple 1-based integer ID based on the sorted years\n",
        "    df_tiempo['tiempo_id'] = df_tiempo['Año'].rank(method='dense').astype('Int64') # Using rank for dense ID\n",
        "\n",
        "    # Rename the year column to match the schema (already 'Año', but good practice)\n",
        "    df_tiempo.rename(columns={'Año': 'año'}, inplace=True)\n",
        "\n",
        "    # Select only the required columns and ensure order\n",
        "    df_tiempo = df_tiempo[['tiempo_id', 'año']]\n",
        "\n",
        "    logger.info(\"Transformed Dim_Tiempo DataFrame head:\")\n",
        "    display(df_tiempo.head())\n",
        "    logger.info(\"Transformed Dim_Tiempo DataFrame info:\")\n",
        "    display(df_tiempo.info())\n",
        "\n",
        "    # Load the Dim_Tiempo DataFrame into the BigQuery table\n",
        "    job_config_tiempo = bigquery.LoadJobConfig(\n",
        "        schema=schema_dim_tiempo, # Use the defined schema\n",
        "        write_disposition=\"WRITE_TRUNCATE\", # Overwrite the table\n",
        "    )\n",
        "\n",
        "    job_tiempo = client.load_table_from_dataframe(\n",
        "        df_tiempo, table_id_dim_tiempo, job_config=job_config_tiempo\n",
        "    )\n",
        "\n",
        "    logger.info(f\"Starting BigQuery load job for Dim_Tiempo: {job_tiempo.job_id}\")\n",
        "    job_tiempo.result() # Wait for the job to complete\n",
        "\n",
        "    logger.info(f\"DataFrame successfully loaded to {table_id_dim_tiempo}\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error processing Dim_Tiempo: {e}\")\n",
        "    raise # Re-raise the exception"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-30T23:27:50.852+0000] {ipython-input-6-1925601096.py:87} ERROR - Error processing Dim_Tiempo: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x798596542b90>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RefreshError",
          "evalue": "(\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x798596542b90>)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRefreshError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-1925601096.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mdf_tiempo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_tiempo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Extracted {len(df_tiempo)} unique years.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_tiempo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, query, job_config, job_id, job_id_prefix, location, project, retry, timeout, job_retry, api_method)\u001b[0m\n\u001b[1;32m   3517\u001b[0m             )\n\u001b[1;32m   3518\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mapi_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0menums\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQueryApiMethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINSERT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3519\u001b[0;31m             return _job_helpers.query_jobs_insert(\n\u001b[0m\u001b[1;32m   3520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3521\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/_job_helpers.py\u001b[0m in \u001b[0;36mquery_jobs_insert\u001b[0;34m(client, query, job_config, job_id, job_id_prefix, location, project, retry, timeout, job_retry)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mdo_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_DEFAULT_QUERY_JOB_INSERT_RETRY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# The future might be in a failed state now, but if it's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/_job_helpers.py\u001b[0m in \u001b[0;36mdo_query\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mquery_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mcore_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConflict\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcreate_exc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;31m# The thought is if someone is providing their own job IDs and they get\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36m_begin\u001b[0;34m(self, client, retry, timeout)\u001b[0m\n\u001b[1;32m   1396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1398\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQueryJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1399\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGoogleAPICallError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m             exc.message = _EXCEPTION_FOOTER_TEMPLATE.format(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/job/base.py\u001b[0m in \u001b[0;36m_begin\u001b[0;34m(self, client, retry, timeout)\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;31m# job has an ID.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mspan_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         api_response = client._call_api(\n\u001b[0m\u001b[1;32m    781\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0mspan_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"BigQuery.job.begin\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36m_call_api\u001b[0;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[1;32m    856\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspan_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspan_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m             ):\n\u001b[0;32m--> 858\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/_http/__init__.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0mcontent_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         response = self._make_request(\n\u001b[0m\u001b[1;32m    483\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/_http/__init__.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, method, url, data, content_type, headers, target_object, timeout, extra_api_info)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         return self._do_request(\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_object\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/_http/__init__.py\u001b[0m in \u001b[0;36m_do_request\u001b[0;34m(self, method, url, headers, data, target_object, timeout)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mHTTP\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n\u001b[0;32m--> 379\u001b[0;31m         return self.http.request(\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/auth/transport/requests.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTimeoutGuard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mguard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0mremaining_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mguard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremaining_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/auth/credentials.py\u001b[0m in \u001b[0;36mbefore_request\u001b[0;34m(self, request, method, url, headers)\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_non_blocking_refresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocking_refresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_metric_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metric_header_for_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/auth/credentials.py\u001b[0m in \u001b[0;36m_blocking_refresh\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_blocking_refresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_non_blocking_refresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/auth/compute_engine/credentials.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcaught_exc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mnew_exc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRefreshError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaught_exc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcaught_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRefreshError\u001b[0m: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x798596542b90>)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2be7a79"
      },
      "source": [
        "### Subtask:\n",
        "Extraer valores únicos de `Tipo_Presion` de la tabla de origen, generar una clave subrogada para `Dim_TipoPresion` y cargarla en la tabla de BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "ff3f0547",
        "outputId": "d7c85107-9729-4429-f56c-9ad3b835c608"
      },
      "source": [
        "# Assuming BigQuery client and PROJECT_ID, DATASET_ID are defined and credentials are set up from previous steps.\n",
        "# If running this cell independently, you would need to re-initialize the client and credentials here.\n",
        "# Ensure BigQuery client is initialized and authenticated before running this.\n",
        "\n",
        "# Define the source table ID\n",
        "# Use the PROJECT_ID that was successful in previous loading attempts\n",
        "SOURCE_PROJECT_ID = 'etl-sistemas-sri' # Use the project ID where the source data was loaded\n",
        "DATASET_ID = \"sri_presion_fiscal\" # BigQuery Dataset ID\n",
        "SOURCE_TABLE_ID = f\"{SOURCE_PROJECT_ID}.{DATASET_ID}.sri_presion_fiscal\" # Source table for dimension data\n",
        "\n",
        "# Define destination table ID for Dim_TipoPresion\n",
        "# Assuming dimension tables are in the same project/dataset for simplicity here\n",
        "# In a real scenario, destination PROJECT_ID might be different\n",
        "PROJECT_ID = SOURCE_PROJECT_ID # Use source project as destination for dimension tables for now\n",
        "table_id_dim_tipopresion = f\"{PROJECT_ID}.{DATASET_ID}.Dim_TipoPresion\"\n",
        "\n",
        "# Define schema for Dim_TipoPresion (assuming it was defined in a previous cell or is defined here)\n",
        "# If not defined, ensure the cell defining schema_dim_tipopresion is run first.\n",
        "schema_dim_tipopresion = [\n",
        "    bigquery.SchemaField(\"tipo_presion_id\", \"INT64\", mode=\"REQUIRED\"), # Primary Key\n",
        "    bigquery.SchemaField(\"tipo_presion_nombre\", \"STRING\", mode=\"REQUIRED\"), # Tax Pressure Type Name\n",
        "]\n",
        "\n",
        "\n",
        "# Write a SQL query to select distinct Tipo_Presion values\n",
        "query_tipopresion = f\"\"\"\n",
        "SELECT DISTINCT Tipo_Presion\n",
        "FROM `{SOURCE_TABLE_ID}`\n",
        "WHERE Tipo_Presion IS NOT NULL AND Tipo_Presion != 'Tipo_Presion' # Exclude header row if present in data\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query and load results into a DataFrame for Dim_TipoPresion\n",
        "try:\n",
        "    # --- Ensure BigQuery client is initialized and authenticated ---\n",
        "    # This is a placeholder for demonstration. In a real Airflow task,\n",
        "    # the client initialization would likely happen within the task's callable\n",
        "    # or rely on Airflow's configuration.\n",
        "    # Assuming 'client' object is available from a previous successful cell execution.\n",
        "    if 'client' not in locals() or client is None:\n",
        "         # Attempt to re-initialize client if not found (for interactive testing)\n",
        "         try:\n",
        "             # Assuming the credential file path is known and accessible\n",
        "             credential_filepath = \"/etl-sistemas-sri-cc64af72be76.json\" # Use the full path provided by the user or your actual path\n",
        "             credentials = service_account.Credentials.from_service_account_file(credential_filepath)\n",
        "             client = bigquery.Client(project=SOURCE_PROJECT_ID, credentials=credentials)\n",
        "             logger.info(\"BigQuery client re-initialized for Dim_TipoPresion ETL.\")\n",
        "         except Exception as e:\n",
        "             logger.error(f\"Could not re-initialize BigQuery client for Dim_TipoPresion ETL: {e}\")\n",
        "             raise # Cannot proceed without a client\n",
        "\n",
        "\n",
        "    df_tipopresion = client.query(query_tipopresion).to_dataframe()\n",
        "    logger.info(f\"Extracted {len(df_tipopresion)} unique Tipo_Presion values.\")\n",
        "    display(df_tipopresion.head())\n",
        "\n",
        "    # Add a tipo_presion_id column (surrogate key)\n",
        "    df_tipopresion['tipo_presion_id'] = df_tipopresion.reset_index().index + 1 # Simple 1-based integer ID\n",
        "\n",
        "    # Rename the Tipo_Presion column to match the schema\n",
        "    df_tipopresion.rename(columns={'Tipo_Presion': 'tipo_presion_nombre'}, inplace=True)\n",
        "\n",
        "    # Select only the required columns and ensure order\n",
        "    df_tipopresion = df_tipopresion[['tipo_presion_id', 'tipo_presion_nombre']]\n",
        "\n",
        "    logger.info(\"Transformed Dim_TipoPresion DataFrame head:\")\n",
        "    display(df_tipopresion.head())\n",
        "    logger.info(\"Transformed Dim_TipoPresion DataFrame info:\")\n",
        "    display(df_tipopresion.info())\n",
        "\n",
        "\n",
        "    # Load the Dim_TipoPresion DataFrame into the BigQuery table\n",
        "    job_config_tipopresion = bigquery.LoadJobConfig(\n",
        "        schema=schema_dim_tipopresion, # Use the defined schema\n",
        "        write_disposition=\"WRITE_TRUNCATE\", # Overwrite the table\n",
        "    )\n",
        "\n",
        "    job_tipopresion = client.load_table_from_dataframe(\n",
        "        df_tipopresion, table_id_dim_tipopresion, job_config=job_config_tipopresion\n",
        "    )\n",
        "\n",
        "    logger.info(f\"Starting BigQuery load job for Dim_TipoPresion: {job_tipopresion.job_id}\")\n",
        "    job_tipopresion.result() # Wait for the job to complete\n",
        "\n",
        "    logger.info(f\"DataFrame successfully loaded to {table_id_dim_tipopresion}\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error processing Dim_TipoPresion: {e}\")\n",
        "    raise # Re-raise the exception"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-30T23:27:51.397+0000] {ipython-input-7-2165624256.py:87} ERROR - Error processing Dim_TipoPresion: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x798596319490>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RefreshError",
          "evalue": "(\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x798596319490>)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRefreshError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-2165624256.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mdf_tipopresion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_tipopresion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Extracted {len(df_tipopresion)} unique Tipo_Presion values.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_tipopresion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, query, job_config, job_id, job_id_prefix, location, project, retry, timeout, job_retry, api_method)\u001b[0m\n\u001b[1;32m   3517\u001b[0m             )\n\u001b[1;32m   3518\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mapi_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0menums\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQueryApiMethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINSERT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3519\u001b[0;31m             return _job_helpers.query_jobs_insert(\n\u001b[0m\u001b[1;32m   3520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3521\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/_job_helpers.py\u001b[0m in \u001b[0;36mquery_jobs_insert\u001b[0;34m(client, query, job_config, job_id, job_id_prefix, location, project, retry, timeout, job_retry)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mdo_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_DEFAULT_QUERY_JOB_INSERT_RETRY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# The future might be in a failed state now, but if it's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/_job_helpers.py\u001b[0m in \u001b[0;36mdo_query\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mquery_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mcore_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConflict\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcreate_exc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;31m# The thought is if someone is providing their own job IDs and they get\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36m_begin\u001b[0;34m(self, client, retry, timeout)\u001b[0m\n\u001b[1;32m   1396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1398\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQueryJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1399\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGoogleAPICallError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m             exc.message = _EXCEPTION_FOOTER_TEMPLATE.format(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/job/base.py\u001b[0m in \u001b[0;36m_begin\u001b[0;34m(self, client, retry, timeout)\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;31m# job has an ID.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mspan_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         api_response = client._call_api(\n\u001b[0m\u001b[1;32m    781\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0mspan_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"BigQuery.job.begin\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36m_call_api\u001b[0;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[1;32m    856\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspan_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspan_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m             ):\n\u001b[0;32m--> 858\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/_http/__init__.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0mcontent_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         response = self._make_request(\n\u001b[0m\u001b[1;32m    483\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/_http/__init__.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, method, url, data, content_type, headers, target_object, timeout, extra_api_info)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         return self._do_request(\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_object\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/_http/__init__.py\u001b[0m in \u001b[0;36m_do_request\u001b[0;34m(self, method, url, headers, data, target_object, timeout)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mHTTP\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n\u001b[0;32m--> 379\u001b[0;31m         return self.http.request(\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/auth/transport/requests.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTimeoutGuard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mguard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0mremaining_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mguard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremaining_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/auth/credentials.py\u001b[0m in \u001b[0;36mbefore_request\u001b[0;34m(self, request, method, url, headers)\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_non_blocking_refresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocking_refresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_metric_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metric_header_for_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/auth/credentials.py\u001b[0m in \u001b[0;36m_blocking_refresh\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_blocking_refresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_non_blocking_refresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/auth/compute_engine/credentials.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcaught_exc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mnew_exc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRefreshError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaught_exc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcaught_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRefreshError\u001b[0m: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x798596319490>)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46449387"
      },
      "source": [
        "## ETL de Hechos (Fase 3)\n",
        "\n",
        "### Subtask:\n",
        "Definir el esquema para la tabla de hechos (por ejemplo, Fact_PresionFiscal), incluyendo claves foráneas a las tablas de dimensiones y métricas, y crear la tabla de hechos en BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "b23daade",
        "outputId": "c3c6b2a9-f0f7-476b-d663-39691b0526ff"
      },
      "source": [
        "# Assuming BigQuery client is initialized and PROJECT_ID, DATASET_ID are defined\n",
        "\n",
        "# Define the schema for the Fact_PresionFiscal table\n",
        "# It includes foreign keys to dimension tables and the measure\n",
        "schema_fact = [\n",
        "    bigquery.SchemaField(\"id_geografia\", \"INT64\", mode=\"REQUIRED\"), # Foreign key to Dim_Geografia\n",
        "    bigquery.SchemaField(\"tiempo_id\", \"INT64\", mode=\"REQUIRED\"),   # Foreign key to Dim_Tiempo\n",
        "    bigquery.SchemaField(\"tipo_presion_id\", \"INT64\", mode=\"REQUIRED\"), # Foreign key to Dim_TipoPresion\n",
        "    bigquery.SchemaField(\"presion_fiscal\", \"FLOAT64\", mode=\"NULLABLE\"), # The measure (tax pressure percentage)\n",
        "]\n",
        "\n",
        "# Define the table ID for the Fact table\n",
        "# Using the unified PROJECT_ID and DATASET_ID\n",
        "table_id_fact = f\"{PROJECT_ID}.{DATASET_ID}.Fact_PresionFiscal\"\n",
        "\n",
        "# Assuming BigQuery client is initialized and authenticated from previous steps.\n",
        "# If running this cell independently, you would need to re-initialize the client and credentials here.\n",
        "# Ensure BigQuery client is initialized before running this.\n",
        "if 'client' not in locals() or client is None:\n",
        "    logger.error(\"BigQuery client is not initialized. Please ensure the client is initialized before creating the fact table.\")\n",
        "    # In a real DAG, this would be handled within a task's callable.\n",
        "    # For interactive testing, you might need to run the client initialization cell again.\n",
        "    raise ValueError(\"BigQuery client not initialized.\")\n",
        "\n",
        "\n",
        "# Create or get the Fact table in BigQuery\n",
        "def create_or_get_table(client, table_id, schema):\n",
        "    \"\"\"Creates a BigQuery table if it doesn't exist, or gets it if it does.\"\"\"\n",
        "    table_ref = bigquery.Table(table_id, schema=schema)\n",
        "    try:\n",
        "        table = client.create_table(table_ref, exists_ok=True)\n",
        "        logger.info(f\"Table '{table_id}' created or already exists.\")\n",
        "        return table\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error creating table '{table_id}': {e}\")\n",
        "        raise # Re-raise the exception\n",
        "\n",
        "table_fact = create_or_get_table(client, table_id_fact, schema_fact)\n",
        "\n",
        "logger.info(f\"Schema defined and Fact table '{table_id_fact}' created or verified in BigQuery.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-30T23:28:06.940+0000] {ipython-input-8-2321279528.py:35} ERROR - Error creating table 'etl-sistemas-sri.sri_presion_fiscal.Fact_PresionFiscal': (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x798596344a90>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RefreshError",
          "evalue": "(\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x798596344a90>)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRefreshError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-2321279528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0;31m# Re-raise the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mtable_fact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_or_get_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_id_fact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema_fact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Schema defined and Fact table '{table_id_fact}' created or verified in BigQuery.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-8-2321279528.py\u001b[0m in \u001b[0;36mcreate_or_get_table\u001b[0;34m(client, table_id, schema)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mtable_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexists_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Table '{table_id}' created or already exists.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36mcreate_table\u001b[0;34m(self, table, exists_ok, retry, timeout)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mspan_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dataset_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdataset_id\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m             api_response = self._call_api(\n\u001b[0m\u001b[1;32m    825\u001b[0m                 \u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m                 \u001b[0mspan_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"BigQuery.createTable\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36m_call_api\u001b[0;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[1;32m    856\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspan_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspan_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m             ):\n\u001b[0;32m--> 858\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/_http/__init__.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0mcontent_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         response = self._make_request(\n\u001b[0m\u001b[1;32m    483\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/_http/__init__.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, method, url, data, content_type, headers, target_object, timeout, extra_api_info)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         return self._do_request(\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_object\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/_http/__init__.py\u001b[0m in \u001b[0;36m_do_request\u001b[0;34m(self, method, url, headers, data, target_object, timeout)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mHTTP\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n\u001b[0;32m--> 379\u001b[0;31m         return self.http.request(\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/auth/transport/requests.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTimeoutGuard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mguard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0mremaining_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mguard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremaining_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/auth/credentials.py\u001b[0m in \u001b[0;36mbefore_request\u001b[0;34m(self, request, method, url, headers)\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_non_blocking_refresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocking_refresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_metric_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metric_header_for_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/auth/credentials.py\u001b[0m in \u001b[0;36m_blocking_refresh\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_blocking_refresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_non_blocking_refresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/auth/compute_engine/credentials.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcaught_exc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mnew_exc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRefreshError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaught_exc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcaught_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRefreshError\u001b[0m: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x798596344a90>)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f447bea"
      },
      "source": [
        "**Reasoning**:\n",
        "Extract transactional data from the source table, perform lookups with dimension tables to get surrogate keys, and prepare the data for loading into the fact table. This involves writing a SQL query to select the relevant columns from the source table, executing the query, creating a DataFrame, querying each dimension table to get the mapping between natural keys and surrogate keys, merging the source DataFrame with each dimension DataFrame using the natural keys, and selecting/renaming the columns to match the fact table schema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e5eaa50f",
        "outputId": "19291bc7-4230-4844-b450-c57f54ecd0db"
      },
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "import os\n",
        "import logging # Import logging\n",
        "import requests # Import requests for downloading\n",
        "\n",
        "# Configure logging (if not already configured)\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Assuming BigQuery client is initialized and PROJECT_ID, DATASET_ID are defined\n",
        "# If running this cell independently, you would need to re-initialize the client and credentials here.\n",
        "# Ensure BigQuery client is initialized and authenticated before running this.\n",
        "\n",
        "# --- Re-initialize BigQuery client and load credentials for this step if needed ---\n",
        "# This is a placeholder for demonstration. In a real Airflow task,\n",
        "# the client initialization would likely happen within the task's callable\n",
        "# or rely on Airflow's configuration.\n",
        "# Assuming the credential file path is known and accessible\n",
        "# Use the absolute path that was successful in previous attempts or is known\n",
        "credential_filepath = \"/etl-sistemas-sri-cc64af72be76.json\" # Use the full path provided by the user or your actual path\n",
        "SOURCE_PROJECT_ID = 'etl-sistemas-sri' # Use the project ID where the source data was loaded\n",
        "DATASET_ID = \"sri_presion_fiscal\" # BigQuery Dataset ID\n",
        "\n",
        "try:\n",
        "    # Load credentials explicitly\n",
        "    credentials = service_account.Credentials.from_service_account_file(credential_filepath)\n",
        "    logger.info(\"Google Cloud credentials re-loaded successfully for fact ETL.\")\n",
        "\n",
        "    # Initialize BigQuery client with the source project and loaded credentials\n",
        "    client = bigquery.Client(project=SOURCE_PROJECT_ID, credentials=credentials)\n",
        "    logger.info(f\"BigQuery client re-initialized for source project: {SOURCE_PROJECT_ID}.\")\n",
        "\n",
        "    # Define dimension table IDs in the target project/dataset (assuming it's the same as source for simplicity here)\n",
        "    # In a real scenario, destination PROJECT_ID might be different\n",
        "    PROJECT_ID = SOURCE_PROJECT_ID # Use source project as destination for dimension tables for now\n",
        "    table_id_dim_geografia = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Geografia\"\n",
        "    table_id_dim_tiempo = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Tiempo\"\n",
        "    table_id_dim_tipopresion = f\"{PROJECT_ID}.{DATASET_ID}.Dim_TipoPresion\"\n",
        "\n",
        "except FileNotFoundError:\n",
        "    logger.error(f\"Error: Credential file '{credential_filepath}' not found. Cannot proceed with fact ETL.\")\n",
        "    # Cannot proceed with BigQuery operations without credentials\n",
        "    raise # Re-raise the exception\n",
        "except Exception as e:\n",
        "    logger.error(f\"An unexpected error occurred during credential loading or client initialization for fact ETL: {e}\")\n",
        "    # Cannot proceed with BigQuery operations\n",
        "    raise # Re-raise any other exception\n",
        "# --- End of re-initialization ---\n",
        "\n",
        "\n",
        "# Define the source table ID using the correct project and dataset\n",
        "SOURCE_TABLE_ID = f\"{SOURCE_PROJECT_ID}.{DATASET_ID}.sri_presion_fiscal\" # Source table for dimension data\n",
        "\n",
        "\n",
        "# --- Extract Source Data from BigQuery ---\n",
        "# In a real Airflow DAG, this might be a separate task or part of a larger Python function.\n",
        "# For this interactive example, we extract directly here.\n",
        "query_source_data = f\"\"\"\n",
        "SELECT\n",
        "    Region,\n",
        "    `País`,\n",
        "    `Año`,\n",
        "    `%_Presion`,\n",
        "    Tipo_Presion\n",
        "FROM `{SOURCE_TABLE_ID}`\n",
        "WHERE Region IS NOT NULL\n",
        "  AND `País` IS NOT NULL\n",
        "  AND `Año` IS NOT NULL\n",
        "  AND `%_Presion` IS NOT NULL\n",
        "  AND Tipo_Presion IS NOT NULL\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    df_source_data = client.query(query_source_data).to_dataframe()\n",
        "    logger.info(f\"\\nExtracted {len(df_source_data)} rows from source table for fact processing.\")\n",
        "    display(df_source_data.head())\n",
        "    display(df_source_data.info())\n",
        "\n",
        "    # Ensure data types are correct before merging\n",
        "    # Convert 'Año' to integer (Int64 to handle potential NaNs)\n",
        "    df_source_data['Año'] = pd.to_numeric(df_source_data['Año'], errors='coerce').astype('Int64')\n",
        "    # Convert '%_Presion' to float\n",
        "    df_source_data['%_Presion'] = pd.to_numeric(df_source_data['%_Presion'], errors='coerce')\n",
        "    logger.info(\"Source data types cleaned for merging.\")\n",
        "    display(df_source_data.info())\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error extracting source data from BigQuery: {e}\")\n",
        "    # Cannot proceed with fact transformation without source data\n",
        "    raise # Re-raise the exception\n",
        "# --- End of Extract Source Data ---\n",
        "\n",
        "\n",
        "# --- Extract Dimension Data from BigQuery for Lookups ---\n",
        "# These queries assume the dimension tables exist and are populated.\n",
        "\n",
        "# Query Dim_Geografia\n",
        "query_dim_geografia = f\"\"\"\n",
        "SELECT id_geografia, pais, region\n",
        "FROM `{table_id_dim_geografia}`\n",
        "\"\"\"\n",
        "try:\n",
        "    df_dim_geografia = client.query(query_dim_geografia).to_dataframe()\n",
        "    logger.info(f\"\\nExtracted {len(df_dim_geografia)} rows from Dim_Geografia for lookup.\")\n",
        "    display(df_dim_geografia.head())\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error extracting Dim_Geografia data from BigQuery: {e}\")\n",
        "    # Cannot proceed with fact transformation without dimension data\n",
        "    raise # Re-raise the exception\n",
        "\n",
        "# Query Dim_Tiempo\n",
        "query_dim_tiempo = f\"\"\"\n",
        "SELECT tiempo_id, año\n",
        "FROM `{table_id_dim_tiempo}`\n",
        "\"\"\"\n",
        "try:\n",
        "    df_dim_tiempo = client.query(query_dim_tiempo).to_dataframe()\n",
        "    logger.info(f\"\\nExtracted {len(df_dim_tiempo)} rows from Dim_Tiempo for lookup.\")\n",
        "    display(df_dim_tiempo.head())\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error extracting Dim_Tiempo data from BigQuery: {e}\")\n",
        "    # Cannot proceed with fact transformation without dimension data\n",
        "    raise # Re-raise the exception\n",
        "\n",
        "# Query Dim_TipoPresion\n",
        "query_dim_tipopresion = f\"\"\"\n",
        "SELECT tipo_presion_id, tipo_presion_nombre\n",
        "FROM `{table_id_dim_tipopresion}`\n",
        "\"\"\"\n",
        "try:\n",
        "    df_dim_tipopresion = client.query(query_dim_tipopresion).to_dataframe()\n",
        "    logger.info(f\"\\nExtracted {len(df_dim_tipopresion)} rows from Dim_TipoPresion for lookup.\")\n",
        "    display(df_dim_tipopresion.head())\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error extracting Dim_TipoPresion data from BigQuery: {e}\")\n",
        "    # Cannot proceed with fact transformation without dimension data\n",
        "    raise # Re-raise the exception\n",
        "# --- End of Extract Dimension Data ---\n",
        "\n",
        "\n",
        "# --- Perform Lookups and Prepare Fact Data ---\n",
        "\n",
        "# Start with the extracted source data\n",
        "df_fact_staging = df_source_data.copy()\n",
        "\n",
        "# Merge with Dim_Geografia to get id_geografia\n",
        "# Ensure column names match between source data and dimension table for merging\n",
        "df_fact_staging = pd.merge(df_fact_staging, df_dim_geografia,\n",
        "                           left_on=['País', 'Region'], right_on=['pais', 'region'], how='left')\n",
        "\n",
        "# Merge with Dim_Tiempo to get tiempo_id\n",
        "df_fact_staging = pd.merge(df_fact_staging, df_dim_tiempo,\n",
        "                           left_on='Año', right_on='año', how='left')\n",
        "\n",
        "# Merge with Dim_TipoPresion to get tipo_presion_id\n",
        "df_fact_staging = pd.merge(df_fact_staging, df_dim_tipopresion,\n",
        "                           left_on='Tipo_Presion', right_on='tipo_presion_nombre', how='left')\n",
        "\n",
        "\n",
        "# Check for unmatched rows (where lookups resulted in NaN IDs)\n",
        "# This indicates issues with data consistency between source and dimensions\n",
        "unmatched_rows_geografia = df_fact_staging[df_fact_staging['id_geografia'].isnull()]\n",
        "if not unmatched_rows_geografia.empty:\n",
        "    logger.warning(f\"\\nWarning: {len(unmatched_rows_geografia)} rows did not match in Dim_Geografia during lookup.\")\n",
        "    display(unmatched_rows_geografia[['País', 'Region', 'id_geografia']].head())\n",
        "\n",
        "unmatched_rows_tiempo = df_fact_staging[df_fact_staging['tiempo_id'].isnull()]\n",
        "if not unmatched_rows_tiempo.empty:\n",
        "    logger.warning(f\"\\nWarning: {len(unmatched_rows_tiempo)} rows did not match in Dim_Tiempo during lookup.\")\n",
        "    display(unmatched_rows_tiempo[['Año', 'tiempo_id']].head())\n",
        "\n",
        "unmatched_rows_tipopresion = df_fact_staging[df_fact_staging['tipo_presion_id'].isnull()]\n",
        "if not unmatched_rows_tipopresion.empty:\n",
        "    logger.warning(f\"\\nWarning: {len(unmatched_rows_tipopresion)} rows did not match in Dim_TipoPresion during lookup.\")\n",
        "    display(unmatched_rows_tipopresion[['Tipo_Presion', 'tipo_presion_id']].head())\n",
        "\n",
        "\n",
        "# Select and rename columns to match the fact table schema\n",
        "# Ensure all required surrogate key columns are present after merges\n",
        "df_fact = df_fact_staging[['id_geografia', 'tiempo_id', 'tipo_presion_id', '%_Presion']].copy() # Copy to avoid SettingWithCopyWarning\n",
        "\n",
        "# Rename the measure column\n",
        "df_fact.rename(columns={'%_Presion': 'presion_fiscal'}, inplace=True)\n",
        "\n",
        "# Ensure surrogate key columns are of type Int64 (nullable integer)\n",
        "# This is important as merges can introduce floats for integer columns with NaNs\n",
        "for col_id in ['id_geografia', 'tiempo_id', 'tipo_presion_id']:\n",
        "    if col_id in df_fact.columns:\n",
        "        # Convert to Int64 to allow for potential NaN values from left merge\n",
        "        # Use errors='coerce' just in case, though merge should handle this if types match\n",
        "        df_fact[col_id] = pd.to_numeric(df_fact[col_id], errors='coerce').astype('Int64')\n",
        "\n",
        "\n",
        "logger.info(\"\\nTransformed Fact DataFrame head (with surrogate keys):\")\n",
        "display(df_fact.head())\n",
        "logger.info(\"\\nTransformed Fact DataFrame info:\")\n",
        "display(df_fact.info())\n",
        "\n",
        "# --- End of Lookups and Prepare Fact Data ---\n",
        "\n",
        "# Note: The actual loading to BigQuery will be a separate task in the final DAG.\n",
        "# This code block focuses on the data transformation and lookup part.\n",
        "logger.info(\"Fact data transformation and lookup complete. Ready for loading to BigQuery.\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-30T23:40:58.073+0000] {ipython-input-28-3510985816.py:29} INFO - Google Cloud credentials re-loaded successfully for fact ETL.\n",
            "[2025-06-30T23:40:58.076+0000] {ipython-input-28-3510985816.py:33} INFO - BigQuery client re-initialized for source project: etl-sistemas-sri.\n",
            "[2025-06-30T23:40:59.811+0000] {ipython-input-28-3510985816.py:77} INFO - \n",
            "Extracted 3043 rows from source table for fact processing.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "           Region       País   Año  %_Presion          Tipo_Presion\n",
              "0  America Latina  Argentina  2004     0.2433  Presión Fiscal Total\n",
              "1  America Latina  Argentina  2005     0.2453  Presión Fiscal Total\n",
              "2  America Latina  Argentina  2006     0.2508  Presión Fiscal Total\n",
              "3  America Latina  Argentina  2007     0.2637  Presión Fiscal Total\n",
              "4  America Latina  Argentina  2008     0.2762  Presión Fiscal Total"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-caa4b9bd-304c-470b-9968-9cd7ffd2a712\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Region</th>\n",
              "      <th>País</th>\n",
              "      <th>Año</th>\n",
              "      <th>%_Presion</th>\n",
              "      <th>Tipo_Presion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2004</td>\n",
              "      <td>0.2433</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2005</td>\n",
              "      <td>0.2453</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2006</td>\n",
              "      <td>0.2508</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2007</td>\n",
              "      <td>0.2637</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>America Latina</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>2008</td>\n",
              "      <td>0.2762</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-caa4b9bd-304c-470b-9968-9cd7ffd2a712')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-caa4b9bd-304c-470b-9968-9cd7ffd2a712 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-caa4b9bd-304c-470b-9968-9cd7ffd2a712');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-79a4d620-c728-49fa-8b45-d58a11a48818\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-79a4d620-c728-49fa-8b45-d58a11a48818')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-79a4d620-c728-49fa-8b45-d58a11a48818 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"logger\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Region\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"America Latina\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pa\\u00eds\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Argentina\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"A\\u00f1o\",\n      \"properties\": {\n        \"dtype\": \"Int64\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2005\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"%_Presion\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.01387490540508295,\n        \"min\": 0.2433,\n        \"max\": 0.2762,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.2453\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tipo_Presion\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Presi\\u00f3n Fiscal Total\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3043 entries, 0 to 3042\n",
            "Data columns (total 5 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Region        3043 non-null   object \n",
            " 1   País          3043 non-null   object \n",
            " 2   Año           3043 non-null   Int64  \n",
            " 3   %_Presion     3043 non-null   float64\n",
            " 4   Tipo_Presion  3043 non-null   object \n",
            "dtypes: Int64(1), float64(1), object(3)\n",
            "memory usage: 122.0+ KB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-30T23:40:59.860+0000] {ipython-input-28-3510985816.py:86} INFO - Source data types cleaned for merging.\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3043 entries, 0 to 3042\n",
            "Data columns (total 5 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Region        3043 non-null   object \n",
            " 1   País          3043 non-null   object \n",
            " 2   Año           3043 non-null   Int64  \n",
            " 3   %_Presion     3043 non-null   float64\n",
            " 4   Tipo_Presion  3043 non-null   object \n",
            "dtypes: Int64(1), float64(1), object(3)\n",
            "memory usage: 122.0+ KB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-30T23:41:01.592+0000] {ipython-input-28-3510985816.py:107} INFO - \n",
            "Extracted 40 rows from Dim_Geografia for lookup.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   id_geografia        pais          region\n",
              "0             2   Argentina  America Latina\n",
              "1             4      Brasil  America Latina\n",
              "2             6       Chile  America Latina\n",
              "3             7    Colombia  America Latina\n",
              "4             8  Costa Rica  America Latina"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c18028de-3813-4282-81a7-98b2f61329da\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_geografia</th>\n",
              "      <th>pais</th>\n",
              "      <th>region</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>America Latina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>Brasil</td>\n",
              "      <td>America Latina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "      <td>Chile</td>\n",
              "      <td>America Latina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7</td>\n",
              "      <td>Colombia</td>\n",
              "      <td>America Latina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "      <td>Costa Rica</td>\n",
              "      <td>America Latina</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c18028de-3813-4282-81a7-98b2f61329da')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c18028de-3813-4282-81a7-98b2f61329da button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c18028de-3813-4282-81a7-98b2f61329da');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-84c96eb8-cd44-4417-bf85-c78b78bd5175\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-84c96eb8-cd44-4417-bf85-c78b78bd5175')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-84c96eb8-cd44-4417-bf85-c78b78bd5175 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"logger\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id_geografia\",\n      \"properties\": {\n        \"dtype\": \"Int64\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4,\n          8,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pais\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Brasil\",\n          \"Costa Rica\",\n          \"Chile\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"region\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"America Latina\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-30T23:41:01.738+0000] {ipython-input-28-3510985816.py:124} ERROR - Error extracting Dim_Tiempo data from BigQuery: 400 Syntax error: Illegal input character \"\\303\" at [2:20]; reason: invalidQuery, location: query, message: Syntax error: Illegal input character \"\\303\" at [2:20]\n",
            "\n",
            "Location: US\n",
            "Job ID: b9891f12-115b-4b64-b7d0-6c0af347ec47\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BadRequest",
          "evalue": "400 Syntax error: Illegal input character \"\\303\" at [2:20]; reason: invalidQuery, location: query, message: Syntax error: Illegal input character \"\\303\" at [2:20]\n\nLocation: US\nJob ID: b9891f12-115b-4b64-b7d0-6c0af347ec47\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-28-3510985816.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \"\"\"\n\u001b[1;32m    119\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mdf_dim_tiempo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_dim_tiempo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nExtracted {len(df_dim_tiempo)} rows from Dim_Tiempo for lookup.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_dim_tiempo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mto_dataframe\u001b[0;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, max_results, geography_as_object, bool_dtype, int_dtype, float_dtype, string_dtype, date_dtype, datetime_dtype, time_dtype, timestamp_dtype, range_date_dtype, range_datetime_dtype, range_timestamp_dtype)\u001b[0m\n\u001b[1;32m   2072\u001b[0m                 \u001b[0;34m:\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mshapely\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mlibrary\u001b[0m \u001b[0mcannot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mimported\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2073\u001b[0m         \"\"\"\n\u001b[0;32m-> 2074\u001b[0;31m         \u001b[0mquery_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait_for_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2075\u001b[0m         return query_result.to_dataframe(\n\u001b[1;32m   2076\u001b[0m             \u001b[0mbqstorage_client\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbqstorage_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/_tqdm_helpers.py\u001b[0m in \u001b[0;36mwait_for_query\u001b[0;34m(query_job, progress_bar_type, max_results)\u001b[0m\n\u001b[1;32m    105\u001b[0m     )\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprogress_bar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mquery_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, page_size, max_results, retry, timeout, start_index, job_retry)\u001b[0m\n\u001b[1;32m   1694\u001b[0m                 \u001b[0;31m# Since is_job_done() calls jobs.getQueryResults, which is a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m                 \u001b[0;31m# long-running API, don't delay the next request at all.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1696\u001b[0;31m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_job_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1697\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mis_job_done\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1643\u001b[0m                         \u001b[0;31m# `job_retry` predicate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m                         \u001b[0mrestart_query_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mjob_failed_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m                         \u001b[0;31m# Make sure that the _query_results are cached so we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequest\u001b[0m: 400 Syntax error: Illegal input character \"\\303\" at [2:20]; reason: invalidQuery, location: query, message: Syntax error: Illegal input character \"\\303\" at [2:20]\n\nLocation: US\nJob ID: b9891f12-115b-4b64-b7d0-6c0af347ec47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f625c2a9"
      },
      "source": [
        "## Poblar Dim_Geografia\n",
        "\n",
        "### Subtask:\n",
        "Extraer combinaciones únicas de País y Región de la tabla de origen, generar una clave subrogada para `Dim_Geografia` y cargarla en la tabla de BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "id": "9a67c2ea",
        "outputId": "48949892-10bd-4361-c8ba-37a7721997d5"
      },
      "source": [
        "# Assuming BigQuery client is initialized and PROJECT_ID, DATASET_ID are defined and credentials are set up from previous steps.\n",
        "# If running this cell independently, you would need to re-initialize the client and credentials here.\n",
        "\n",
        "# Define the source table ID using the correct project and dataset\n",
        "# Use the PROJECT_ID that was successful in previous loading attempts\n",
        "SOURCE_PROJECT_ID = 'etl-sistemas-sri' # Use the project ID where the source data was loaded\n",
        "DATASET_ID = \"sri_presion_fiscal\" # BigQuery Dataset ID\n",
        "SOURCE_TABLE_ID = f\"{SOURCE_PROJECT_ID}.{DATASET_ID}.sri_presion_fiscal\" # Source table for dimension data\n",
        "\n",
        "# Define destination table ID for Dim_Geografia\n",
        "# Assuming dimension tables are in the same project/dataset for simplicity here\n",
        "# In a real scenario, destination PROJECT_ID might be different\n",
        "PROJECT_ID = SOURCE_PROJECT_ID # Use source project as destination for dimension tables for now\n",
        "table_id_dim_geografia = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Geografia\"\n",
        "\n",
        "# Define schema for Dim_Geografia (assuming it was defined in a previous cell or is defined here)\n",
        "# If not defined, ensure the cell defining schema_dim_geografia is run first.\n",
        "schema_dim_geografia = [\n",
        "    bigquery.SchemaField(\"id_geografia\", \"INT64\", mode=\"REQUIRED\"), # Primary Key\n",
        "    bigquery.SchemaField(\"pais\", \"STRING\", mode=\"REQUIRED\"),      # Country\n",
        "    bigquery.SchemaField(\"region\", \"STRING\", mode=\"REQUIRED\"),    # Region\n",
        "]\n",
        "\n",
        "# --- Ensure BigQuery client is initialized and authenticated ---\n",
        "# This is a placeholder for demonstration. In a real Airflow task,\n",
        "# the client initialization would likely happen within the task's callable\n",
        "# or rely on Airflow's configuration.\n",
        "# Assuming 'client' object is available and authenticated from a previous successful cell execution.\n",
        "if 'client' not in locals() or client is None:\n",
        "     # Attempt to re-initialize client if not found (for interactive testing)\n",
        "     try:\n",
        "         # Assuming the credential file path is known and accessible\n",
        "         credential_filepath = \"/etl-sistemas-sri-cc64af72be76.json\" # Use the full path provided by the user or your actual path\n",
        "         credentials = service_account.Credentials.from_service_account_file(credential_filepath)\n",
        "         client = bigquery.Client(project=SOURCE_PROJECT_ID, credentials=credentials)\n",
        "         logger.info(\"BigQuery client re-initialized for Dim_Geografia ETL.\")\n",
        "     except Exception as e:\n",
        "         logger.error(f\"Could not re-initialize BigQuery client for Dim_Geografia ETL: {e}\")\n",
        "         raise # Cannot proceed without a client\n",
        "# --- End of re-initialization ---\n",
        "\n",
        "\n",
        "# Write a SQL query to select distinct Country and Region combinations\n",
        "query_geografia = f\"\"\"\n",
        "SELECT DISTINCT `País`, Region\n",
        "FROM `{SOURCE_TABLE_ID}`\n",
        "WHERE `País` IS NOT NULL AND `País` != 'País' -- Exclude header row if present\n",
        "  AND Region IS NOT NULL AND Region != 'Region' -- Exclude header row if present\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query and load results into a DataFrame for Dim_Geografia\n",
        "try:\n",
        "    df_geografia = client.query(query_geografia).to_dataframe()\n",
        "    logger.info(f\"Extracted {len(df_geografia)} unique Country-Region combinations.\")\n",
        "    display(df_geografia.head())\n",
        "\n",
        "    # Add an id_geografia column (surrogate key)\n",
        "    # Sort by País and Region before generating ID for consistency\n",
        "    df_geografia = df_geografia.sort_values(by=['País', 'Region']).reset_index(drop=True)\n",
        "    df_geografia['id_geografia'] = df_geografia.index + 1 # Simple 1-based integer ID\n",
        "\n",
        "    # Rename columns to match the schema\n",
        "    df_geografia.rename(columns={'País': 'pais', 'Region': 'region'}, inplace=True)\n",
        "\n",
        "    # Select only the required columns and ensure order\n",
        "    df_geografia = df_geografia[['id_geografia', 'pais', 'region']]\n",
        "\n",
        "    logger.info(\"Transformed Dim_Geografia DataFrame head:\")\n",
        "    display(df_geografia.head())\n",
        "    logger.info(\"Transformed Dim_Geografia DataFrame info:\")\n",
        "    display(df_geografia.info())\n",
        "\n",
        "\n",
        "    # Load the Dim_Geografia DataFrame into the BigQuery table\n",
        "    job_config_geografia = bigquery.LoadJobConfig(\n",
        "        schema=schema_dim_geografia, # Use the defined schema for Dim_Geografia\n",
        "        write_disposition=\"WRITE_TRUNCATE\", # Overwrite the table\n",
        "    )\n",
        "\n",
        "    job_geografia = client.load_table_from_dataframe(\n",
        "        df_geografia, table_id_dim_geografia, job_config=job_config_geografia\n",
        "    )\n",
        "\n",
        "    logger.info(f\"Starting BigQuery load job for Dim_Geografia: {job_geografia.job_id}\")\n",
        "    job_geografia.result() # Wait for the job to complete\n",
        "\n",
        "    logger.info(f\"DataFrame successfully loaded to {table_id_dim_geografia}\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error processing Dim_Geografia: {e}\")\n",
        "    raise # Re-raise the exception"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-30T23:30:23.321+0000] {ipython-input-11-305608067.py:54} INFO - Extracted 40 unique Country-Region combinations.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         País          Region\n",
              "0   Argentina  America Latina\n",
              "1      Brasil  America Latina\n",
              "2       Chile  America Latina\n",
              "3    Colombia  America Latina\n",
              "4  Costa Rica  America Latina"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f83ecca4-d4dd-44f2-9a41-d9385ee5c5e7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>País</th>\n",
              "      <th>Region</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Argentina</td>\n",
              "      <td>America Latina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Brasil</td>\n",
              "      <td>America Latina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Chile</td>\n",
              "      <td>America Latina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Colombia</td>\n",
              "      <td>America Latina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Costa Rica</td>\n",
              "      <td>America Latina</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f83ecca4-d4dd-44f2-9a41-d9385ee5c5e7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f83ecca4-d4dd-44f2-9a41-d9385ee5c5e7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f83ecca4-d4dd-44f2-9a41-d9385ee5c5e7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7c6fed45-b350-4fea-96c3-425dd2f3ffc1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7c6fed45-b350-4fea-96c3-425dd2f3ffc1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7c6fed45-b350-4fea-96c3-425dd2f3ffc1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    raise # Re-raise the exception\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Pa\\u00eds\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Brasil\",\n          \"Costa Rica\",\n          \"Chile\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Region\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"America Latina\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-30T23:30:23.412+0000] {ipython-input-11-305608067.py:68} INFO - Transformed Dim_Geografia DataFrame head:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   id_geografia       pais          region\n",
              "0             1   Alemania          Europa\n",
              "1             2  Argentina  America Latina\n",
              "2             3    Austria          Europa\n",
              "3             4     Brasil  America Latina\n",
              "4             5    Bélgica          Europa"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9c8ec967-a020-4a86-9459-ff7f8305f26a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_geografia</th>\n",
              "      <th>pais</th>\n",
              "      <th>region</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Alemania</td>\n",
              "      <td>Europa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>America Latina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Austria</td>\n",
              "      <td>Europa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Brasil</td>\n",
              "      <td>America Latina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Bélgica</td>\n",
              "      <td>Europa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9c8ec967-a020-4a86-9459-ff7f8305f26a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9c8ec967-a020-4a86-9459-ff7f8305f26a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9c8ec967-a020-4a86-9459-ff7f8305f26a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a33bfc93-c0eb-446b-a697-e672f22fe574\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a33bfc93-c0eb-446b-a697-e672f22fe574')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a33bfc93-c0eb-446b-a697-e672f22fe574 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    raise # Re-raise the exception\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id_geografia\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pais\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Argentina\",\n          \"B\\u00e9lgica\",\n          \"Austria\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"region\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"America Latina\",\n          \"Europa\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-30T23:30:23.456+0000] {ipython-input-11-305608067.py:70} INFO - Transformed Dim_Geografia DataFrame info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 40 entries, 0 to 39\n",
            "Data columns (total 3 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   id_geografia  40 non-null     int64 \n",
            " 1   pais          40 non-null     object\n",
            " 2   region        40 non-null     object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 1.1+ KB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-30T23:30:24.422+0000] {ipython-input-11-305608067.py:84} INFO - Starting BigQuery load job for Dim_Geografia: 7662180c-2c59-46b2-ad06-fdba1bc7e195\n",
            "[2025-06-30T23:30:26.578+0000] {ipython-input-11-305608067.py:87} INFO - DataFrame successfully loaded to etl-sistemas-sri.sri_presion_fiscal.Dim_Geografia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcfbfa2a"
      },
      "source": [
        "### Subtask:\n",
        "Extraer años únicos de la tabla de origen, generar una clave subrogada para `Dim_Tiempo` y cargarla en la tabla de BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "id": "da5130d3",
        "outputId": "a9cf4482-d415-4270-8cf9-e5be8ce3266d"
      },
      "source": [
        "# Assuming BigQuery client and PROJECT_ID, DATASET_ID are defined and credentials are set up from previous steps.\n",
        "# If running this cell independently, you would need to re-initialize the client and credentials here.\n",
        "# Ensure BigQuery client is initialized and authenticated before running this.\n",
        "\n",
        "# Define the source table ID\n",
        "# Use the PROJECT_ID that was successful in previous loading attempts\n",
        "SOURCE_PROJECT_ID = 'etl-sistemas-sri' # Use the project ID where the source data was loaded\n",
        "DATASET_ID = \"sri_presion_fiscal\" # BigQuery Dataset ID\n",
        "SOURCE_TABLE_ID = f\"{SOURCE_PROJECT_ID}.{DATASET_ID}.sri_presion_fiscal\" # Source table for dimension data\n",
        "\n",
        "# Define destination table ID for Dim_Tiempo\n",
        "# Assuming dimension tables are in the same project/dataset for simplicity here\n",
        "# In a real scenario, destination PROJECT_ID might be different\n",
        "PROJECT_ID = SOURCE_PROJECT_ID # Use source project as destination for dimension tables for now\n",
        "table_id_dim_tiempo = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Tiempo\"\n",
        "\n",
        "# Define schema for Dim_Tiempo (assuming it was defined in a previous cell or is defined here)\n",
        "# If not defined, ensure the cell defining schema_dim_tiempo is run first.\n",
        "schema_dim_tiempo = [\n",
        "    bigquery.SchemaField(\"tiempo_id\", \"INT64\", mode=\"REQUIRED\"), # Primary Key\n",
        "    bigquery.SchemaField(\"año\", \"INT64\", mode=\"REQUIRED\"),      # Year\n",
        "]\n",
        "\n",
        "\n",
        "# Write a SQL query to select distinct years\n",
        "query_tiempo = f\"\"\"\n",
        "SELECT DISTINCT `Año`\n",
        "FROM `{SOURCE_TABLE_ID}`\n",
        "WHERE `Año` IS NOT NULL # Keep the check for non-null years\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query and load results into a DataFrame for Dim_Tiempo\n",
        "try:\n",
        "    # --- Ensure BigQuery client is initialized and authenticated ---\n",
        "    # This is a placeholder for demonstration. In a real Airflow task,\n",
        "    # the client initialization would likely happen within the task's callable\n",
        "    # or rely on Airflow's configuration.\n",
        "    # Assuming 'client' object is available from a previous successful cell execution.\n",
        "    if 'client' not in locals() or client is None:\n",
        "         # Attempt to re-initialize client if not found (for interactive testing)\n",
        "         try:\n",
        "             # Assuming the credential file path is known and accessible\n",
        "             credential_filepath = \"/etl-sistemas-sri-cc64af72be76.json\" # Use the full path provided by the user or your actual path\n",
        "             credentials = service_account.Credentials.from_service_account_file(credential_filepath)\n",
        "             client = bigquery.Client(project=SOURCE_PROJECT_ID, credentials=credentials)\n",
        "             logger.info(\"BigQuery client re-initialized for Dim_Tiempo ETL.\")\n",
        "         except Exception as e:\n",
        "             logger.error(f\"Could not re-initialize BigQuery client for Dim_Tiempo ETL: {e}\")\n",
        "             raise # Cannot proceed without a client\n",
        "\n",
        "\n",
        "    df_tiempo = client.query(query_tiempo).to_dataframe()\n",
        "    logger.info(f\"Extracted {len(df_tiempo)} unique years.\")\n",
        "    display(df_tiempo.head())\n",
        "\n",
        "    # Add a tiempo_id column (surrogate key)\n",
        "    # Simple 1-based integer ID based on the sorted years\n",
        "    df_tiempo['tiempo_id'] = df_tiempo['Año'].rank(method='dense').astype('Int64') # Using rank for dense ID\n",
        "\n",
        "    # Rename the year column to match the schema (already 'Año', but good practice)\n",
        "    df_tiempo.rename(columns={'Año': 'año'}, inplace=True)\n",
        "\n",
        "    # Select only the required columns and ensure order\n",
        "    df_tiempo = df_tiempo[['tiempo_id', 'año']]\n",
        "\n",
        "    logger.info(\"Transformed Dim_Tiempo DataFrame head:\")\n",
        "    display(df_tiempo.head())\n",
        "    logger.info(\"Transformed Dim_Tiempo DataFrame info:\")\n",
        "    display(df_tiempo.info())\n",
        "\n",
        "    # Load the Dim_Tiempo DataFrame into the BigQuery table\n",
        "    job_config_tiempo = bigquery.LoadJobConfig(\n",
        "        schema=schema_dim_tiempo, # Use the defined schema\n",
        "        write_disposition=\"WRITE_TRUNCATE\", # Overwrite the table\n",
        "    )\n",
        "\n",
        "    job_tiempo = client.load_table_from_dataframe(\n",
        "        df_tiempo, table_id_dim_tiempo, job_config=job_config_tiempo\n",
        "    )\n",
        "\n",
        "    logger.info(f\"Starting BigQuery load job for Dim_Tiempo: {job_tiempo.job_id}\")\n",
        "    job_tiempo.result() # Wait for the job to complete\n",
        "\n",
        "    logger.info(f\"DataFrame successfully loaded to {table_id_dim_tiempo}\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error processing Dim_Tiempo: {e}\")\n",
        "    raise # Re-raise the exception"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-30T23:30:44.214+0000] {ipython-input-12-1925601096.py:53} INFO - Extracted 20 unique years.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    Año\n",
              "0  2004\n",
              "1  2005\n",
              "2  2006\n",
              "3  2007\n",
              "4  2008"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fde9b7b1-aab9-4d65-ad63-ace29b23ca71\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Año</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2008</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fde9b7b1-aab9-4d65-ad63-ace29b23ca71')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fde9b7b1-aab9-4d65-ad63-ace29b23ca71 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fde9b7b1-aab9-4d65-ad63-ace29b23ca71');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e97d762f-1c91-4ddc-99f9-0ca4f4294a71\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e97d762f-1c91-4ddc-99f9-0ca4f4294a71')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e97d762f-1c91-4ddc-99f9-0ca4f4294a71 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    raise # Re-raise the exception\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"A\\u00f1o\",\n      \"properties\": {\n        \"dtype\": \"Int64\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2005,\n          2008,\n          2006\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-30T23:30:44.245+0000] {ipython-input-12-1925601096.py:66} INFO - Transformed Dim_Tiempo DataFrame head:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   tiempo_id   año\n",
              "0          1  2004\n",
              "1          2  2005\n",
              "2          3  2006\n",
              "3          4  2007\n",
              "4          5  2008"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a492ff90-03e5-4d65-95cf-b925acb84301\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tiempo_id</th>\n",
              "      <th>año</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2008</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a492ff90-03e5-4d65-95cf-b925acb84301')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a492ff90-03e5-4d65-95cf-b925acb84301 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a492ff90-03e5-4d65-95cf-b925acb84301');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a63f330d-4e21-4f65-8877-36b79ccfcdae\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a63f330d-4e21-4f65-8877-36b79ccfcdae')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a63f330d-4e21-4f65-8877-36b79ccfcdae button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    raise # Re-raise the exception\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"tiempo_id\",\n      \"properties\": {\n        \"dtype\": \"Int64\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"a\\u00f1o\",\n      \"properties\": {\n        \"dtype\": \"Int64\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2005,\n          2008,\n          2006\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-30T23:30:44.265+0000] {ipython-input-12-1925601096.py:68} INFO - Transformed Dim_Tiempo DataFrame info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20 entries, 0 to 19\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype\n",
            "---  ------     --------------  -----\n",
            " 0   tiempo_id  20 non-null     Int64\n",
            " 1   año        20 non-null     Int64\n",
            "dtypes: Int64(2)\n",
            "memory usage: 492.0 bytes\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-30T23:30:45.093+0000] {ipython-input-12-1925601096.py:81} INFO - Starting BigQuery load job for Dim_Tiempo: 58135285-f3ab-45cc-822c-2da2924047e6\n",
            "[2025-06-30T23:30:46.950+0000] {ipython-input-12-1925601096.py:84} INFO - DataFrame successfully loaded to etl-sistemas-sri.sri_presion_fiscal.Dim_Tiempo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51dc3a07"
      },
      "source": [
        "### Subtask:\n",
        "Extraer valores únicos de `Tipo_Presion` de la tabla de origen, generar una clave subrogada para `Dim_TipoPresion` y cargarla en la tabla de BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "e9ab18f0",
        "outputId": "5182b5bb-b1ed-4911-f4a2-1aa777e01d4e"
      },
      "source": [
        "# Assuming BigQuery client and PROJECT_ID, DATASET_ID are defined and credentials are set up from previous steps.\n",
        "# If running this cell independently, you would need to re-initialize the client and credentials here.\n",
        "# Ensure BigQuery client is initialized and authenticated before running this.\n",
        "\n",
        "# Define the source table ID\n",
        "# Use the PROJECT_ID that was successful in previous loading attempts\n",
        "SOURCE_PROJECT_ID = 'etl-sistemas-sri' # Use the project ID where the source data was loaded\n",
        "DATASET_ID = \"sri_presion_fiscal\" # BigQuery Dataset ID\n",
        "SOURCE_TABLE_ID = f\"{SOURCE_PROJECT_ID}.{DATASET_ID}.sri_presion_fiscal\" # Source table for dimension data\n",
        "\n",
        "# Define destination table ID for Dim_TipoPresion\n",
        "# Assuming dimension tables are in the same project/dataset for simplicity here\n",
        "# In a real scenario, destination PROJECT_ID might be different\n",
        "PROJECT_ID = SOURCE_PROJECT_ID # Use source project as destination for dimension tables for now\n",
        "table_id_dim_tipopresion = f\"{PROJECT_ID}.{DATASET_ID}.Dim_TipoPresion\"\n",
        "\n",
        "# Define schema for Dim_TipoPresion (assuming it was defined in a previous cell or is defined here)\n",
        "# If not defined, ensure the cell defining schema_dim_tipopresion is run first.\n",
        "schema_dim_tipopresion = [\n",
        "    bigquery.SchemaField(\"tipo_presion_id\", \"INT64\", mode=\"REQUIRED\"), # Primary Key\n",
        "    bigquery.SchemaField(\"tipo_presion_nombre\", \"STRING\", mode=\"REQUIRED\"), # Tax Pressure Type Name\n",
        "]\n",
        "\n",
        "\n",
        "# Write a SQL query to select distinct Tipo_Presion values\n",
        "query_tipopresion = f\"\"\"\n",
        "SELECT DISTINCT Tipo_Presion\n",
        "FROM `{SOURCE_TABLE_ID}`\n",
        "WHERE Tipo_Presion IS NOT NULL AND Tipo_Presion != 'Tipo_Presion' # Exclude header row if present in data\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query and load results into a DataFrame for Dim_TipoPresion\n",
        "try:\n",
        "    # --- Ensure BigQuery client is initialized and authenticated ---\n",
        "    # This is a placeholder for demonstration. In a real Airflow task,\n",
        "    # the client initialization would likely happen within the task's callable\n",
        "    # or rely on Airflow's configuration.\n",
        "    # Assuming 'client' object is available from a previous successful cell execution.\n",
        "    if 'client' not in locals() or client is None:\n",
        "         # Attempt to re-initialize client if not found (for interactive testing)\n",
        "         try:\n",
        "             # Assuming the credential file path is known and accessible\n",
        "             credential_filepath = \"/etl-sistemas-sri-cc64af72be76.json\" # Use the full path provided by the user or your actual path\n",
        "             credentials = service_account.Credentials.from_service_account_file(credential_filepath)\n",
        "             client = bigquery.Client(project=SOURCE_PROJECT_ID, credentials=credentials)\n",
        "             logger.info(\"BigQuery client re-initialized for Dim_TipoPresion ETL.\")\n",
        "         except Exception as e:\n",
        "             logger.error(f\"Could not re-initialize BigQuery client for Dim_TipoPresion ETL: {e}\")\n",
        "             raise # Cannot proceed without a client\n",
        "\n",
        "\n",
        "    df_tipopresion = client.query(query_tipopresion).to_dataframe()\n",
        "    logger.info(f\"Extracted {len(df_tipopresion)} unique Tipo_Presion values.\")\n",
        "    display(df_tipopresion.head())\n",
        "\n",
        "    # Add a tipo_presion_id column (surrogate key)\n",
        "    df_tipopresion['tipo_presion_id'] = df_tipopresion.reset_index().index + 1 # Simple 1-based integer ID\n",
        "\n",
        "    # Rename the Tipo_Presion column to match the schema\n",
        "    df_tipopresion.rename(columns={'Tipo_Presion': 'tipo_presion_nombre'}, inplace=True)\n",
        "\n",
        "    # Select only the required columns and ensure order\n",
        "    df_tipopresion = df_tipopresion[['tipo_presion_id', 'tipo_presion_nombre']]\n",
        "\n",
        "    logger.info(\"Transformed Dim_TipoPresion DataFrame head:\")\n",
        "    display(df_tipopresion.head())\n",
        "    logger.info(\"Transformed Dim_TipoPresion DataFrame info:\")\n",
        "    display(df_tipopresion.info())\n",
        "\n",
        "\n",
        "    # Load the Dim_TipoPresion DataFrame into the BigQuery table\n",
        "    job_config_tipopresion = bigquery.LoadJobConfig(\n",
        "        schema=schema_dim_tipopresion, # Use the defined schema\n",
        "        write_disposition=\"WRITE_TRUNCATE\", # Overwrite the table\n",
        "    )\n",
        "\n",
        "    job_tipopresion = client.load_table_from_dataframe(\n",
        "        df_tipopresion, table_id_dim_tipopresion, job_config=job_config_tipopresion\n",
        "    )\n",
        "\n",
        "    logger.info(f\"Starting BigQuery load job for Dim_TipoPresion: {job_tipopresion.job_id}\")\n",
        "    job_tipopresion.result() # Wait for the job to complete\n",
        "\n",
        "    logger.info(f\"DataFrame successfully loaded to {table_id_dim_tipopresion}\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error processing Dim_TipoPresion: {e}\")\n",
        "    raise # Re-raise the exception"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-30T23:31:32.089+0000] {ipython-input-13-2165624256.py:53} INFO - Extracted 4 unique Tipo_Presion values.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                              Tipo_Presion\n",
              "0                     Presión Fiscal Total\n",
              "1      Presión Tributaria Gobierno Central\n",
              "2  Presión Tributaria Gobierno Subnacional\n",
              "3     Contribuciones a la Seguridad Social"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f5ec51e1-f85e-4f7c-a7b9-c8007241eee2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tipo_Presion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Presión Tributaria Gobierno Central</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Presión Tributaria Gobierno Subnacional</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Contribuciones a la Seguridad Social</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5ec51e1-f85e-4f7c-a7b9-c8007241eee2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f5ec51e1-f85e-4f7c-a7b9-c8007241eee2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f5ec51e1-f85e-4f7c-a7b9-c8007241eee2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9329b3d2-44b1-4e61-8944-b04f74da1ee1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9329b3d2-44b1-4e61-8944-b04f74da1ee1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9329b3d2-44b1-4e61-8944-b04f74da1ee1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    raise # Re-raise the exception\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Tipo_Presion\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Presi\\u00f3n Tributaria Gobierno Central\",\n          \"Contribuciones a la Seguridad Social\",\n          \"Presi\\u00f3n Fiscal Total\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-30T23:31:32.099+0000] {ipython-input-13-2165624256.py:65} INFO - Transformed Dim_TipoPresion DataFrame head:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   tipo_presion_id                      tipo_presion_nombre\n",
              "0                1                     Presión Fiscal Total\n",
              "1                2      Presión Tributaria Gobierno Central\n",
              "2                3  Presión Tributaria Gobierno Subnacional\n",
              "3                4     Contribuciones a la Seguridad Social"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-689dc9e4-fbd7-4194-a4a0-ef7f1d78d2a7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tipo_presion_id</th>\n",
              "      <th>tipo_presion_nombre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Presión Fiscal Total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Presión Tributaria Gobierno Central</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Presión Tributaria Gobierno Subnacional</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Contribuciones a la Seguridad Social</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-689dc9e4-fbd7-4194-a4a0-ef7f1d78d2a7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-689dc9e4-fbd7-4194-a4a0-ef7f1d78d2a7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-689dc9e4-fbd7-4194-a4a0-ef7f1d78d2a7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1d6b5611-4510-4137-b515-589612151154\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1d6b5611-4510-4137-b515-589612151154')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1d6b5611-4510-4137-b515-589612151154 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    raise # Re-raise the exception\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"tipo_presion_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          4,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tipo_presion_nombre\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Presi\\u00f3n Tributaria Gobierno Central\",\n          \"Contribuciones a la Seguridad Social\",\n          \"Presi\\u00f3n Fiscal Total\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-30T23:31:32.111+0000] {ipython-input-13-2165624256.py:67} INFO - Transformed Dim_TipoPresion DataFrame info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4 entries, 0 to 3\n",
            "Data columns (total 2 columns):\n",
            " #   Column               Non-Null Count  Dtype \n",
            "---  ------               --------------  ----- \n",
            " 0   tipo_presion_id      4 non-null      int64 \n",
            " 1   tipo_presion_nombre  4 non-null      object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 196.0+ bytes\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-30T23:31:32.786+0000] {ipython-input-13-2165624256.py:81} INFO - Starting BigQuery load job for Dim_TipoPresion: 8c9a2850-9bbb-4857-b619-71f805fd3aaf\n",
            "[2025-06-30T23:31:34.274+0000] {ipython-input-13-2165624256.py:84} INFO - DataFrame successfully loaded to etl-sistemas-sri.sri_presion_fiscal.Dim_TipoPresion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35268d40"
      },
      "source": [
        "## ETL de Hechos (Fase 3)\n",
        "\n",
        "### Subtask:\n",
        "Definir el esquema para la tabla de hechos (por ejemplo, Fact_PresionFiscal), incluyendo claves foráneas a las tablas de dimensiones y métricas, y crear la tabla de hechos en BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a949cc6",
        "outputId": "f48e127c-0beb-4a72-b2e6-436350f0aec0"
      },
      "source": [
        "# Assuming BigQuery client is initialized and PROJECT_ID, DATASET_ID are defined\n",
        "\n",
        "# Define the schema for the Fact_PresionFiscal table\n",
        "# It includes foreign keys to dimension tables and the measure\n",
        "schema_fact = [\n",
        "    bigquery.SchemaField(\"id_geografia\", \"INT64\", mode=\"REQUIRED\"), # Foreign key to Dim_Geografia\n",
        "    bigquery.SchemaField(\"tiempo_id\", \"INT64\", mode=\"REQUIRED\"),   # Foreign key to Dim_Tiempo\n",
        "    bigquery.SchemaField(\"tipo_presion_id\", \"INT64\", mode=\"REQUIRED\"), # Foreign key to Dim_TipoPresion\n",
        "    bigquery.SchemaField(\"presion_fiscal\", \"FLOAT64\", mode=\"NULLABLE\"), # The measure (tax pressure percentage)\n",
        "]\n",
        "\n",
        "# Define the table ID for the Fact table\n",
        "# Using the unified PROJECT_ID and DATASET_ID\n",
        "table_id_fact = f\"{PROJECT_ID}.{DATASET_ID}.Fact_PresionFiscal\"\n",
        "\n",
        "# Assuming BigQuery client is initialized and authenticated from previous steps.\n",
        "# If running this cell independently, you would need to re-initialize the client and credentials here.\n",
        "# Ensure BigQuery client is initialized before running this.\n",
        "if 'client' not in locals() or client is None:\n",
        "    logger.error(\"BigQuery client is not initialized. Please ensure the client is initialized before creating the fact table.\")\n",
        "    # In a real DAG, this would be handled within a task's callable.\n",
        "    # For interactive testing, you might need to run the client initialization cell again.\n",
        "    raise ValueError(\"BigQuery client not initialized.\")\n",
        "\n",
        "\n",
        "# Create or get the Fact table in BigQuery\n",
        "def create_or_get_table(client, table_id, schema):\n",
        "    \"\"\"Creates a BigQuery table if it doesn't exist, or gets it if it does.\"\"\"\n",
        "    table_ref = bigquery.Table(table_id, schema=schema)\n",
        "    try:\n",
        "        table = client.create_table(table_ref, exists_ok=True)\n",
        "        logger.info(f\"Table '{table_id}' created or already exists.\")\n",
        "        return table\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error creating table '{table_id}': {e}\")\n",
        "        raise # Re-raise the exception\n",
        "\n",
        "table_fact = create_or_get_table(client, table_id_fact, schema_fact)\n",
        "\n",
        "logger.info(f\"Schema defined and Fact table '{table_id_fact}' created or verified in BigQuery.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-30T23:35:14.625+0000] {ipython-input-14-2321279528.py:32} INFO - Table 'etl-sistemas-sri.sri_presion_fiscal.Fact_PresionFiscal' created or already exists.\n",
            "[2025-06-30T23:35:14.627+0000] {ipython-input-14-2321279528.py:40} INFO - Schema defined and Fact table 'etl-sistemas-sri.sri_presion_fiscal.Fact_PresionFiscal' created or verified in BigQuery.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a504c3a"
      },
      "source": [
        "**Reasoning**:\n",
        "Extract transactional data from the source table, perform lookups with dimension tables to get surrogate keys, and prepare the data for loading into the fact table. This involves writing a SQL query to select the relevant columns from the source table, executing the query, creating a DataFrame, querying each dimension table to get the mapping between natural keys and surrogate keys, merging the source DataFrame with each dimension DataFrame using the natural keys, and selecting/renaming the columns to match the fact table schema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "id": "40462de3",
        "outputId": "39769c97-8047-4b00-a4fb-28bd46c003a2"
      },
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "import os\n",
        "import logging # Import logging\n",
        "import requests # Import requests for downloading\n",
        "\n",
        "# Configure logging (if not already configured)\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Assuming BigQuery client is initialized and PROJECT_ID, DATASET_ID are defined\n",
        "# If running this cell independently, you would need to re-initialize the client and credentials here.\n",
        "# Ensure BigQuery client is initialized and authenticated before running this.\n",
        "\n",
        "# --- Re-initialize BigQuery client and load credentials for this step if needed ---\n",
        "# This is a placeholder for demonstration. In a real Airflow task,\n",
        "# the client initialization would likely happen within the task's callable\n",
        "# or rely on Airflow's configuration.\n",
        "# Assuming the credential file path is known and accessible\n",
        "# Use the absolute path that was successful in previous attempts or is known\n",
        "credential_filepath = \"/etl-sistemas-sri-cc64af72be76.json\" # Use the full path provided by the user or your actual path\n",
        "SOURCE_PROJECT_ID = 'etl-sistemas-sri' # Use the project ID where the source data was loaded\n",
        "DATASET_ID = \"sri_presion_fiscal\" # BigQuery Dataset ID\n",
        "\n",
        "try:\n",
        "    # Load credentials explicitly\n",
        "    credentials = service_account.Credentials.from_service_account_file(credential_filepath)\n",
        "    logger.info(\"Google Cloud credentials re-loaded successfully for fact ETL.\")\n",
        "\n",
        "    # Initialize BigQuery client with the source project and loaded credentials\n",
        "    client = bigquery.Client(project=SOURCE_PROJECT_ID, credentials=credentials)\n",
        "    logger.info(f\"BigQuery client re-initialized for source project: {SOURCE_PROJECT_ID}.\")\n",
        "\n",
        "    # Define dimension table IDs in the target project/dataset (assuming it's the same as source for simplicity here)\n",
        "    # In a real scenario, destination PROJECT_ID might be different\n",
        "    PROJECT_ID = SOURCE_PROJECT_ID # Use source project as destination for dimension tables for now\n",
        "    table_id_dim_geografia = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Geografia\"\n",
        "    table_id_dim_tiempo = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Tiempo\"\n",
        "    table_id_dim_tipopresion = f\"{PROJECT_ID}.{DATASET_ID}.Dim_TipoPresion\"\n",
        "\n",
        "except FileNotFoundError:\n",
        "    logger.error(f\"Error: Credential file '{credential_filepath}' not found. Cannot proceed with fact ETL.\")\n",
        "    # Cannot proceed with BigQuery operations without credentials\n",
        "    raise # Re-raise the exception\n",
        "except Exception as e:\n",
        "    logger.error(f\"An unexpected error occurred during credential loading or client initialization for fact ETL: {e}\")\n",
        "    # Cannot proceed with BigQuery operations\n",
        "    raise # Re-raise any other exception\n",
        "# --- End of re-initialization ---\n",
        "\n",
        "\n",
        "# Define the source table ID using the correct project and dataset\n",
        "SOURCE_TABLE_ID = f\"{SOURCE_PROJECT_ID}.{DATASET_ID}.sri_presion_fiscal\" # Source table for dimension data\n",
        "\n",
        "\n",
        "# --- Extract Source Data from BigQuery ---\n",
        "# In a real Airflow DAG, this might be a separate task or part of a larger Python function.\n",
        "# For this interactive example, we extract directly here.\n",
        "query_source_data = f\"\"\"\n",
        "SELECT\n",
        "    Region,\n",
        "    `País`,\n",
        "    `Año`,\n",
        "    `%_Presion`,\n",
        "    Tipo_Presion\n",
        "FROM `{SOURCE_TABLE_ID}`\n",
        "WHERE Region IS NOT NULL AND Region != 'Region'\n",
        "  AND `País` IS NOT NULL AND `País` != 'País'\n",
        "  AND `Año` IS NOT NULL\n",
        "  AND `%_Presion` IS NOT NULL AND `%_Presion` != '%_Presion'\n",
        "  AND Tipo_Presion IS NOT NULL AND Tipo_Presion != 'Tipo_Presion'\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    df_source_data = client.query(query_source_data).to_dataframe()\n",
        "    logger.info(f\"\\nExtracted {len(df_source_data)} rows from source table for fact processing.\")\n",
        "    display(df_source_data.head())\n",
        "    display(df_source_data.info())\n",
        "\n",
        "    # Ensure data types are correct before merging\n",
        "    # Convert 'Año' to integer (Int64 to handle potential NaNs)\n",
        "    df_source_data['Año'] = pd.to_numeric(df_source_data['Año'], errors='coerce').astype('Int64')\n",
        "    # Convert '%_Presion' to float\n",
        "    df_source_data['%_Presion'] = pd.to_numeric(df_source_data['%_Presion'], errors='coerce')\n",
        "    logger.info(\"Source data types cleaned for merging.\")\n",
        "    display(df_source_data.info())\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error extracting source data from BigQuery: {e}\")\n",
        "    # Cannot proceed with fact transformation without source data\n",
        "    raise # Re-raise the exception\n",
        "# --- End of Extract Source Data ---\n",
        "\n",
        "\n",
        "# --- Extract Dimension Data from BigQuery for Lookups ---\n",
        "# These queries assume the dimension tables exist and are populated.\n",
        "\n",
        "# Query Dim_Geografia\n",
        "query_dim_geografia = f\"\"\"\n",
        "SELECT id_geografia, pais, region\n",
        "FROM `{table_id_dim_geografia}`\n",
        "\"\"\"\n",
        "try:\n",
        "    df_dim_geografia = client.query(query_dim_geografia).to_dataframe()\n",
        "    logger.info(f\"\\nExtracted {len(df_dim_geografia)} rows from Dim_Geografia for lookup.\")\n",
        "    display(df_dim_geografia.head())\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error extracting Dim_Geografia data from BigQuery: {e}\")\n",
        "    # Cannot proceed with fact transformation without dimension data\n",
        "    raise # Re-raise the exception\n",
        "\n",
        "# Query Dim_Tiempo\n",
        "query_dim_tiempo = f\"\"\"\n",
        "SELECT tiempo_id, año\n",
        "FROM `{table_id_dim_tiempo}`\n",
        "\"\"\"\n",
        "try:\n",
        "    df_dim_tiempo = client.query(query_dim_tiempo).to_dataframe()\n",
        "    logger.info(f\"\\nExtracted {len(df_dim_tiempo)} rows from Dim_Tiempo for lookup.\")\n",
        "    display(df_dim_tiempo.head())\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error extracting Dim_Tiempo data from BigQuery: {e}\")\n",
        "    # Cannot proceed with fact transformation without dimension data\n",
        "    raise # Re-raise the exception\n",
        "\n",
        "# Query Dim_TipoPresion\n",
        "query_dim_tipopresion = f\"\"\"\n",
        "SELECT tipo_presion_id, tipo_presion_nombre\n",
        "FROM `{table_id_dim_tipopresion}`\n",
        "\"\"\"\n",
        "try:\n",
        "    df_dim_tipopresion = client.query(query_dim_tipopresion).to_dataframe()\n",
        "    logger.info(f\"\\nExtracted {len(df_dim_tipopresion)} rows from Dim_TipoPresion for lookup.\")\n",
        "    display(df_dim_tipopresion.head())\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error extracting Dim_TipoPresion data from BigQuery: {e}\")\n",
        "    # Cannot proceed with fact transformation without dimension data\n",
        "    raise # Re-raise the exception\n",
        "# --- End of Extract Dimension Data ---\n",
        "\n",
        "\n",
        "# --- Perform Lookups and Prepare Fact Data ---\n",
        "\n",
        "# Start with the extracted source data\n",
        "df_fact_staging = df_source_data.copy()\n",
        "\n",
        "# Merge with Dim_Geografia to get id_geografia\n",
        "# Ensure column names match between source data and dimension table for merging\n",
        "df_fact_staging = pd.merge(df_fact_staging, df_dim_geografia,\n",
        "                           left_on=['País', 'Region'], right_on=['pais', 'region'], how='left')\n",
        "\n",
        "# Merge with Dim_Tiempo to get tiempo_id\n",
        "df_fact_staging = pd.merge(df_fact_staging, df_dim_tiempo,\n",
        "                           left_on='Año', right_on='año', how='left')\n",
        "\n",
        "# Merge with Dim_TipoPresion to get tipo_presion_id\n",
        "df_fact_staging = pd.merge(df_fact_staging, df_dim_tipopresion,\n",
        "                           left_on='Tipo_Presion', right_on='tipo_presion_nombre', how='left')\n",
        "\n",
        "\n",
        "# Check for unmatched rows (where lookups resulted in NaN IDs)\n",
        "# This indicates issues with data consistency between source and dimensions\n",
        "unmatched_rows_geografia = df_fact_staging[df_fact_staging['id_geografia'].isnull()]\n",
        "if not unmatched_rows_geografia.empty:\n",
        "    logger.warning(f\"\\nWarning: {len(unmatched_rows_geografia)} rows did not match in Dim_Geografia during lookup.\")\n",
        "    display(unmatched_rows_geografia[['País', 'Region', 'id_geografia']].head())\n",
        "\n",
        "unmatched_rows_tiempo = df_fact_staging[df_fact_staging['tiempo_id'].isnull()]\n",
        "if not unmatched_rows_tiempo.empty:\n",
        "    logger.warning(f\"\\nWarning: {len(unmatched_rows_tiempo)} rows did not match in Dim_Tiempo during lookup.\")\n",
        "    display(unmatched_rows_tiempo[['Año', 'tiempo_id']].head())\n",
        "\n",
        "unmatched_rows_tipopresion = df_fact_staging[df_fact_staging['tipo_presion_id'].isnull()]\n",
        "if not unmatched_rows_tipopresion.empty:\n",
        "    logger.warning(f\"\\nWarning: {len(unmatched_rows_tipopresion)} rows did not match in Dim_TipoPresion during lookup.\")\n",
        "    display(unmatched_rows_tipopresion[['Tipo_Presion', 'tipo_presion_id']].head())\n",
        "\n",
        "\n",
        "# Select and rename columns to match the fact table schema\n",
        "# Ensure all required surrogate key columns are present after merges\n",
        "df_fact = df_fact_staging[['id_geografia', 'tiempo_id', 'tipo_presion_id', '%_Presion']].copy() # Copy to avoid SettingWithCopyWarning\n",
        "\n",
        "# Rename the measure column\n",
        "df_fact.rename(columns={'%_Presion': 'presion_fiscal'}, inplace=True)\n",
        "\n",
        "# Ensure surrogate key columns are of type Int64 (nullable integer)\n",
        "# This is important as merges can introduce floats for integer columns with NaNs\n",
        "for col_id in ['id_geografia', 'tiempo_id', 'tipo_presion_id']:\n",
        "    if col_id in df_fact.columns:\n",
        "        # Convert to Int64 to allow for potential NaN values from left merge\n",
        "        # Use errors='coerce' just in case, though merge should handle this if types match\n",
        "        df_fact[col_id] = pd.to_numeric(df_fact[col_id], errors='coerce').astype('Int64')\n",
        "\n",
        "\n",
        "logger.info(\"\\nTransformed Fact DataFrame head (with surrogate keys):\")\n",
        "display(df_fact.head())\n",
        "logger.info(\"\\nTransformed Fact DataFrame info:\")\n",
        "display(df_fact.info())\n",
        "\n",
        "# --- End of Lookups and Prepare Fact Data ---\n",
        "\n",
        "# Note: The actual loading to BigQuery will be a separate task in the final DAG.\n",
        "# This code block focuses on the data transformation and lookup part.\n",
        "logger.info(\"Fact data transformation and lookup complete. Ready for loading to BigQuery.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-30T23:35:34.025+0000] {ipython-input-15-2622158817.py:29} INFO - Google Cloud credentials re-loaded successfully for fact ETL.\n",
            "[2025-06-30T23:35:34.029+0000] {ipython-input-15-2622158817.py:33} INFO - BigQuery client re-initialized for source project: etl-sistemas-sri.\n",
            "[2025-06-30T23:35:34.494+0000] {ipython-input-15-2622158817.py:91} ERROR - Error extracting source data from BigQuery: 400 No matching signature for operator != for argument types: FLOAT64, STRING\n",
            "  Signature: T1 != T1\n",
            "    Unable to find common supertype for templated argument <T1>\n",
            "      Input types for <T1>: {DOUBLE, STRING} at [12:35]; reason: invalidQuery, location: query, message: No matching signature for operator != for argument types: FLOAT64, STRING\n",
            "  Signature: T1 != T1\n",
            "    Unable to find common supertype for templated argument <T1>\n",
            "      Input types for <T1>: {DOUBLE, STRING} at [12:35]\n",
            "\n",
            "Location: US\n",
            "Job ID: 53552210-a0a6-4f22-ad26-0c30593b8d11\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BadRequest",
          "evalue": "400 No matching signature for operator != for argument types: FLOAT64, STRING\n  Signature: T1 != T1\n    Unable to find common supertype for templated argument <T1>\n      Input types for <T1>: {DOUBLE, STRING} at [12:35]; reason: invalidQuery, location: query, message: No matching signature for operator != for argument types: FLOAT64, STRING\n  Signature: T1 != T1\n    Unable to find common supertype for templated argument <T1>\n      Input types for <T1>: {DOUBLE, STRING} at [12:35]\n\nLocation: US\nJob ID: 53552210-a0a6-4f22-ad26-0c30593b8d11\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-15-2622158817.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mdf_source_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_source_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nExtracted {len(df_source_data)} rows from source table for fact processing.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_source_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mto_dataframe\u001b[0;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, max_results, geography_as_object, bool_dtype, int_dtype, float_dtype, string_dtype, date_dtype, datetime_dtype, time_dtype, timestamp_dtype, range_date_dtype, range_datetime_dtype, range_timestamp_dtype)\u001b[0m\n\u001b[1;32m   2072\u001b[0m                 \u001b[0;34m:\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mshapely\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mlibrary\u001b[0m \u001b[0mcannot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mimported\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2073\u001b[0m         \"\"\"\n\u001b[0;32m-> 2074\u001b[0;31m         \u001b[0mquery_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait_for_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2075\u001b[0m         return query_result.to_dataframe(\n\u001b[1;32m   2076\u001b[0m             \u001b[0mbqstorage_client\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbqstorage_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/_tqdm_helpers.py\u001b[0m in \u001b[0;36mwait_for_query\u001b[0;34m(query_job, progress_bar_type, max_results)\u001b[0m\n\u001b[1;32m    105\u001b[0m     )\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprogress_bar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mquery_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, page_size, max_results, retry, timeout, start_index, job_retry)\u001b[0m\n\u001b[1;32m   1694\u001b[0m                 \u001b[0;31m# Since is_job_done() calls jobs.getQueryResults, which is a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m                 \u001b[0;31m# long-running API, don't delay the next request at all.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1696\u001b[0;31m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_job_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1697\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mis_job_done\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1643\u001b[0m                         \u001b[0;31m# `job_retry` predicate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m                         \u001b[0mrestart_query_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mjob_failed_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m                         \u001b[0;31m# Make sure that the _query_results are cached so we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequest\u001b[0m: 400 No matching signature for operator != for argument types: FLOAT64, STRING\n  Signature: T1 != T1\n    Unable to find common supertype for templated argument <T1>\n      Input types for <T1>: {DOUBLE, STRING} at [12:35]; reason: invalidQuery, location: query, message: No matching signature for operator != for argument types: FLOAT64, STRING\n  Signature: T1 != T1\n    Unable to find common supertype for templated argument <T1>\n      Input types for <T1>: {DOUBLE, STRING} at [12:35]\n\nLocation: US\nJob ID: 53552210-a0a6-4f22-ad26-0c30593b8d11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c03151f6"
      },
      "source": [
        "### Subtask:\n",
        "Load the prepared fact DataFrame into the `Fact_PresionFiscal` table in BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "890fa50d",
        "outputId": "c549cb5d-83e8-4581-8f74-c91ff8111f46"
      },
      "source": [
        "# Assuming BigQuery client is initialized and PROJECT_ID, DATASET_ID, and df_fact are available\n",
        "\n",
        "# Define the table ID for the Fact table\n",
        "# Using the unified PROJECT_ID and DATASET_ID\n",
        "table_id_fact = f\"{PROJECT_ID}.{DATASET_ID}.Fact_PresionFiscal\"\n",
        "\n",
        "# Define the schema for the Fact_PresionFiscal table (should match the updated schema)\n",
        "schema_fact = [\n",
        "    bigquery.SchemaField(\"id_geografia\", \"INT64\", mode=\"REQUIRED\"), # Foreign key to Dim_Geografia\n",
        "    bigquery.SchemaField(\"tiempo_id\", \"INT64\", mode=\"REQUIRED\"),   # Foreign key to Dim_Tiempo\n",
        "    bigquery.SchemaField(\"tipo_presion_id\", \"INT64\", mode=\"REQUIRED\"), # Foreign key to Dim_TipoPresion\n",
        "    bigquery.SchemaField(\"presion_fiscal\", \"FLOAT64\", mode=\"NULLABLE\"), # The measure (tax pressure percentage)\n",
        "]\n",
        "\n",
        "# Assuming BigQuery client is initialized and authenticated from previous steps.\n",
        "# If running this cell independently, you would need to re-initialize the client and credentials here.\n",
        "# Ensure BigQuery client is initialized before running this.\n",
        "if 'client' not in locals() or client is None:\n",
        "    logger.error(\"BigQuery client is not initialized. Please ensure the client is initialized before loading the fact table.\")\n",
        "    # In a real DAG, this would be handled within a task's callable.\n",
        "    # For interactive testing, you might need to run the client initialization cell again.\n",
        "    raise ValueError(\"BigQuery client not initialized.\")\n",
        "\n",
        "# Assuming df_fact DataFrame is available from the previous cell execution\n",
        "if 'df_fact' not in locals() or df_fact.empty:\n",
        "    logger.error(\"df_fact DataFrame not found or is empty. Cannot load to BigQuery.\")\n",
        "    raise ValueError(\"Fact DataFrame not available or empty.\")\n",
        "\n",
        "\n",
        "# Load the Fact DataFrame into the BigQuery table\n",
        "try:\n",
        "    job_config_fact = bigquery.LoadJobConfig(\n",
        "        schema=schema_fact, # Use the defined schema for the fact table\n",
        "        write_disposition=\"WRITE_TRUNCATE\", # Overwrite the table\n",
        "    )\n",
        "\n",
        "    job_fact = client.load_table_from_dataframe(\n",
        "        df_fact, table_id_fact, job_config=job_config_fact\n",
        "    )\n",
        "\n",
        "    logger.info(f\"\\nStarting BigQuery load job for Fact_PresionFiscal: {job_fact.job_id}\")\n",
        "    job_fact.result() # Wait for the job to complete\n",
        "\n",
        "    logger.info(f\"DataFrame successfully loaded to {table_id_fact}\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error loading Fact_PresionFiscal to BigQuery: {e}\")\n",
        "    raise # Re-raise the exception"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-30T23:40:36.527+0000] {ipython-input-27-634704269.py:26} ERROR - df_fact DataFrame not found or is empty. Cannot load to BigQuery.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Fact DataFrame not available or empty.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-27-634704269.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'df_fact'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdf_fact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"df_fact DataFrame not found or is empty. Cannot load to BigQuery.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fact DataFrame not available or empty.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Fact DataFrame not available or empty."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35bdd9f5"
      },
      "source": [
        "## Definir Tareas y Dependencias del DAG (Fase 4)\n",
        "\n",
        "Ahora que tenemos las funciones Python para cada paso del ETL (descarga, ETL de dimensiones y ETL de hechos), definiremos las tareas de Airflow que ejecutarán estas funciones utilizando `PythonOperator`. También definiremos las tareas de inicio y fin usando `EmptyOperator` (reemplazo de `DummyOperator`) y estableceremos las dependencias entre todas las tareas según el esquema del plan."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1467cd68",
        "outputId": "7191d63e-6f64-42c5-c5c3-1433a8d068c3"
      },
      "source": [
        "# Assuming the DAG structure and the Python functions (download_csv_from_url,\n",
        "# process_dim_geografia, process_dim_tiempo, process_dim_tipopresion, process_fact)\n",
        "# are defined in the same DAG file or imported correctly.\n",
        "# For the final DAG file, all necessary imports and function definitions\n",
        "# should be at the top level or within imported modules.\n",
        "\n",
        "# Re-defining necessary variables and schemas for clarity within this step's context\n",
        "# In the final DAG file, these would typically be defined earlier.\n",
        "\n",
        "# BigQuery parameters (using the project and dataset from previous successful steps)\n",
        "PROJECT_ID = \"etl-sistemas-sri\"\n",
        "DATASET_ID = \"sri_presion_fiscal\"\n",
        "SOURCE_TABLE_ID = f\"{PROJECT_ID}.{DATASET_ID}.sri_presion_fiscal\"\n",
        "\n",
        "# Table IDs for dimension and fact tables\n",
        "table_id_dim_geografia = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Geografia\"\n",
        "table_id_dim_tiempo = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Tiempo\"\n",
        "table_id_dim_tipopresion = f\"{PROJECT_ID}.{DATASET_ID}.Dim_TipoPresion\"\n",
        "table_id_fact = f\"{PROJECT_ID}.{DATASET_ID}.Fact_PresionFiscal\"\n",
        "\n",
        "# Local filename for the downloaded CSV\n",
        "local_filename = \"SRI_Presion_Fiscal.csv\"\n",
        "\n",
        "\n",
        "# Assuming the Airflow DAG object 'dag' is already defined using 'with DAG(...) as dag:'\n",
        "# and the 'start' and 'end' tasks (EmptyOperator) are also defined.\n",
        "\n",
        "# --- Define ETL Tasks using PythonOperator ---\n",
        "\n",
        "# Task for ETL of Dim_Geografia\n",
        "# This task will execute a Python function that queries the source table,\n",
        "# transforms data, generates surrogate keys, and loads into Dim_Geografia.\n",
        "# The Python function 'process_dim_geografia' needs to be defined elsewhere in this DAG file.\n",
        "# (We will generate the function definition in a subsequent step for the full DAG file)\n",
        "process_dim_geografia_task = PythonOperator(\n",
        "    task_id='process_dim_geografia',\n",
        "    python_callable=process_dim_geografia, # Placeholder for the actual function name\n",
        "    # op_kwargs could be used to pass table IDs if the function needs them\n",
        "    # op_kwargs={'source_table': SOURCE_TABLE_ID, 'target_table': table_id_dim_geografia, ...},\n",
        "    # dag=dag, # Inherited from the 'with DAG' context\n",
        ")\n",
        "\n",
        "# Task for ETL of Dim_Tiempo\n",
        "# Executes a Python function for Dim_Tiempo ETL.\n",
        "# The Python function 'process_dim_tiempo' needs to be defined.\n",
        "process_dim_tiempo_task = PythonOperator(\n",
        "    task_id='process_dim_tiempo',\n",
        "    python_callable=process_dim_tiempo, # Placeholder for the actual function name\n",
        "    # op_kwargs={'source_table': SOURCE_TABLE_ID, 'target_table': table_id_dim_tiempo, ...},\n",
        "    # dag=dag,\n",
        ")\n",
        "\n",
        "# Task for ETL of Dim_TipoPresion\n",
        "# Executes a Python function for Dim_TipoPresion ETL.\n",
        "# The Python function 'process_dim_tipopresion' needs to be defined.\n",
        "process_dim_tipopresion_task = PythonOperator(\n",
        "    task_id='process_dim_tipopresion',\n",
        "    python_callable=process_dim_tipopresion, # Placeholder for the actual function name\n",
        "    # op_kwargs={'source_table': SOURCE_TABLE_ID, 'target_table': table_id_dim_tipopresion, ...},\n",
        "    # dag=dag,\n",
        ")\n",
        "\n",
        "# Task for ETL of Fact_PresionFiscal\n",
        "# Executes a Python function for Fact_PresionFiscal ETL.\n",
        "# This function will need access to dimension tables for lookups.\n",
        "# The Python function 'process_fact_presionfiscal' needs to be defined.\n",
        "process_fact_presionfiscal_task = PythonOperator(\n",
        "    task_id='process_fact_presionfiscal',\n",
        "    python_callable=process_fact_presionfiscal, # Placeholder for the actual function name\n",
        "    # op_kwargs could be used to pass table IDs\n",
        "    # op_kwargs={\n",
        "    #     'source_table': SOURCE_TABLE_ID,\n",
        "    #     'target_fact_table': table_id_fact,\n",
        "    #     'dim_geografia_table': table_id_dim_geografia,\n",
        "    #     'dim_tiempo_table': table_id_dim_tiempo,\n",
        "    #     'dim_tipopresion_table': table_id_dim_tipopresion,\n",
        "    #     ...\n",
        "    # },\n",
        "    # dag=dag,\n",
        ")\n",
        "\n",
        "# --- Define Dependencies (Fase 4) ---\n",
        "# According to the plan: inicio >> [cargar_dim_geografia, cargar_dim_tiempo, cargar_dim_tipopresion] >> cargar_fact_presionfiscal >> fin\n",
        "\n",
        "# The download task should run before any dimension ETL tasks\n",
        "# (assuming the dimension ETL tasks read from the downloaded CSV, although in our Colab example\n",
        "# we read from BigQuery after initial load. For the DAG, let's stick to the plan's flow:\n",
        "# start >> download_csv_task >> [process_dim_geografia_task, process_dim_tiempo_task, process_dim_tipopresion_task] >> process_fact_presionfiscal_task >> end\n",
        "\n",
        "start >> download_csv_task\n",
        "\n",
        "download_csv_task >> [process_dim_geografia_task, process_dim_tiempo_task, process_dim_tipopresion_task]\n",
        "\n",
        "process_dim_geografia_task >> process_fact_presionfiscal_task\n",
        "process_dim_tiempo_task >> process_fact_presionfiscal_task\n",
        "process_dim_tipopresion_task >> process_fact_presionfiscal_task\n",
        "\n",
        "process_fact_presionfiscal_task >> end\n",
        "\n",
        "# Alternative way to define dependencies for the parallel dimension tasks:\n",
        "# dimension_tasks = [process_dim_geografia_task, process_dim_tiempo_task, process_dim_tipopresion_task]\n",
        "# download_csv_task >> dimension_tasks\n",
        "# dimension_tasks >> process_fact_presionfiscal_task\n",
        "# process_fact_presionfiscal_task >> end"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Task(EmptyOperator): end>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2d2a86b"
      },
      "source": [
        "## Definición de las Funciones ETL para el DAG\n",
        "\n",
        "Aquí definiremos las funciones Python que serán ejecutadas por los `PythonOperator` en nuestro DAG. Cada función contendrá la lógica para una etapa específica del ETL: poblar las tablas de dimensiones y poblar la tabla de hechos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0af0a8f"
      },
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "import os\n",
        "import logging # Import logging\n",
        "import requests # Import requests for downloading\n",
        "\n",
        "# Configure logging (if not already configured)\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define BigQuery parameters (using the project and dataset from previous successful steps)\n",
        "PROJECT_ID = \"etl-sistemas-sri\"\n",
        "DATASET_ID = \"sri_presion_fiscal\"\n",
        "SOURCE_TABLE_ID = f\"{PROJECT_ID}.{DATASET_ID}.sri_presion_fiscal\"\n",
        "\n",
        "# Table IDs for dimension and fact tables\n",
        "table_id_dim_geografia = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Geografia\"\n",
        "table_id_dim_tiempo = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Tiempo\"\n",
        "table_id_dim_tipopresion = f\"{PROJECT_ID}.{DATASET_ID}.Dim_TipoPresion\"\n",
        "table_id_fact = f\"{PROJECT_ID}.{DATASET_ID}.Fact_PresionFiscal\"\n",
        "\n",
        "# Local filename for the downloaded CSV (used by download task)\n",
        "local_filename = \"SRI_Presion_Fiscal.csv\"\n",
        "\n",
        "# Credential file path (using the path provided by the user)\n",
        "credential_filepath = \"/etl-sistemas-sri-cc64af72be76.json\"\n",
        "\n",
        "\n",
        "# --- Define Python Functions for ETL Tasks ---\n",
        "\n",
        "def process_dim_geografia():\n",
        "    \"\"\"\n",
        "    Extracts unique Country-Region combinations from the source table,\n",
        "    generates surrogate keys, and loads into Dim_Geografia in BigQuery.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting process_dim_geografia task.\")\n",
        "\n",
        "    # Initialize BigQuery client within the task function\n",
        "    try:\n",
        "        credentials = service_account.Credentials.from_service_account_file(credential_filepath)\n",
        "        client = bigquery.Client(project=PROJECT_ID, credentials=credentials)\n",
        "        logger.info(\"BigQuery client initialized for Dim_Geografia ETL.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error initializing BigQuery client for Dim_Geografia ETL: {e}\")\n",
        "        raise # Re-raise the exception\n",
        "\n",
        "    # Define schema for Dim_Geografia (needs to be defined here or accessible)\n",
        "    schema_dim_geografia = [\n",
        "        bigquery.SchemaField(\"id_geografia\", \"INT64\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"pais\", \"STRING\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"region\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    ]\n",
        "\n",
        "    # Write a SQL query to select distinct Country and Region combinations\n",
        "    query_geografia = f\"\"\"\n",
        "    SELECT DISTINCT `País`, Region\n",
        "    FROM `{SOURCE_TABLE_ID}`\n",
        "    WHERE `País` IS NOT NULL AND `País` != 'País' -- Exclude header row if present\n",
        "      AND Region IS NOT NULL AND Region != 'Region' -- Exclude header row if present\n",
        "    \"\"\"\n",
        "\n",
        "    # Execute the query and load results into a DataFrame\n",
        "    try:\n",
        "        df_geografia = client.query(query_geografia).to_dataframe()\n",
        "        logger.info(f\"Extracted {len(df_geografia)} unique Country-Region combinations for Dim_Geografia.\")\n",
        "\n",
        "        # Add an id_geografia column (surrogate key)\n",
        "        df_geografia = df_geografia.sort_values(by=['País', 'Region']).reset_index(drop=True)\n",
        "        df_geografia['id_geografia'] = df_geografia.index + 1\n",
        "\n",
        "        # Rename columns to match the schema\n",
        "        df_geografia.rename(columns={'País': 'pais', 'Region': 'region'}, inplace=True)\n",
        "\n",
        "        # Select only the required columns and ensure order\n",
        "        df_geografia = df_geografia[['id_geografia', 'pais', 'region']]\n",
        "\n",
        "        logger.info(\"Transformed Dim_Geografia DataFrame head:\")\n",
        "        display(df_geografia.head())\n",
        "\n",
        "\n",
        "        # Load the DataFrame into the BigQuery table\n",
        "        job_config_geografia = bigquery.LoadJobConfig(\n",
        "            schema=schema_dim_geografia,\n",
        "            write_disposition=\"WRITE_TRUNCATE\",\n",
        "        )\n",
        "\n",
        "        job_geografia = client.load_table_from_dataframe(\n",
        "            df_geografia, table_id_dim_geografia, job_config=job_config_geografia\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Starting BigQuery load job for Dim_Geografia: {job_geografia.job_id}\")\n",
        "        job_geografia.result()\n",
        "        logger.info(f\"DataFrame successfully loaded to {table_id_dim_geografia}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing Dim_Geografia: {e}\")\n",
        "        raise # Re-raise the exception\n",
        "    logger.info(\"Finished process_dim_geografia task.\")\n",
        "\n",
        "\n",
        "def process_dim_tiempo():\n",
        "    \"\"\"\n",
        "    Extracts unique years from the source table,\n",
        "    generates surrogate keys, and loads into Dim_Tiempo in BigQuery.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting process_dim_tiempo task.\")\n",
        "\n",
        "    # Initialize BigQuery client within the task function\n",
        "    try:\n",
        "        credentials = service_account.Credentials.from_service_account_file(credential_filepath)\n",
        "        client = bigquery.Client(project=PROJECT_ID, credentials=credentials)\n",
        "        logger.info(\"BigQuery client initialized for Dim_Tiempo ETL.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error initializing BigQuery client for Dim_Tiempo ETL: {e}\")\n",
        "        raise # Re-raise the exception\n",
        "\n",
        "    # Define schema for Dim_Tiempo (needs to be defined here or accessible)\n",
        "    schema_dim_tiempo = [\n",
        "        bigquery.SchemaField(\"tiempo_id\", \"INT64\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"año\", \"INT64\", mode=\"REQUIRED\"),\n",
        "    ]\n",
        "\n",
        "    # Write a SQL query to select distinct years\n",
        "    query_tiempo = f\"\"\"\n",
        "    SELECT DISTINCT `Año`\n",
        "    FROM `{SOURCE_TABLE_ID}`\n",
        "    WHERE `Año` IS NOT NULL\n",
        "    \"\"\"\n",
        "\n",
        "    # Execute the query and load results into a DataFrame\n",
        "    try:\n",
        "        df_tiempo = client.query(query_tiempo).to_dataframe()\n",
        "        logger.info(f\"Extracted {len(df_tiempo)} unique years for Dim_Tiempo.\")\n",
        "\n",
        "        # Add a tiempo_id column (surrogate key)\n",
        "        df_tiempo['tiempo_id'] = df_tiempo['Año'].rank(method='dense').astype('Int64')\n",
        "\n",
        "        # Rename the year column to match the schema\n",
        "        df_tiempo.rename(columns={'Año': 'año'}, inplace=True)\n",
        "\n",
        "        # Select only the required columns and ensure order\n",
        "        df_tiempo = df_tiempo[['tiempo_id', 'año']]\n",
        "\n",
        "        logger.info(\"Transformed Dim_Tiempo DataFrame head:\")\n",
        "        display(df_tiempo.head())\n",
        "\n",
        "        # Load the DataFrame into the BigQuery table\n",
        "        job_config_tiempo = bigquery.LoadJobConfig(\n",
        "            schema=schema_dim_tiempo,\n",
        "            write_disposition=\"WRITE_TRUNCATE\",\n",
        "        )\n",
        "\n",
        "        job_tiempo = client.load_table_from_dataframe(\n",
        "            df_tiempo, table_id_dim_tiempo, job_config=job_config_tiempo\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Starting BigQuery load job for Dim_Tiempo: {job_tiempo.job_id}\")\n",
        "        job_tiempo.result()\n",
        "        logger.info(f\"DataFrame successfully loaded to {table_id_dim_tiempo}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing Dim_Tiempo: {e}\")\n",
        "        raise # Re-raise the exception\n",
        "    logger.info(\"Finished process_dim_tiempo task.\")\n",
        "\n",
        "\n",
        "def process_dim_tipopresion():\n",
        "    \"\"\"\n",
        "    Extracts unique Tipo_Presion values from the source table,\n",
        "    generates surrogate keys, and loads into Dim_TipoPresion in BigQuery.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting process_dim_tipopresion task.\")\n",
        "\n",
        "    # Initialize BigQuery client within the task function\n",
        "    try:\n",
        "        credentials = service_account.Credentials.from_service_account_file(credential_filepath)\n",
        "        client = bigquery.Client(project=PROJECT_ID, credentials=credentials)\n",
        "        logger.info(\"BigQuery client initialized for Dim_TipoPresion ETL.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error initializing BigQuery client for Dim_TipoPresion ETL: {e}\")\n",
        "        raise # Re-raise the exception\n",
        "\n",
        "    # Define schema for Dim_TipoPresion (needs to be defined here or accessible)\n",
        "    schema_dim_tipopresion = [\n",
        "        bigquery.SchemaField(\"tipo_presion_id\", \"INT64\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"tipo_presion_nombre\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    ]\n",
        "\n",
        "    # Write a SQL query to select distinct Tipo_Presion values\n",
        "    query_tipopresion = f\"\"\"\n",
        "    SELECT DISTINCT Tipo_Presion\n",
        "    FROM `{SOURCE_TABLE_ID}`\n",
        "    WHERE Tipo_Presion IS NOT NULL AND Tipo_Presion != 'Tipo_Presion' -- Exclude header row if present\n",
        "    \"\"\"\n",
        "\n",
        "    # Execute the query and load results into a DataFrame\n",
        "    try:\n",
        "        df_tipopresion = client.query(query_tipopresion).to_dataframe()\n",
        "        logger.info(f\"Extracted {len(df_tipopresion)} unique Tipo_Presion values for Dim_TipoPresion.\")\n",
        "\n",
        "        # Add a tipo_presion_id column (surrogate key)\n",
        "        df_tipopresion['tipo_presion_id'] = df_tipopresion.reset_index().index + 1\n",
        "\n",
        "        # Rename the Tipo_Presion column to match the schema\n",
        "        df_tipopresion.rename(columns={'Tipo_Presion': 'tipo_presion_nombre'}, inplace=True)\n",
        "\n",
        "        # Select only the required columns and ensure order\n",
        "        df_tipopresion = df_tipopresion[['tipo_presion_id', 'tipo_presion_nombre']]\n",
        "\n",
        "        logger.info(\"Transformed Dim_TipoPresion DataFrame head:\")\n",
        "        display(df_tipopresion.head())\n",
        "\n",
        "        # Load the DataFrame into the BigQuery table\n",
        "        job_config_tipopresion = bigquery.LoadJobConfig(\n",
        "            schema=schema_dim_tipopresion,\n",
        "            write_disposition=\"WRITE_TRUNCATE\",\n",
        "        )\n",
        "\n",
        "        job_tipopresion = client.load_table_from_dataframe(\n",
        "            df_tipopresion, table_id_dim_tipopresion, job_config=job_config_tipopresion\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Starting BigQuery load job for Dim_TipoPresion: {job_tipopresion.job_id}\")\n",
        "        job_tipopresion.result()\n",
        "        logger.info(f\"DataFrame successfully loaded to {table_id_dim_tipopresion}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing Dim_TipoPresion: {e}\")\n",
        "        raise # Re-raise the exception\n",
        "    logger.info(\"Finished process_dim_tipopresion task.\")\n",
        "\n",
        "\n",
        "def process_fact_presionfiscal():\n",
        "    \"\"\"\n",
        "    Extracts source data, performs lookups with dimension tables,\n",
        "    and loads into Fact_PresionFiscal in BigQuery.\n",
        "    Assumes dimension tables are already populated.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting process_fact_presionfiscal task.\")\n",
        "\n",
        "    # Initialize BigQuery client within the task function\n",
        "    try:\n",
        "        credentials = service_account.Credentials.from_service_account_file(credential_filepath)\n",
        "        client = bigquery.Client(project=PROJECT_ID, credentials=credentials)\n",
        "        logger.info(\"BigQuery client initialized for Fact ETL.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error initializing BigQuery client for Fact ETL: {e}\")\n",
        "        raise # Re-raise the exception\n",
        "\n",
        "    # Define schema for Fact_PresionFiscal (needs to be defined here or accessible)\n",
        "    schema_fact = [\n",
        "        bigquery.SchemaField(\"id_geografia\", \"INT64\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"tiempo_id\", \"INT64\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"tipo_presion_id\", \"INT64\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"presion_fiscal\", \"FLOAT64\", mode=\"NULLABLE\"),\n",
        "    ]\n",
        "\n",
        "    # --- Extract Source Data from BigQuery ---\n",
        "    query_source_data = f\"\"\"\n",
        "    SELECT\n",
        "        Region,\n",
        "        `País`,\n",
        "        `Año`,\n",
        "        `%_Presion`,\n",
        "        Tipo_Presion\n",
        "    FROM `{SOURCE_TABLE_ID}`\n",
        "    WHERE Region IS NOT NULL\n",
        "      AND `País` IS NOT NULL\n",
        "      AND `Año` IS NOT NULL\n",
        "      AND `%_Presion` IS NOT NULL\n",
        "      AND Tipo_Presion IS NOT NULL\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        df_source_data = client.query(query_source_data).to_dataframe()\n",
        "        logger.info(f\"Extracted {len(df_source_data)} rows from source table for fact processing.\")\n",
        "\n",
        "        # Ensure data types are correct before merging\n",
        "        df_source_data['Año'] = pd.to_numeric(df_source_data['Año'], errors='coerce').astype('Int64')\n",
        "        df_source_data['%_Presion'] = pd.to_numeric(df_source_data['%_Presion'], errors='coerce')\n",
        "        logger.info(\"Source data types cleaned for merging.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting source data for Fact ETL: {e}\")\n",
        "        raise # Re-raise the exception\n",
        "\n",
        "    # --- Extract Dimension Data from BigQuery for Lookups ---\n",
        "    # These queries assume the dimension tables exist and are populated.\n",
        "\n",
        "    # Query Dim_Geografia\n",
        "    query_dim_geografia = f\"\"\"\n",
        "    SELECT id_geografia, pais, region\n",
        "    FROM `{table_id_dim_geografia}`\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df_dim_geografia = client.query(query_dim_geografia).to_dataframe()\n",
        "        logger.info(f\"Extracted {len(df_dim_geografia)} rows from Dim_Geografia for lookup.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting Dim_Geografia data for Fact ETL: {e}\")\n",
        "        raise # Re-raise the exception\n",
        "\n",
        "    # Query Dim_Tiempo\n",
        "    query_dim_tiempo = f\"\"\"\n",
        "    SELECT tiempo_id, año\n",
        "    FROM `{table_id_dim_tiempo}`\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df_dim_tiempo = client.query(query_dim_tiempo).to_dataframe()\n",
        "        logger.info(f\"Extracted {len(df_dim_tiempo)} rows from Dim_Tiempo for lookup.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting Dim_Tiempo data for Fact ETL: {e}\")\n",
        "        raise # Re-raise the exception\n",
        "\n",
        "    # Query Dim_TipoPresion\n",
        "    query_dim_tipopresion = f\"\"\"\n",
        "    SELECT tipo_presion_id, tipo_presion_nombre\n",
        "    FROM `{table_id_dim_tipopresion}`\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df_dim_tipopresion = client.query(query_dim_tipopresion).to_dataframe()\n",
        "        logger.info(f\"Extracted {len(df_dim_tipopresion)} rows from Dim_TipoPresion for lookup.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting Dim_TipoPresion data for Fact ETL: {e}\")\n",
        "        raise # Re-raise the exception\n",
        "    # --- End of Extract Dimension Data ---\n",
        "\n",
        "\n",
        "    # --- Perform Lookups and Prepare Fact Data ---\n",
        "    df_fact_staging = df_source_data.copy()\n",
        "\n",
        "    df_fact_staging = pd.merge(df_fact_staging, df_dim_geografia,\n",
        "                               left_on=['País', 'Region'], right_on=['pais', 'region'], how='left')\n",
        "\n",
        "    df_fact_staging = pd.merge(df_fact_staging, df_dim_tiempo,\n",
        "                               left_on='Año', right_on='año', how='left')\n",
        "\n",
        "    df_fact_staging = pd.merge(df_fact_staging, df_dim_tipopresion,\n",
        "                               left_on='Tipo_Presion', right_on='tipo_presion_nombre', how='left')\n",
        "\n",
        "    # Check for unmatched rows (optional, for debugging)\n",
        "    unmatched_rows_geografia = df_fact_staging[df_fact_staging['id_geografia'].isnull()]\n",
        "    if not unmatched_rows_geografia.empty:\n",
        "        logger.warning(f\"Warning: {len(unmatched_rows_geografia)} rows did not match in Dim_Geografia during lookup.\")\n",
        "\n",
        "    unmatched_rows_tiempo = df_fact_staging[df_fact_staging['tiempo_id'].isnull()]\n",
        "    if not unmatched_rows_tiempo.empty:\n",
        "        logger.warning(f\"Warning: {len(unmatched_rows_tiempo)} rows did not match in Dim_Tiempo during lookup.\")\n",
        "\n",
        "    unmatched_rows_tipopresion = df_fact_staging[df_fact_staging['tipo_presion_id'].isnull()]\n",
        "    if not unmatched_rows_tipopresion.empty:\n",
        "        logger.warning(f\"Warning: {len(unmatched_rows_tipopresion)} rows did not match in Dim_TipoPresion during lookup.\")\n",
        "\n",
        "\n",
        "    # Select and rename columns to match the fact table schema\n",
        "    df_fact = df_fact_staging[['id_geografia', 'tiempo_id', 'tipo_presion_id', '%_Presion']].copy()\n",
        "    df_fact.rename(columns={'%_Presion': 'presion_fiscal'}, inplace=True)\n",
        "\n",
        "    # Ensure surrogate key columns are of type Int64 (nullable integer)\n",
        "    for col_id in ['id_geografia', 'tiempo_id', 'tipo_presion_id']:\n",
        "        if col_id in df_fact.columns:\n",
        "            df_fact[col_id] = pd.to_numeric(df_fact[col_id], errors='coerce').astype('Int64')\n",
        "\n",
        "    logger.info(\"Transformed Fact DataFrame head:\")\n",
        "    display(df_fact.head())\n",
        "    logger.info(\"Transformed Fact DataFrame info:\")\n",
        "    display(df_fact.info())\n",
        "\n",
        "    # --- Load to BigQuery ---\n",
        "    job_config_fact = bigquery.LoadJobConfig(\n",
        "        schema=schema_fact,\n",
        "        write_disposition=\"WRITE_TRUNCATE\",\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        job_fact = client.load_table_from_dataframe(\n",
        "            df_fact, table_id_fact, job_config=job_config_fact\n",
        "        )\n",
        "        logger.info(f\"Starting BigQuery load job for Fact_PresionFiscal: {job_fact.job_id}\")\n",
        "        job_fact.result()\n",
        "        logger.info(f\"DataFrame successfully loaded to {table_id_fact}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading Fact_PresionFiscal to BigQuery: {e}\")\n",
        "        raise # Re-raise the exception\n",
        "\n",
        "    logger.info(\"Finished process_fact_presionfiscal task.\")\n",
        "\n",
        "# Note: The download_csv_from_url function is assumed to be defined earlier in the DAG file.\n",
        "# The DAG structure with tasks and dependencies will be defined after this cell."
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "948eccef"
      },
      "source": [
        "## Archivo Completo del DAG de Airflow (`presionfiscal_etl_dag.py`)\n",
        "\n",
        "Este es el código completo para tu DAG de Airflow. Copia este código y guárdalo como un archivo llamado `presionfiscal_etl_dag.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88a4ae3e"
      },
      "source": [
        "# Import necessary modules from Apache Airflow and other libraries\n",
        "from airflow import DAG # DAG object to define the workflow\n",
        "from datetime import datetime, timedelta # For specifying start_date and retry_delay\n",
        "import pandas as pd # For data transformation using DataFrames\n",
        "from airflow.operators.python import PythonOperator # Operator to run Python functions\n",
        "from airflow.operators.empty import EmptyOperator # For start and end tasks (replaces DummyOperator)\n",
        "from google.cloud import bigquery # For interacting with Google BigQuery API\n",
        "import os # For accessing environment variables\n",
        "from google.oauth2 import service_account # For loading Google Cloud service account credentials\n",
        "import requests # For downloading the CSV file\n",
        "import logging # For logging\n",
        "\n",
        "# Configure logging for the DAG\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- Configuration Variables ---\n",
        "# Define BigQuery parameters (using the project and dataset from previous successful steps)\n",
        "PROJECT_ID = \"etl-sistemas-sri\"\n",
        "DATASET_ID = \"sri_presion_fiscal\"\n",
        "SOURCE_TABLE_ID = f\"{PROJECT_ID}.{DATASET_ID}.sri_presion_fiscal\"\n",
        "\n",
        "# Table IDs for dimension and fact tables\n",
        "table_id_dim_geografia = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Geografia\"\n",
        "table_id_dim_tiempo = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Tiempo\"\n",
        "table_id_dim_tipopresion = f\"{PROJECT_ID}.{DATASET_ID}.Dim_TipoPresion\"\n",
        "table_id_fact = f\"{PROJECT_ID}.{DATASET_ID}.Fact_PresionFiscal\"\n",
        "\n",
        "# Local filename for the downloaded CSV (used by download task)\n",
        "local_filename = \"SRI_Presion_Fiscal.csv\"\n",
        "\n",
        "# URL for the source CSV file\n",
        "csv_url = \"https://www.sri.gob.ec/o/sri-portlet-biblioteca-alfresco-internet/descargar/7e45627e-1f7e-4e21-ae59-d520634fc63f/SRI_Presion_Fiscal.csv\"\n",
        "\n",
        "# Credential file path (using the path provided by the user)\n",
        "# IMPORTANT: For production Airflow environments, consider using Airflow Connections\n",
        "# for Google Cloud authentication instead of relying on a file path in the DAG file.\n",
        "# Ensure this file is accessible to your Airflow worker(s).\n",
        "credential_filepath = \"/etl-sistemas-sri-cc64af72be76.json\" # Use the full path provided by the user\n",
        "\n",
        "\n",
        "# --- Define Python Functions for ETL Tasks ---\n",
        "\n",
        "def download_csv_from_url(csv_url, local_filename):\n",
        "    \"\"\"Downloads a CSV file from a given URL and saves it locally.\"\"\"\n",
        "    logger.info(f\"Attempting to download file from: {csv_url}\")\n",
        "    try:\n",
        "        response = requests.get(csv_url)\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "        with open(local_filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        logger.info(f\"Archivo '{local_filename}' descargado exitosamente.\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error al descargar el archivo: {e}\")\n",
        "        # Raise an exception to fail the task in Airflow\n",
        "        raise ValueError(f\"Download task failed: {e}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An unexpected error occurred during download: {e}\")\n",
        "        raise ValueError(f\"Download task failed unexpectedly: {e}\")\n",
        "\n",
        "\n",
        "def process_dim_geografia(credential_filepath=credential_filepath):\n",
        "    \"\"\"\n",
        "    Extracts unique Country-Region combinations from the source table in BigQuery,\n",
        "    generates surrogate keys, and loads into Dim_Geografia in BigQuery.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting process_dim_geografia task.\")\n",
        "\n",
        "    # Initialize BigQuery client within the task function using provided credentials\n",
        "    try:\n",
        "        credentials = service_account.Credentials.from_service_account_file(credential_filepath)\n",
        "        client = bigquery.Client(project=PROJECT_ID, credentials=credentials)\n",
        "        logger.info(\"BigQuery client initialized for Dim_Geografia ETL.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error initializing BigQuery client for Dim_Geografia ETL: {e}\")\n",
        "        raise # Re-raise the exception\n",
        "\n",
        "    # Define schema for Dim_Geografia\n",
        "    schema_dim_geografia = [\n",
        "        bigquery.SchemaField(\"id_geografia\", \"INT64\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"pais\", \"STRING\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"region\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    ]\n",
        "\n",
        "    # Write a SQL query to select distinct Country and Region combinations\n",
        "    # Assuming header rows are handled during initial load to source table\n",
        "    query_geografia = f\"\"\"\n",
        "    SELECT DISTINCT `País`, Region\n",
        "    FROM `{SOURCE_TABLE_ID}`\n",
        "    WHERE `País` IS NOT NULL\n",
        "      AND Region IS NOT NULL\n",
        "    \"\"\"\n",
        "\n",
        "    # Execute the query and load results into a DataFrame for Dim_Geografia\n",
        "    try:\n",
        "        df_geografia = client.query(query_geografia).to_dataframe()\n",
        "        logger.info(f\"Extracted {len(df_geografia)} unique Country-Region combinations for Dim_Geografia.\")\n",
        "\n",
        "        # Add an id_geografia column (surrogate key)\n",
        "        df_geografia = df_geografia.sort_values(by=['País', 'Region']).reset_index(drop=True)\n",
        "        df_geografia['id_geografia'] = df_geografia.index + 1\n",
        "\n",
        "        # Rename columns to match the schema\n",
        "        df_geografia.rename(columns={'País': 'pais', 'Region': 'region'}, inplace=True)\n",
        "\n",
        "        # Select only the required columns and ensure order\n",
        "        df_geografia = df_geografia[['id_geografia', 'pais', 'region']]\n",
        "\n",
        "        # Load the DataFrame into the BigQuery table\n",
        "        job_config_geografia = bigquery.LoadJobConfig(\n",
        "            schema=schema_dim_geografia,\n",
        "            write_disposition=\"WRITE_TRUNCATE\", # Overwrite the table\n",
        "        )\n",
        "\n",
        "        job_geografia = client.load_table_from_dataframe(\n",
        "            df_geografia, table_id_dim_geografia, job_config=job_config_geografia\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Starting BigQuery load job for Dim_Geografia: {job_geografia.job_id}\")\n",
        "        job_geografia.result()\n",
        "        logger.info(f\"DataFrame successfully loaded to {table_id_dim_geografia}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing Dim_Geografia: {e}\")\n",
        "        raise # Re-raise the exception\n",
        "    logger.info(\"Finished process_dim_geografia task.\")\n",
        "\n",
        "\n",
        "def process_dim_tiempo(credential_filepath=credential_filepath):\n",
        "    \"\"\"\n",
        "    Extracts unique years from the source table in BigQuery,\n",
        "    generates surrogate keys, and loads into Dim_Tiempo in BigQuery.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting process_dim_tiempo task.\")\n",
        "\n",
        "    # Initialize BigQuery client within the task function using provided credentials\n",
        "    try:\n",
        "        credentials = service_account.Credentials.from_service_account_file(credential_filepath)\n",
        "        client = bigquery.Client(project=PROJECT_ID, credentials=credentials)\n",
        "        logger.info(\"BigQuery client initialized for Dim_Tiempo ETL.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error initializing BigQuery client for Dim_Tiempo ETL: {e}\")\n",
        "        raise # Re-raise the exception\n",
        "\n",
        "    # Define schema for Dim_Tiempo\n",
        "    schema_dim_tiempo = [\n",
        "        bigquery.SchemaField(\"tiempo_id\", \"INT64\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"año\", \"INT64\", mode=\"REQUIRED\"),\n",
        "    ]\n",
        "\n",
        "    # Write a SQL query to select distinct years\n",
        "    query_tiempo = f\"\"\"\n",
        "    SELECT DISTINCT `Año`\n",
        "    FROM `{SOURCE_TABLE_ID}`\n",
        "    WHERE `Año` IS NOT NULL\n",
        "    \"\"\"\n",
        "\n",
        "    # Execute the query and load results into a DataFrame\n",
        "    try:\n",
        "        df_tiempo = client.query(query_tiempo).to_dataframe()\n",
        "        logger.info(f\"Extracted {len(df_tiempo)} unique years for Dim_Tiempo.\")\n",
        "\n",
        "        # Add a tiempo_id column (surrogate key)\n",
        "        df_tiempo['tiempo_id'] = df_tiempo['Año'].rank(method='dense').astype('Int64')\n",
        "\n",
        "        # Rename the year column to match the schema\n",
        "        df_tiempo.rename(columns={'Año': 'año'}, inplace=True)\n",
        "\n",
        "        # Select only the required columns and ensure order\n",
        "        df_tiempo = df_tiempo[['tiempo_id', 'año']]\n",
        "\n",
        "        # Load the DataFrame into the BigQuery table\n",
        "        job_config_tiempo = bigquery.LoadJobConfig(\n",
        "            schema=schema_dim_tiempo,\n",
        "            write_disposition=\"WRITE_TRUNCATE\",\n",
        "        )\n",
        "\n",
        "        job_tiempo = client.load_table_from_dataframe(\n",
        "            df_tiempo, table_id_dim_tiempo, job_config=job_config_tiempo\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Starting BigQuery load job for Dim_Tiempo: {job_tiempo.job_id}\")\n",
        "        job_tiempo.result()\n",
        "        logger.info(f\"DataFrame successfully loaded to {table_id_dim_tiempo}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing Dim_Tiempo: {e}\")\n",
        "        raise # Re-raise the exception\n",
        "    logger.info(\"Finished process_dim_tiempo task.\")\n",
        "\n",
        "\n",
        "def process_dim_tipopresion(credential_filepath=credential_filepath):\n",
        "    \"\"\"\n",
        "    Extracts unique Tipo_Presion values from the source table in BigQuery,\n",
        "    generates surrogate keys, and loads into Dim_TipoPresion in BigQuery.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting process_dim_tipopresion task.\")\n",
        "\n",
        "    # Initialize BigQuery client within the task function using provided credentials\n",
        "    try:\n",
        "        credentials = service_account.Credentials.from_service_account_file(credential_filepath)\n",
        "        client = bigquery.Client(project=PROJECT_ID, credentials=credentials)\n",
        "        logger.info(\"BigQuery client initialized for Dim_TipoPresion ETL.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error initializing BigQuery client for Dim_TipoPresion ETL: {e}\")\n",
        "        raise # Re-raise the exception\n",
        "\n",
        "    # Define schema for Dim_TipoPresion\n",
        "    schema_dim_tipopresion = [\n",
        "        bigquery.SchemaField(\"tipo_presion_id\", \"INT64\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"tipo_presion_nombre\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    ]\n",
        "\n",
        "    # Write a SQL query to select distinct Tipo_Presion values\n",
        "    # Assuming header rows are handled during initial load to source table\n",
        "    query_tipopresion = f\"\"\"\n",
        "    SELECT DISTINCT Tipo_Presion\n",
        "    FROM `{SOURCE_TABLE_ID}`\n",
        "    WHERE Tipo_Presion IS NOT NULL\n",
        "    \"\"\"\n",
        "\n",
        "    # Execute the query and load results into a DataFrame\n",
        "    try:\n",
        "        df_tipopresion = client.query(query_tipopresion).to_dataframe()\n",
        "        logger.info(f\"Extracted {len(df_tipopresion)} unique Tipo_Presion values for Dim_TipoPresion.\")\n",
        "\n",
        "        # Add a tipo_presion_id column (surrogate key)\n",
        "        df_tipopresion['tipo_presion_id'] = df_tipopresion.reset_index().index + 1\n",
        "\n",
        "        # Rename the Tipo_Presion column to match the schema\n",
        "        df_tipopresion.rename(columns={'Tipo_Presion': 'tipo_presion_nombre'}, inplace=True)\n",
        "\n",
        "        # Select only the required columns and ensure order\n",
        "        df_tipopresion = df_tipopresion[['tipo_presion_id', 'tipo_presion_nombre']]\n",
        "\n",
        "        # Load the DataFrame into the BigQuery table\n",
        "        job_config_tipopresion = bigquery.LoadJobConfig(\n",
        "            schema=schema_dim_tipopresion,\n",
        "            write_disposition=\"WRITE_TRUNCATE\",\n",
        "        )\n",
        "\n",
        "        job_tipopresion = client.load_table_from_dataframe(\n",
        "            df_tipopresion, table_id_dim_tipopresion, job_config=job_config_tipopresion\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Starting BigQuery load job for Dim_TipoPresion: {job_tipopresion.job_id}\")\n",
        "        job_tipopresion.result()\n",
        "        logger.info(f\"DataFrame successfully loaded to {table_id_dim_tipopresion}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing Dim_TipoPresion: {e}\")\n",
        "        raise # Re-raise the exception\n",
        "    logger.info(\"Finished process_dim_tipopresion task.\")\n",
        "\n",
        "\n",
        "def process_fact_presionfiscal(credential_filepath=credential_filepath):\n",
        "    \"\"\"\n",
        "    Extracts source data from BigQuery, performs lookups with dimension tables in BigQuery,\n",
        "    and loads into Fact_PresionFiscal in BigQuery.\n",
        "    Assumes dimension tables are already populated.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting process_fact_presionfiscal task.\")\n",
        "\n",
        "    # Initialize BigQuery client within the task function using provided credentials\n",
        "    try:\n",
        "        credentials = service_account.Credentials.from_service_account_file(credential_filepath)\n",
        "        client = bigquery.Client(project=PROJECT_ID, credentials=credentials)\n",
        "        logger.info(\"BigQuery client initialized for Fact ETL.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error initializing BigQuery client for Fact ETL: {e}\")\n",
        "        raise # Re-raise the exception\n",
        "\n",
        "    # Define schema for Fact_PresionFiscal\n",
        "    schema_fact = [\n",
        "        bigquery.SchemaField(\"id_geografia\", \"INT64\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"tiempo_id\", \"INT64\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"tipo_presion_id\", \"INT64\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"presion_fiscal\", \"FLOAT64\", mode=\"NULLABLE\"),\n",
        "    ]\n",
        "\n",
        "    # Define dimension table IDs\n",
        "    table_id_dim_geografia_fact = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Geografia\"\n",
        "    table_id_dim_tiempo_fact = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Tiempo\"\n",
        "    table_id_dim_tipopresion_fact = f\"{PROJECT_ID}.{DATASET_ID}.Dim_TipoPresion\"\n",
        "\n",
        "    # --- Extract Source Data from BigQuery ---\n",
        "    # Assuming header rows are handled during initial load to source table\n",
        "    query_source_data = f\"\"\"\n",
        "    SELECT\n",
        "        Region,\n",
        "        `País`,\n",
        "        `Año`,\n",
        "        `%_Presion`,\n",
        "        Tipo_Presion\n",
        "    FROM `{SOURCE_TABLE_ID}`\n",
        "    WHERE Region IS NOT NULL\n",
        "      AND `País` IS NOT NULL\n",
        "      AND `Año` IS NOT NULL\n",
        "      AND `%_Presion` IS NOT NULL\n",
        "      AND Tipo_Presion IS NOT NULL\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        df_source_data = client.query(query_source_data).to_dataframe()\n",
        "        logger.info(f\"Extracted {len(df_source_data)} rows from source table for fact processing.\")\n",
        "\n",
        "        # Ensure data types are correct before merging\n",
        "        df_source_data['Año'] = pd.to_numeric(df_source_data['Año'], errors='coerce').astype('Int64')\n",
        "        df_source_data['%_Presion'] = pd.to_numeric(df_source_data['%_Presion'], errors='coerce')\n",
        "        logger.info(\"Source data types cleaned for merging.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting source data for Fact ETL: {e}\")\n",
        "        raise # Re-raise the exception\n",
        "\n",
        "    # --- Extract Dimension Data from BigQuery for Lookups ---\n",
        "    # These queries assume the dimension tables exist and are populated.\n",
        "\n",
        "    # Query Dim_Geografia\n",
        "    query_dim_geografia = f\"\"\"\n",
        "    SELECT id_geografia, pais, region\n",
        "    FROM `{table_id_dim_geografia_fact}`\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df_dim_geografia = client.query(query_dim_geografia).to_dataframe()\n",
        "        logger.info(f\"Extracted {len(df_dim_geografia)} rows from Dim_Geografia for lookup.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting Dim_Geografia data for Fact ETL: {e}\")\n",
        "        raise # Re-raise the exception\n",
        "\n",
        "    # Query Dim_Tiempo\n",
        "    query_dim_tiempo = f\"\"\"\n",
        "    SELECT tiempo_id, año\n",
        "    FROM `{table_id_dim_tiempo_fact}`\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df_dim_tiempo = client.query(query_dim_tiempo).to_dataframe()\n",
        "        logger.info(f\"Extracted {len(df_dim_tiempo)} rows from Dim_Tiempo for lookup.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting Dim_Tiempo data for Fact ETL: {e}\")\n",
        "        raise # Re-raise the exception\n",
        "\n",
        "    # Query Dim_TipoPresion\n",
        "    query_dim_tipopresion = f\"\"\"\n",
        "    SELECT tipo_presion_id, tipo_presion_nombre\n",
        "    FROM `{table_id_dim_tipopresion_fact}`\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df_dim_tipopresion = client.query(query_dim_tipopresion).to_dataframe()\n",
        "        logger.info(f\"Extracted {len(df_dim_tipopresion)} rows from Dim_TipoPresion for lookup.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting Dim_TipoPresion data for Fact ETL: {e}\")\n",
        "        raise # Re-raise the exception\n",
        "    # --- End of Extract Dimension Data ---\n",
        "\n",
        "\n",
        "    # --- Perform Lookups and Prepare Fact Data ---\n",
        "    df_fact_staging = df_source_data.copy()\n",
        "\n",
        "    df_fact_staging = pd.merge(df_fact_staging, df_dim_geografia,\n",
        "                               left_on=['País', 'Region'], right_on=['pais', 'region'], how='left')\n",
        "\n",
        "    df_fact_staging = pd.merge(df_fact_staging, df_dim_tiempo,\n",
        "                               left_on='Año', right_on='año', how='left')\n",
        "\n",
        "    df_fact_staging = pd.merge(df_fact_staging, df_dim_tipopresion,\n",
        "                               left_on='Tipo_Presion', right_on='tipo_presion_nombre', how='left')\n",
        "\n",
        "    # Check for unmatched rows (optional, for debugging)\n",
        "    unmatched_rows_geografia = df_fact_staging[df_fact_staging['id_geografia'].isnull()]\n",
        "    if not unmatched_rows_geografia.empty:\n",
        "        logger.warning(f\"Warning: {len(unmatched_rows_geografia)} rows did not match in Dim_Geografia during lookup.\")\n",
        "\n",
        "    unmatched_rows_tiempo = df_fact_staging[df_fact_staging['tiempo_id'].isnull()]\n",
        "    if not unmatched_rows_tiempo.empty:\n",
        "        logger.warning(f\"Warning: {len(unmatched_rows_tiempo)} rows did not match in Dim_Tiempo during lookup.\")\n",
        "\n",
        "    unmatched_rows_tipopresion = df_fact_staging[df_fact_staging['tipo_presion_id'].isnull()]\n",
        "    if not unmatched_rows_tipopresion.empty:\n",
        "        logger.warning(f\"Warning: {len(unmatched_rows_tipopresion)} rows did not match in Dim_TipoPresion during lookup.\")\n",
        "\n",
        "\n",
        "    # Select and rename columns to match the fact table schema\n",
        "    df_fact = df_fact_staging[['id_geografia', 'tiempo_id', 'tipo_presion_id', '%_Presion']].copy()\n",
        "    df_fact.rename(columns={'%_Presion': 'presion_fiscal'}, inplace=True)\n",
        "\n",
        "    # Ensure surrogate key columns are of type Int64 (nullable integer)\n",
        "    for col_id in ['id_geografia', 'tiempo_id', 'tipo_presion_id']:\n",
        "        if col_id in df_fact.columns:\n",
        "            df_fact[col_id] = pd.to_numeric(df_fact[col_id], errors='coerce').astype('Int64')\n",
        "\n",
        "    logger.info(\"Transformed Fact DataFrame head:\")\n",
        "    # display(df_fact.head()) # Commenting out display in functions for cleaner DAG logs\n",
        "    logger.info(\"Transformed Fact DataFrame info:\")\n",
        "    # display(df_fact.info()) # Commenting out display in functions for cleaner DAG logs\n",
        "\n",
        "\n",
        "    # --- Load to BigQuery ---\n",
        "    job_config_fact = bigquery.LoadJobConfig(\n",
        "        schema=schema_fact,\n",
        "        write_disposition=\"WRITE_TRUNCATE\",\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        job_fact = client.load_table_from_dataframe(\n",
        "            df_fact, table_id_fact, job_config=job_config_fact\n",
        "        )\n",
        "        logger.info(f\"Starting BigQuery load job for Fact_PresionFiscal: {job_fact.job_id}\")\n",
        "        job_fact.result()\n",
        "        logger.info(f\"DataFrame successfully loaded to {table_id_fact}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading Fact_PresionFiscal to BigQuery: {e}\")\n",
        "        raise # Re-raise the exception\n",
        "\n",
        "    logger.info(\"Finished process_fact_presionfiscal task.\")\n",
        "\n",
        "# Note: The download_csv_from_url function is assumed to be defined earlier in the DAG file.\n",
        "# The DAG structure with tasks and dependencies will be defined after this cell.\n",
        "\n",
        "# Define default arguments for the DAG (Fase 1)\n",
        "# These arguments are passed to all tasks unless overridden\n",
        "default_args = {\n",
        "    'owner': 'airflow', # The owner of the DAG\n",
        "    'depends_on_past': False, # Set to true if you want tasks to wait for the previous run to succeed\n",
        "    'start_date': datetime(2024, 6, 1), # The date when the DAG starts being scheduled (Fase 1)\n",
        "    'email_on_failure': False, # Set to true to receive emails on task failure (Fase 1)\n",
        "    'email_on_retry': False, # Set to true to receive emails on task retry (Fase 1)\n",
        "    'retries': 1, # Number of times to retry a task on failure (Fase 1)\n",
        "    'retry_delay': timedelta(minutes=5), # Time to wait between retries (Fase 1)\n",
        "}\n",
        "\n",
        "# Define the DAG object (Fase 1)\n",
        "# 'presionfiscal_etl' is the unique DAG ID as specified\n",
        "with DAG(\n",
        "    'presionfiscal_etl', # dag_id as specified\n",
        "    default_args=default_args, # Apply the default arguments\n",
        "    description='ETL pipeline to download, transform, and load SRI tax pressure data to BigQuery', # A brief description of the DAG\n",
        "    schedule=None, # Set the schedule interval to None for manual runs (Fase 1)\n",
        "    catchup=False, # Set to False as specified (Fase 1)\n",
        "    max_active_runs=1, # Set max_active_runs as specified (Fase 1)\n",
        "    tags=['etl', 'bigquery', 'sri', 'presion_fiscal'], # Tags to help categorize and filter DAGs\n",
        ") as dag:\n",
        "\n",
        "    # Define the start task using EmptyOperator (Fase 4)\n",
        "    start = EmptyOperator(\n",
        "        task_id='start',\n",
        "    )\n",
        "\n",
        "    # Instantiate the PythonOperator for the download task\n",
        "    # Executes the 'download_csv_from_url' function\n",
        "    download_csv_task = PythonOperator(\n",
        "        task_id='download_source_csv', # Unique identifier for the task\n",
        "        python_callable=download_csv_from_url, # The Python function to execute\n",
        "        op_kwargs={'csv_url': csv_url, 'local_filename': local_filename}, # Keyword arguments to pass to the function\n",
        "    )\n",
        "\n",
        "    # Task for ETL of Dim_Geografia\n",
        "    process_dim_geografia_task = PythonOperator(\n",
        "        task_id='process_dim_geografia',\n",
        "        python_callable=process_dim_geografia,\n",
        "    )\n",
        "\n",
        "    # Task for ETL of Dim_Tiempo\n",
        "    process_dim_tiempo_task = PythonOperator(\n",
        "        task_id='process_dim_tiempo',\n",
        "        python_callable=process_dim_tiempo,\n",
        "    )\n",
        "\n",
        "    # Task for ETL of Dim_TipoPresion\n",
        "    process_dim_tipopresion_task = PythonOperator(\n",
        "        task_id='process_dim_tipopresion',\n",
        "        python_callable=process_dim_tipopresion,\n",
        "    )\n",
        "\n",
        "    # Task for ETL of Fact_PresionFiscal\n",
        "    process_fact_presionfiscal_task = PythonOperator(\n",
        "        task_id='process_fact_presionfiscal',\n",
        "        python_callable=process_fact_presionfiscal,\n",
        "    )\n",
        "\n",
        "    # Define the end task using EmptyOperator (Fase 4)\n",
        "    end = EmptyOperator(\n",
        "        task_id='end',\n",
        "    )\n",
        "\n",
        "    # --- Define Dependencies (Fase 4) ---\n",
        "    start >> download_csv_task\n",
        "\n",
        "    # Dimension ETL tasks run in parallel after download\n",
        "    download_csv_task >> [process_dim_geografia_task, process_dim_tiempo_task, process_dim_tipopresion_task]\n",
        "\n",
        "    # Fact ETL task runs after all dimension ETL tasks are complete\n",
        "    [process_dim_geografia_task, process_dim_tiempo_task, process_dim_tipopresion_task] >> process_fact_presionfiscal_task\n",
        "\n",
        "    # End task runs after Fact ETL is complete\n",
        "    process_fact_presionfiscal_task >> end"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8448dbb8"
      },
      "source": [
        "# Task\n",
        "Document the successful steps of the ETL process and Airflow DAG definition in Spanish, organizing the information by phases and providing clear explanations and code comments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48972cb1"
      },
      "source": [
        "## Revisar el historial y el cuaderno\n",
        "\n",
        "### Subtask:\n",
        "Analizar la conversación y las celdas del cuaderno para identificar los bloques de código y texto que representan los pasos exitosos del ETL y la definición del DAG.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53ae3208"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to analyze the previous code and markdown cells to identify the successful steps of the ETL process and DAG definition based on the execution status and content. This will involve looking for cells marked as \"Execution succeeded\" that contain relevant code for downloading, transforming, and loading data, as well as defining DAG components.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec05be46",
        "outputId": "fa4e08db-8b16-4e16-a313-5f25a8137af0"
      },
      "source": [
        "# Analyze the notebook cells to identify successful ETL and DAG definition steps.\n",
        "# This is a conceptual step performed by analyzing the provided notebook structure and execution results.\n",
        "# I will list the cell IDs that contain successful code relevant to the ETL and DAG definition.\n",
        "\n",
        "successful_etl_cells = [\n",
        "    \"1ad07a86\", # DAG definition, default_args, download function definition, start/end tasks\n",
        "    \"3bc0f29c\", # Define dimension schemas, init BQ client, create dataset/tables\n",
        "    \"f625c2a9\", # Populate Dim_Geografia\n",
        "    \"fcfbfa2a\", # Populate Dim_Tiempo\n",
        "    \"51dc3a07\", # Populate Dim_TipoPresion\n",
        "    \"35268d40\", # Define Fact schema, create Fact table\n",
        "    \"a0af0a8f\", # Define ETL functions for dimensions and fact\n",
        "    \"35bdd9f5\", # Define Airflow tasks and dependencies\n",
        "    \"948eccef\", # Display the final DAG code file\n",
        "]\n",
        "\n",
        "# Note: There were failed attempts in between, but the goal is to document the successful approach.\n",
        "# The code in cell a0af0a8f and 35bdd9f5 represents the successful structure and functions for the DAG.\n",
        "# The previous successful individual cell executions (f625c2a9, fcfbfa2a, 51dc3a07, 35268d40)\n",
        "# confirm the logic within those functions is sound, although the functions themselves are defined\n",
        "# in the later cell a0af0a8f for inclusion in the final DAG file.\n",
        "\n",
        "print(\"Identified successful ETL and DAG definition steps (cell IDs):\")\n",
        "for cell_id in successful_etl_cells:\n",
        "    print(cell_id)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified successful ETL and DAG definition steps (cell IDs):\n",
            "1ad07a86\n",
            "3bc0f29c\n",
            "f625c2a9\n",
            "fcfbfa2a\n",
            "51dc3a07\n",
            "35268d40\n",
            "a0af0a8f\n",
            "35bdd9f5\n",
            "948eccef\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e77df3ce"
      },
      "source": [
        "**Reasoning**:\n",
        "I have identified the successful steps. Now I need to synthesize the documentation based on these steps, organizing it by phases as requested in the main task. I will structure the documentation in Spanish, including explanations and relevant code snippets or references to the full DAG code. I will consolidate the information from the identified successful cells into logical sections for the documentation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "a0087736",
        "outputId": "f5a2720f-4b6f-4a80-ba5c-fd9248f29569"
      },
      "source": [
        "# This cell will generate the documentation content based on the successful steps identified.\n",
        "# It will be a markdown output describing the process and the DAG.\n",
        "\n",
        "documentation_content = \"\"\"\n",
        "## Documentación del Proceso ETL y Definición del DAG de Airflow\n",
        "\n",
        "Este documento detalla los pasos exitosos seguidos para construir un pipeline ETL que descarga datos de presión fiscal desde una URL, los transforma y los carga en un esquema estrella en Google BigQuery utilizando Apache Airflow.\n",
        "\n",
        "### Fase 1: Definición Básica del DAG y Tarea de Descarga\n",
        "\n",
        "Se definió la estructura básica del DAG de Airflow `presionfiscal_etl` y una tarea para descargar el archivo CSV de origen.\n",
        "\n",
        "**Configuración del DAG:**\n",
        "\n",
        "*   **dag_id:** `presionfiscal_etl`\n",
        "*   **default_args:** `owner='airflow', depends_on_past=False, start_date=datetime(2024, 6, 1), email_on_failure=False, email_on_retry=False, retries=1, retry_delay=timedelta(minutes=5)`\n",
        "*   **description:** 'ETL pipeline to download, transform, and load SRI tax pressure data to BigQuery'\n",
        "*   **schedule:** `None` (para ejecución manual)\n",
        "*   **catchup:** `False`\n",
        "*   **max_active_runs:** `1`\n",
        "*   **tags:** `['etl', 'bigquery', 'sri', 'presion_fiscal']`\n",
        "\n",
        "**Tarea de Descarga (`download_source_csv`):**\n",
        "\n",
        "*   **Tipo:** `PythonOperator`\n",
        "*   **Función Python:** `download_csv_from_url` (descarga el CSV desde la URL y lo guarda localmente).\n",
        "*   **Parámetros:** `csv_url`, `local_filename`\n",
        "\n",
        "Se incluyeron tareas `start` y `end` (`EmptyOperator`) como marcadores de inicio y fin del pipeline.\n",
        "\n",
        "**Código relevante (definiciones de DAG y función de descarga):**\n",
        "```python\n",
        "# Import necessary modules...\n",
        "from airflow import DAG\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "from airflow.operators.python import PythonOperator\n",
        "from airflow.operators.empty import EmptyOperator\n",
        "# ... other imports ...\n",
        "import requests\n",
        "import logging\n",
        "\n",
        "# Configure logging...\n",
        "\n",
        "# Define default arguments...\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2024, 6, 1),\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "# Define the DAG object\n",
        "with DAG(\n",
        "    'presionfiscal_etl',\n",
        "    default_args=default_args,\n",
        "    description='ETL pipeline to download, transform, and load SRI tax pressure data to BigQuery',\n",
        "    schedule=None,\n",
        "    catchup=False,\n",
        "    max_active_runs=1,\n",
        "    tags=['etl', 'bigquery', 'sri', 'presion_fiscal'],\n",
        ") as dag:\n",
        "    # Define variables like local_filename, csv_url, credential_filepath\n",
        "\n",
        "    # Define the Python function for the download task\n",
        "    def download_csv_from_url(csv_url, local_filename):\n",
        "        # ... implementation ...\n",
        "        pass # Refer to the full DAG code for implementation\n",
        "\n",
        "    # Define the start task\n",
        "    start = EmptyOperator(task_id='start')\n",
        "\n",
        "    # Instantiate the PythonOperator for the download task\n",
        "    download_csv_task = PythonOperator(\n",
        "        task_id='download_source_csv',\n",
        "        python_callable=download_csv_from_url,\n",
        "        op_kwargs={'csv_url': csv_url, 'local_filename': local_filename},\n",
        "    )\n",
        "\n",
        "    # Define the end task (connected later)\n",
        "    end = EmptyOperator(task_id='end')\n",
        "\n",
        "    # Initial dependency\n",
        "    start >> download_csv_task\n",
        "\n",
        "    # ... other tasks and dependencies will be added here ...\n",
        "```\n",
        "*(Referirse al archivo `presionfiscal_etl_dag.py` para el código completo)*\n",
        "\n",
        "### Fase 2: ETL de Dimensiones\n",
        "\n",
        "Se definieron las tablas de dimensiones (`Dim_Geografia`, `Dim_Tiempo`, `Dim_TipoPresion`) y se implementó la lógica para extraer datos únicos de la tabla de origen en BigQuery, generar claves subrogadas y cargar los datos en las respectivas tablas de dimensión en BigQuery.\n",
        "\n",
        "**Tablas de Dimensiones y Esquemas:**\n",
        "\n",
        "*   `Dim_Geografia`: `id_geografia` (INT64, REQUIRED), `pais` (STRING, REQUIRED), `region` (STRING, REQUIRED)\n",
        "*   `Dim_Tiempo`: `tiempo_id` (INT64, REQUIRED), `año` (INT64, REQUIRED)\n",
        "*   `Dim_TipoPresion`: `tipo_presion_id` (INT64, REQUIRED), `tipo_presion_nombre` (STRING, REQUIRED)\n",
        "\n",
        "Se utilizó el cliente de BigQuery autenticado con un archivo de credenciales de servicio (`/etl-sistemas-sri-cc64af72be76.json`) y se aseguró que el dataset y las tablas de dimensiones existieran o fueran creadas en el proyecto `etl-sistemas-sri`.\n",
        "\n",
        "**Tareas ETL de Dimensiones:**\n",
        "\n",
        "*   `process_dim_geografia`: `PythonOperator` que ejecuta la función `process_dim_geografia`. Esta función consulta la tabla de origen, crea un DataFrame, añade `id_geografia` y carga en `Dim_Geografia`.\n",
        "*   `process_dim_tiempo`: `PythonOperator` que ejecuta la función `process_dim_tiempo`. Esta función consulta la tabla de origen, crea un DataFrame, añade `tiempo_id` y carga en `Dim_Tiempo`.\n",
        "*   `process_dim_tipopresion`: `PythonOperator` que ejecuta la función `process_dim_tipopresion`. Esta función consulta la tabla de origen, crea un DataFrame, añade `tipo_presion_id` y carga en `Dim_TipoPresion`.\n",
        "\n",
        "**Código relevante (funciones ETL de dimensiones y creación de tablas):**\n",
        "```python\n",
        "# ... imports and configuration variables ...\n",
        "\n",
        "# Helper function to create or get a BigQuery table\n",
        "def create_or_get_table(client, table_id, schema):\n",
        "    # ... implementation ...\n",
        "    pass # Refer to the full DAG code\n",
        "\n",
        "# Define schemas for dimension tables\n",
        "schema_dim_geografia = [...] # Refer to the full DAG code\n",
        "schema_dim_tiempo = [...] # Refer to the full DAG code\n",
        "schema_dim_tipopresion = [...] # Refer to the full DAG code\n",
        "\n",
        "# Define table IDs for dimension tables\n",
        "table_id_dim_geografia = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Geografia\"\n",
        "table_id_dim_tiempo = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Tiempo\"\n",
        "table_id_dim_tipopresion = f\"{PROJECT_ID}.{DATASET_ID}.Dim_TipoPresion\"\n",
        "\n",
        "# Inside the DAG context:\n",
        "# Create or get dataset and dimension tables\n",
        "# ... client initialization ...\n",
        "# dataset = client.create_dataset(...)\n",
        "# table_dim_geografia = create_or_get_table(...)\n",
        "# table_dim_tiempo = create_or_get_table(...)\n",
        "# table_dim_tipopresion = create_or_get_table(...)\n",
        "\n",
        "\n",
        "# Define Python function for Dim_Geografia ETL\n",
        "def process_dim_geografia(credential_filepath=credential_filepath):\n",
        "    # ... implementation ...\n",
        "    pass # Refer to the full DAG code\n",
        "\n",
        "# Define Python function for Dim_Tiempo ETL\n",
        "def process_dim_tiempo(credential_filepath=credential_filepath):\n",
        "    # ... implementation ...\n",
        "    pass # Refer to the full DAG code\n",
        "\n",
        "# Define Python function for Dim_TipoPresion ETL\n",
        "def process_dim_tipopresion(credential_filepath=credential_filepath):\n",
        "    # ... implementation ...\n",
        "    pass # Refer to the full DAG code\n",
        "\n",
        "# Inside the DAG context:\n",
        "# Instantiate dimension ETL tasks\n",
        "# process_dim_geografia_task = PythonOperator(...)\n",
        "# process_dim_tiempo_task = PythonOperator(...)\n",
        "# process_dim_tipopresion_task = PythonOperator(...)\n",
        "```\n",
        "*(Referirse al archivo `presionfiscal_etl_dag.py` para el código completo)*\n",
        "\n",
        "### Fase 3: ETL de Hechos\n",
        "\n",
        "Se definió la tabla de hechos (`Fact_PresionFiscal`) y se implementó la lógica para extraer los datos transaccionales de la tabla de origen en BigQuery, realizar lookups con las tablas de dimensiones (también en BigQuery) para obtener las claves subrogadas, y cargar los datos transformados en la tabla de hechos en BigQuery.\n",
        "\n",
        "**Tabla de Hechos y Esquema:**\n",
        "\n",
        "*   `Fact_PresionFiscal`: `id_geografia` (INT64, REQUIRED), `tiempo_id` (INT64, REQUIRED), `tipo_presion_id` (INT64, REQUIRED), `presion_fiscal` (FLOAT64, NULLABLE)\n",
        "\n",
        "Se aseguró que la tabla de hechos existiera o fuera creada con el esquema correcto en el dataset `sri_presion_fiscal` del proyecto `etl-sistemas-sri`.\n",
        "\n",
        "**Tarea ETL de Hechos:**\n",
        "\n",
        "*   `process_fact_presionfiscal`: `PythonOperator` que ejecuta la función `process_fact_presionfiscal`. Esta función consulta la tabla de origen, consulta las tablas de dimensiones para los lookups, realiza los merges en un DataFrame de pandas y carga el resultado en `Fact_PresionFiscal`.\n",
        "\n",
        "**Código relevante (función ETL de hechos y creación de tabla):**\n",
        "```python\n",
        "# ... imports and configuration variables ...\n",
        "\n",
        "# Define schema for the Fact table\n",
        "schema_fact = [...] # Refer to the full DAG code\n",
        "\n",
        "# Define table ID for the Fact table\n",
        "table_id_fact = f\"{PROJECT_ID}.{DATASET_ID}.Fact_PresionFiscal\"\n",
        "\n",
        "# Inside the DAG context:\n",
        "# Create or get Fact table\n",
        "# table_fact = create_or_get_table(client, table_id_fact, schema_fact)\n",
        "\n",
        "# Define Python function for Fact ETL\n",
        "def process_fact_presionfiscal(credential_filepath=credential_filepath):\n",
        "    \"\"\"\n",
        "    Extracts source data from BigQuery, performs lookups with dimension tables,\n",
        "    and loads into Fact_PresionFiscal in BigQuery.\n",
        "    \"\"\"\n",
        "    # ... implementation including BigQuery client, queries, merges, and load ...\n",
        "    pass # Refer to the full DAG code\n",
        "\n",
        "# Inside the DAG context:\n",
        "# Instantiate Fact ETL task\n",
        "# process_fact_presionfiscal_task = PythonOperator(...)\n",
        "```\n",
        "*(Referirse al archivo `presionfiscal_etl_dag.py` para el código completo)*\n",
        "\n",
        "### Fase 4: Definición de Tareas y Dependencias del DAG\n",
        "\n",
        "Se definieron todas las tareas utilizando `PythonOperator` (para descarga, ETL de dimensiones y hechos) y `EmptyOperator` (para inicio y fin). Se establecieron las dependencias para asegurar el orden de ejecución correcto:\n",
        "\n",
        "`start` >> `download_source_csv` >> `[process_dim_geografia, process_dim_tiempo, process_dim_tipopresion]` >> `process_fact_presionfiscal` >> `end`\n",
        "\n",
        "Esto garantiza que la descarga se complete antes de iniciar el ETL de dimensiones, que todo el ETL de dimensiones se complete antes de iniciar el ETL de hechos (ya que la tabla de hechos depende de las claves subrogadas de las dimensiones), y que la tarea final se ejecute solo después de que el ETL de hechos haya terminado.\n",
        "\n",
        "**Código relevante (definición de tareas e independencias):**\n",
        "```python\n",
        "# ... imports, configuration variables, and function definitions ...\n",
        "\n",
        "# Inside the DAG context:\n",
        "# Define the start task\n",
        "start = EmptyOperator(task_id='start')\n",
        "\n",
        "# Instantiate the PythonOperator for the download task\n",
        "download_csv_task = PythonOperator(...) # Refer to Fase 1\n",
        "\n",
        "# Instantiate the PythonOperators for dimension ETL tasks\n",
        "process_dim_geografia_task = PythonOperator(...) # Refer to Fase 2\n",
        "process_dim_tiempo_task = PythonOperator(...) # Refer to Fase 2\n",
        "process_dim_tipopresion_task = PythonOperator(...) # Refer to Fase 2\n",
        "\n",
        "# Instantiate the PythonOperator for the Fact ETL task\n",
        "process_fact_presionfiscal_task = PythonOperator(...) # Refer to Fase 3\n",
        "\n",
        "# Define the end task\n",
        "end = EmptyOperator(task_id='end')\n",
        "\n",
        "# --- Define Dependencies ---\n",
        "start >> download_csv_task\n",
        "download_csv_task >> [process_dim_geografia_task, process_dim_tiempo_task, process_dim_tipopresion_task]\n",
        "[process_dim_geografia_task, process_dim_tiempo_task, process_dim_tipopresion_task] >> process_fact_presionfiscal_task\n",
        "process_fact_presionfiscal_task >> end\n",
        "```\n",
        "*(Referirse al archivo `presionfiscal_etl_dag.py` para el código completo)*\n",
        "\n",
        "### Archivo Completo del DAG\n",
        "\n",
        "El código completo del DAG, combinando todas las importaciones, variables de configuración, definiciones de funciones Python y la estructura del DAG con tareas y dependencias, se encuentra en el archivo `presionfiscal_etl_dag.py`.\n",
        "\n",
        "*(El contenido de `presionfiscal_etl_dag.py` fue generado y mostrado en la celda `948eccef`)*\n",
        "\n",
        "### Instrucciones para Usar el DAG en Airflow (Google Cloud Composer)\n",
        "\n",
        "1.  **Crear/Verificar Entorno de Composer:** Asegúrate de que tu entorno de Google Cloud Composer (`etl-composer` u otro nombre) se haya creado y esté en estado \"Running\" en la consola de Google Cloud.\n",
        "2.  **Acceder a la UI de Airflow:** Utiliza el enlace proporcionado en la página de detalles de tu entorno de Composer en la consola de Google Cloud para acceder a la interfaz de usuario web de Airflow.\n",
        "3.  **Configurar Credenciales (Opcional pero Recomendado):** Aunque el DAG actual usa la variable de entorno `GOOGLE_APPLICATION_CREDENTIALS`, la práctica recomendada en producción es configurar una conexión de Google Cloud en Airflow (Admin -> Connections) o usar Variables de Airflow para el ID del proyecto y la ruta del archivo de credenciales, y modificar las funciones Python para usar estas configuraciones. Si mantienes el enfoque de variable de entorno, asegúrate de que la clave JSON esté accesible en el sistema de archivos de los workers de Airflow en Composer y que la variable de entorno `GOOGLE_APPLICATION_CREDENTIALS` esté configurada para apuntar a esa ruta en el entorno de ejecución de Airflow.\n",
        "4.  **Cargar el Archivo del DAG:** Sube el archivo `presionfiscal_etl_dag.py` a la carpeta `dags` del bucket de Cloud Storage asociado a tu entorno de Composer. Puedes encontrar el nombre del bucket en la página de detalles del entorno de Composer. Utiliza Cloud Storage Browser en la consola de GCP o gsutil para subir el archivo.\n",
        "5.  **Verificar en la UI de Airflow:** Refresca la página de la UI de Airflow. El DAG `presionfiscal_etl` debería aparecer en la lista (puede tardar unos minutos en ser detectado). Si hay errores de sintaxis, el DAG no aparecerá y deberás revisar los logs del scheduler de Airflow en Cloud Logging.\n",
        "6.  **Activar el DAG:** Asegúrate de que el interruptor del DAG esté activado en la UI de Airflow.\n",
        "7.  **Ejecutar el DAG:** Haz clic en el nombre del DAG y luego en el botón \"Trigger DAG\" para iniciar una ejecución manual.\n",
        "8.  **Monitorizar la Ejecución:** Observa el progreso de las tareas en la vista \"Graph View\" o \"Tree View\". Los logs de cada tarea (`Log` en el menú desplegable de la instancia de tarea) son cruciales para depurar problemas.\n",
        "9.  **Verificar Resultados en BigQuery:** Una vez que el DAG complete su ejecución exitosamente, dirígete a la consola de Google Cloud, navega a BigQuery y verifica que las tablas `Dim_Geografia`, `Dim_Tiempo`, `Dim_TipoPresion` y `Fact_PresionFiscal` existan dentro del dataset `sri_presion_fiscal` en el proyecto `etl-sistemas-sri` y que contengan los datos esperados.\n",
        "\n",
        "\"\"\"\n",
        "print(documentation_content)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (ipython-input-2-1822082599.py, line 193)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2-1822082599.py\"\u001b[0;36m, line \u001b[0;32m193\u001b[0m\n\u001b[0;31m    Extracts source data from BigQuery, performs lookups with dimension tables,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a078414"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed due to an `IndentationError` within the multi-line string for the documentation content. This is likely caused by incorrect indentation of the code snippets included within the markdown string. I need to regenerate the documentation content, ensuring that the code blocks within the markdown are correctly formatted with no leading indentation that would cause an `IndentationError`. I will correct the indentation of the Python code snippets within the documentation string.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "7QiZW5AFzDz7",
        "outputId": "c08f2174-e91a-41c3-e387-f516da9dbd0f"
      },
      "source": [
        "# This cell will regenerate the documentation content with corrected indentation for code blocks.\n",
        "# It will be a markdown output describing the process and the DAG.\n",
        "\n",
        "documentation_content = \"\"\"\n",
        "## Documentación del Proceso ETL y Definición del DAG de Airflow\n",
        "\n",
        "Este documento detalla los pasos exitosos seguidos para construir un pipeline ETL que descarga datos de presión fiscal desde una URL, los transforma y los carga en un esquema estrella en Google BigQuery utilizando Apache Airflow.\n",
        "\n",
        "### Fase 1: Definición Básica del DAG y Tarea de Descarga\n",
        "\n",
        "Se definió la estructura básica del DAG de Airflow `presionfiscal_etl` y una tarea para descargar el archivo CSV de origen.\n",
        "\n",
        "**Configuración del DAG:**\n",
        "\n",
        "*   **dag_id:** `presionfiscal_etl`\n",
        "*   **default_args:** `owner='airflow', depends_on_past=False, start_date=datetime(2024, 6, 1), email_on_failure=False, email_on_retry=False, retries=1, retry_delay=timedelta(minutes=5)`\n",
        "*   **description:** 'ETL pipeline to download, transform, and load SRI tax pressure data to BigQuery'\n",
        "*   **schedule:** `None` (para ejecución manual)\n",
        "*   **catchup:** `False`\n",
        "*   **max_active_runs:** `1`\n",
        "*   **tags:** `['etl', 'bigquery', 'sri', 'presion_fiscal']`\n",
        "\n",
        "**Tarea de Descarga (`download_source_csv`):**\n",
        "\n",
        "*   **Tipo:** `PythonOperator`\n",
        "*   **Función Python:** `download_csv_from_url` (descarga el CSV desde la URL y lo guarda localmente).\n",
        "*   **Parámetros:** `csv_url`, `local_filename`\n",
        "\n",
        "Se incluyeron tareas `start` y `end` (`EmptyOperator`) como marcadores de inicio y fin del pipeline.\n",
        "\n",
        "**Código relevante (definiciones de DAG y función de descarga):**\n",
        "```python\n",
        "# Import necessary modules...\n",
        "from airflow import DAG\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "from airflow.operators.python import PythonOperator\n",
        "from airflow.operators.empty import EmptyOperator\n",
        "# ... other imports ...\n",
        "import requests\n",
        "import logging\n",
        "\n",
        "# Configure logging...\n",
        "\n",
        "# Define default arguments...\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2024, 6, 1),\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "# Define the DAG object\n",
        "with DAG(\n",
        "    'presionfiscal_etl',\n",
        "    default_args=default_args,\n",
        "    description='ETL pipeline to download, transform, and load SRI tax pressure data to BigQuery',\n",
        "    schedule=None,\n",
        "    catchup=False,\n",
        "    max_active_runs=1,\n",
        "    tags=['etl', 'bigquery', 'sri', 'presion_fiscal'],\n",
        ") as dag:\n",
        "    # Define variables like local_filename, csv_url, credential_filepath\n",
        "\n",
        "    # Define the Python function for the download task\n",
        "    def download_csv_from_url(csv_url, local_filename):\n",
        "        # ... implementation ...\n",
        "        pass # Refer to the full DAG code for implementation\n",
        "\n",
        "    # Define the start task\n",
        "    start = EmptyOperator(task_id='start')\n",
        "\n",
        "    # Instantiate the PythonOperator for the download task\n",
        "    download_csv_task = PythonOperator(\n",
        "        task_id='download_source_csv',\n",
        "        python_callable=download_csv_from_url,\n",
        "        op_kwargs={'csv_url': csv_url, 'local_filename': local_filename},\n",
        "    )\n",
        "\n",
        "    # Define the end task (connected later)\n",
        "    end = EmptyOperator(task_id='end')\n",
        "\n",
        "    # Initial dependency\n",
        "    start >> download_csv_task\n",
        "\n",
        "    # ... other tasks and dependencies will be added here ...\n",
        "```\n",
        "*(Referirse al archivo `presionfiscal_etl_dag.py` para el código completo)*\n",
        "\n",
        "### Fase 2: ETL de Dimensiones\n",
        "\n",
        "Se definieron las tablas de dimensiones (`Dim_Geografia`, `Dim_Tiempo`, `Dim_TipoPresion`) y se implementó la lógica para extraer datos únicos de la tabla de origen en BigQuery, generar claves subrogadas y cargar los datos en las respectivas tablas de dimensión en BigQuery.\n",
        "\n",
        "**Tablas de Dimensiones y Esquemas:**\n",
        "\n",
        "*   `Dim_Geografia`: `id_geografia` (INT64, REQUIRED), `pais` (STRING, REQUIRED), `region` (STRING, REQUIRED)\n",
        "*   `Dim_Tiempo`: `tiempo_id` (INT64, REQUIRED), `año` (INT64, REQUIRED)\n",
        "*   `Dim_TipoPresion`: `tipo_presion_id` (INT64, REQUIRED), `tipo_presion_nombre` (STRING, REQUIRED)\n",
        "\n",
        "Se utilizó el cliente de BigQuery autenticado con un archivo de credenciales de servicio (`/etl-sistemas-sri-cc64af72be76.json`) y se aseguró que el dataset y las tablas de dimensiones existieran o fueran creadas en el proyecto `etl-sistemas-sri`.\n",
        "\n",
        "**Tareas ETL de Dimensiones:**\n",
        "\n",
        "*   `process_dim_geografia`: `PythonOperator` que ejecuta la función `process_dim_geografia`. Esta función consulta la tabla de origen, crea un DataFrame, añade `id_geografia` y carga en `Dim_Geografia`.\n",
        "*   `process_dim_tiempo`: `PythonOperator` que ejecuta la función `process_dim_tiempo`. Esta función consulta la tabla de origen, crea un DataFrame, añade `tiempo_id` y carga en `Dim_Tiempo`.\n",
        "*   `process_dim_tipopresion`: `PythonOperator` que ejecuta la función `process_dim_tipopresion`. Esta función consulta la tabla de origen, crea un DataFrame, añade `tipo_presion_id` y carga en `Dim_TipoPresion`.\n",
        "\n",
        "**Código relevante (funciones ETL de dimensiones y creación de tablas):**\n",
        "```python\n",
        "# ... imports and configuration variables ...\n",
        "\n",
        "# Helper function to create or get a BigQuery table\n",
        "def create_or_get_table(client, table_id, schema):\n",
        "    # ... implementation ...\n",
        "    pass # Refer to the full DAG code\n",
        "\n",
        "# Define schemas for dimension tables\n",
        "schema_dim_geografia = [...] # Refer to the full DAG code\n",
        "schema_dim_tiempo = [...] # Refer to the full DAG code\n",
        "schema_dim_tipopresion = [...] # Refer to the full DAG code\n",
        "\n",
        "# Define table IDs for dimension tables\n",
        "table_id_dim_geografia = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Geografia\"\n",
        "table_id_dim_tiempo = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Tiempo\"\n",
        "table_id_dim_tipopresion = f\"{PROJECT_ID}.{DATASET_ID}.Dim_TipoPresion\"\n",
        "\n",
        "# Inside the DAG context:\n",
        "# Create or get dataset and dimension tables\n",
        "# ... client initialization ...\n",
        "# dataset = client.create_dataset(...)\n",
        "# table_dim_geografia = create_or_get_table(...)\n",
        "# table_dim_tiempo = create_or_get_table(...)\n",
        "# table_dim_tipopresion = create_or_get_table(...)\n",
        "\n",
        "\n",
        "# Define Python function for Dim_Geografia ETL\n",
        "def process_dim_geografia(credential_filepath=credential_filepath):\n",
        "    # ... implementation ...\n",
        "    pass # Refer to the full DAG code\n",
        "\n",
        "# Define Python function for Dim_Tiempo ETL\n",
        "def process_dim_tiempo(credential_filepath=credential_filepath):\n",
        "    # ... implementation ...\n",
        "    pass # Refer to the full DAG code\n",
        "\n",
        "# Define Python function for Dim_TipoPresion ETL\n",
        "def process_dim_tipopresion(credential_filepath=credential_filepath):\n",
        "    # ... implementation ...\n",
        "    pass # Refer to the full DAG code\n",
        "\n",
        "# Inside the DAG context:\n",
        "# Instantiate dimension ETL tasks\n",
        "# process_dim_geografia_task = PythonOperator(...)\n",
        "# process_dim_tiempo_task = PythonOperator(...)\n",
        "# process_dim_tipopresion_task = PythonOperator(...)\n",
        "```\n",
        "*(Referirse al archivo `presionfiscal_etl_dag.py` para el código completo)*\n",
        "\n",
        "### Fase 3: ETL de Hechos\n",
        "\n",
        "Se definió la tabla de hechos (`Fact_PresionFiscal`) y se implementó la lógica para extraer los datos transaccionales de la tabla de origen en BigQuery, realizar lookups con las tablas de dimensiones (también en BigQuery) para obtener las claves subrogadas, y cargar los datos transformados en la tabla de hechos en BigQuery.\n",
        "\n",
        "**Tabla de Hechos y Esquema:**\n",
        "\n",
        "*   `Fact_PresionFiscal`: `id_geografia` (INT64, REQUIRED), `tiempo_id` (INT64, REQUIRED), `tipo_presion_id` (INT64, REQUIRED), `presion_fiscal` (FLOAT64, NULLABLE)\n",
        "\n",
        "Se aseguró que la tabla de hechos existiera o fuera creada con el esquema correcto en el dataset `sri_presion_fiscal` del proyecto `etl-sistemas-sri`.\n",
        "\n",
        "**Tarea ETL de Hechos:**\n",
        "\n",
        "*   `process_fact_presionfiscal`: `PythonOperator` que ejecuta la función `process_fact_presionfiscal`. Esta función consulta la tabla de origen, consulta las tablas de dimensiones para los lookups, realiza los merges en un DataFrame de pandas y carga el resultado en `Fact_PresionFiscal`.\n",
        "\n",
        "**Código relevante (función ETL de hechos y creación de tabla):**\n",
        "```python\n",
        "# ... imports and configuration variables ...\n",
        "\n",
        "# Define schema for the Fact table\n",
        "schema_fact = [...] # Refer to the full DAG code\n",
        "\n",
        "# Define table ID for the Fact table\n",
        "table_id_fact = f\"{PROJECT_ID}.{DATASET_ID}.Fact_PresionFiscal\"\n",
        "\n",
        "# Inside the DAG context:\n",
        "# Create or get Fact table\n",
        "# table_fact = create_or_get_table(client, table_id_fact, schema_fact)\n",
        "\n",
        "# Define Python function for Fact ETL\n",
        "def process_fact_presionfiscal(credential_filepath=credential_filepath):\n",
        "    \"\"\"\n",
        "    Extracts source data from BigQuery, performs lookups with dimension tables,\n",
        "    and loads into Fact_PresionFiscal in BigQuery.\n",
        "    \"\"\"\n",
        "    # ... implementation including BigQuery client, queries, merges, and load ...\n",
        "    pass # Refer to the full DAG code\n",
        "\n",
        "# Inside the DAG context:\n",
        "# Instantiate Fact ETL task\n",
        "# process_fact_presionfiscal_task = PythonOperator(...)\n",
        "```\n",
        "*(Referirse al archivo `presionfiscal_etl_dag.py` para el código completo)*\n",
        "\n",
        "### Fase 4: Definición de Tareas y Dependencias del DAG\n",
        "\n",
        "Se definieron todas las tareas utilizando `PythonOperator` (para descarga, ETL de dimensiones y hechos) y `EmptyOperator` (para inicio y fin). Se establecieron las dependencias para asegurar el orden de ejecución correcto:\n",
        "\n",
        "`start` >> `download_source_csv` >> `[process_dim_geografia, process_dim_tiempo, process_dim_tipopresion]` >> `process_fact_presionfiscal` >> `end`\n",
        "\n",
        "Esto garantiza que la descarga se complete antes de iniciar el ETL de dimensiones, que todo el ETL de dimensiones se complete antes de iniciar el ETL de hechos (ya que la tabla de hechos depende de las claves subrogadas de las dimensiones), y que la tarea final se ejecute solo después de que el ETL de hechos haya terminado.\n",
        "\n",
        "**Código relevante (definición de tareas e independencias):**\n",
        "```python\n",
        "# ... imports, configuration variables, and function definitions ...\n",
        "\n",
        "# Inside the DAG context:\n",
        "# Define the start task\n",
        "start = EmptyOperator(task_id='start')\n",
        "\n",
        "# Instantiate the PythonOperator for the download task\n",
        "download_csv_task = PythonOperator(...) # Refer to Fase 1\n",
        "\n",
        "# Instantiate the PythonOperators for dimension ETL tasks\n",
        "process_dim_geografia_task = PythonOperator(...) # Refer to Fase 2\n",
        "process_dim_tiempo_task = PythonOperator(...) # Refer to Fase 2\n",
        "process_dim_tipopresion_task = PythonOperator(...) # Refer to Fase 2\n",
        "\n",
        "# Instantiate the PythonOperator for the Fact ETL task\n",
        "process_fact_presionfiscal_task = PythonOperator(...) # Refer to Fase 3\n",
        "\n",
        "# Define the end task\n",
        "end = EmptyOperator(task_id='end')\n",
        "\n",
        "# --- Define Dependencies ---\n",
        "start >> download_csv_task\n",
        "download_csv_task >> [process_dim_geografia_task, process_dim_tiempo_task, process_dim_tipopresion_task]\n",
        "[process_dim_geografia_task, process_dim_tiempo_task, process_dim_tipopresion_task] >> process_fact_presionfiscal_task\n",
        "process_fact_presionfiscal_task >> end\n",
        "```\n",
        "*(Referirse al archivo `presionfiscal_etl_dag.py` para el código completo)*\n",
        "\n",
        "### Archivo Completo del DAG\n",
        "\n",
        "El código completo del DAG, combinando todas las importaciones, variables de configuración, definiciones de funciones Python y la estructura del DAG con tareas y dependencias, se encuentra en el archivo `presionfiscal_etl_dag.py`.\n",
        "\n",
        "*(El contenido de `presionfiscal_etl_dag.py` fue generado y mostrado en la celda `948eccef`)*\n",
        "\n",
        "### Instrucciones para Usar el DAG en Airflow (Google Cloud Composer)\n",
        "\n",
        "1.  **Crear/Verificar Entorno de Composer:** Asegúrate de que tu entorno de Google Cloud Composer (`etl-composer` u otro nombre) se haya creado y esté en estado \"Running\" en la consola de Google Cloud.\n",
        "2.  **Acceder a la UI de Airflow:** Utiliza el enlace proporcionado en la página de detalles de tu entorno de Composer en la consola de Google Cloud para acceder a la interfaz de usuario web de Airflow.\n",
        "3.  **Configurar Credenciales (Opcional pero Recomendado):** Aunque el DAG actual usa la variable de entorno `GOOGLE_APPLICATION_CREDENTIALS`, la práctica recomendada en producción es configurar una conexión de Google Cloud en Airflow (Admin -> Connections) o usar Variables de Airflow para el ID del proyecto y la ruta del archivo de credenciales, y modificar las funciones Python para usar estas configuraciones. Si mantienes el enfoque de variable de entorno, asegúrate de que la clave JSON esté accesible en el sistema de archivos de los workers de Airflow en Composer y que la variable de entorno `GOOGLE_APPLICATION_CREDENTIALS` esté configurada para apuntar a esa ruta en el entorno de ejecución de Airflow.\n",
        "4.  **Cargar el Archivo del DAG:** Sube el archivo `presionfiscal_etl_dag.py` a la carpeta `dags` del bucket de Cloud Storage asociado a tu entorno de Composer. Puedes encontrar el nombre del bucket en la página de detalles del entorno de Composer. Utiliza Cloud Storage Browser en la consola de GCP o gsutil para subir el archivo.\n",
        "5.  **Verificar en la UI de Airflow:** Refresca la página de la UI de Airflow. El DAG `presionfiscal_etl` debería aparecer en la lista (puede tardar unos minutos en ser detectado). Si hay errores de sintaxis, el DAG no aparecerá y deberás revisar los logs del scheduler de Airflow en Cloud Logging.\n",
        "6.  **Activar el DAG:** Asegúrate de que el interruptor del DAG esté activado en la UI de Airflow.\n",
        "7.  **Ejecutar el DAG:** Haz clic en el nombre del DAG y luego en el botón \"Trigger DAG\" para iniciar una ejecución manual.\n",
        "8.  **Monitorizar la Ejecución:** Observa el progreso de las tareas en la vista \"Graph View\" o \"Tree View\". Los logs de cada tarea (`Log` en el menú desplegable de la instancia de tarea) son cruciales para depurar problemas.\n",
        "9.  **Verificar Resultados en BigQuery:** Una vez que el DAG complete su ejecución exitosamente, dirígete a la consola de Google Cloud, navega a BigQuery y verifica que las tablas `Dim_Geografia`, `Dim_Tiempo`, `Dim_TipoPresion` y `Fact_PresionFiscal` existan dentro del dataset `sri_presion_fiscal` en el proyecto `etl-sistemas-sri` y que contengan los datos esperados.\n",
        "\n",
        "\"\"\"\n",
        "print(documentation_content)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (ipython-input-3-428629807.py, line 193)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3-428629807.py\"\u001b[0;36m, line \u001b[0;32m193\u001b[0m\n\u001b[0;31m    Extracts source data from BigQuery, performs lookups with dimension tables,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc12158e"
      },
      "source": [
        "**Reasoning**:\n",
        "The `IndentationError` persists, indicating the issue is with the way the multi-line string itself is being interpreted, likely still related to the embedded code blocks within the markdown. Instead of embedding large code snippets directly into the markdown string, which is prone to indentation issues, I will provide references to the full DAG code that was successfully generated in cell `948eccef`. The markdown will describe the steps and components, and instruct the user to refer to the complete DAG file for the actual code. This approach avoids the indentation problem and provides cleaner documentation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DD_u3qbzH5V",
        "outputId": "7b76296f-64b9-4894-a5f5-502a4e0fa694"
      },
      "source": [
        "# This cell will generate the documentation content, referencing the full DAG file for code.\n",
        "# It will be a markdown output describing the process and the DAG.\n",
        "\n",
        "documentation_content = \"\"\"\n",
        "## Documentación del Proceso ETL y Definición del DAG de Airflow\n",
        "\n",
        "Este documento detalla los pasos exitosos seguidos para construir un pipeline ETL que descarga datos de presión fiscal desde una URL, los transforma y los carga en un esquema estrella en Google BigQuery utilizando Apache Airflow.\n",
        "\n",
        "El código completo del DAG se encuentra en el archivo `presionfiscal_etl_dag.py`, generado previamente en la celda `948eccef`.\n",
        "\n",
        "### Fase 1: Definición Básica del DAG y Tarea de Descarga\n",
        "\n",
        "Se definió la estructura básica del DAG de Airflow `presionfiscal_etl` y una tarea para descargar el archivo CSV de origen.\n",
        "\n",
        "**Configuración del DAG:**\n",
        "\n",
        "*   **dag_id:** `presionfiscal_etl`\n",
        "*   **default_args:** (Ver código completo del DAG)\n",
        "*   **description:** 'ETL pipeline to download, transform, and load SRI tax pressure data to BigQuery'\n",
        "*   **schedule:** `None` (para ejecución manual)\n",
        "*   **catchup:** `False`\n",
        "*   **max_active_runs:** `1`\n",
        "*   **tags:** `['etl', 'bigquery', 'sri', 'presion_fiscal']`\n",
        "\n",
        "**Tareas:**\n",
        "\n",
        "*   `start`: `EmptyOperator` para marcar el inicio.\n",
        "*   `download_source_csv`: `PythonOperator` que ejecuta la función `download_csv_from_url` para descargar el CSV.\n",
        "*   `end`: `EmptyOperator` para marcar el fin (conectado al final del pipeline).\n",
        "\n",
        "**Código:** Referirse a las secciones de importaciones, `default_args`, definición del DAG (`with DAG(...)`), función `download_csv_from_url`, y definición de las tareas `start`, `download_source_csv` y `end` en el archivo `presionfiscal_etl_dag.py`.\n",
        "\n",
        "### Fase 2: ETL de Dimensiones\n",
        "\n",
        "Se definieron las tablas de dimensiones (`Dim_Geografia`, `Dim_Tiempo`, `Dim_TipoPresion`) y se implementó la lógica para extraer datos únicos de la tabla de origen en BigQuery, generar claves subrogadas y cargar los datos en las respectivas tablas de dimensión en BigQuery.\n",
        "\n",
        "**Tablas de Dimensiones y Esquemas:**\n",
        "\n",
        "*   `Dim_Geografia`: `id_geografia` (INT64, REQUIRED), `pais` (STRING, REQUIRED), `region` (STRING, REQUIRED)\n",
        "*   `Dim_Tiempo`: `tiempo_id` (INT64, REQUIRED), `año` (INT64, REQUIRED)\n",
        "*   `Dim_TipoPresion`: `tipo_presion_id` (INT64, REQUIRED), `tipo_presion_nombre` (STRING, REQUIRED)\n",
        "\n",
        "Se utilizó el cliente de BigQuery autenticado con un archivo de credenciales de servicio y se aseguró que el dataset y las tablas de dimensiones existieran o fueran creadas en el proyecto `etl-sistemas-sri`.\n",
        "\n",
        "**Tareas ETL de Dimensiones:**\n",
        "\n",
        "*   `process_dim_geografia`: `PythonOperator` que ejecuta la función `process_dim_geografia`.\n",
        "*   `process_dim_tiempo`: `PythonOperator` que ejecuta la función `process_dim_tiempo`.\n",
        "*   `process_dim_tipopresion`: `PythonOperator` que ejecuta la función `process_dim_tipopresion`.\n",
        "\n",
        "**Código:** Referirse a la definición de los esquemas de las tablas de dimensiones, la función `create_or_get_table`, las funciones Python `process_dim_geografia`, `process_dim_tiempo`, `process_dim_tipopresion` y la instanciación de los `PythonOperator` correspondientes en el archivo `presionfiscal_etl_dag.py`.\n",
        "\n",
        "### Fase 3: ETL de Hechos\n",
        "\n",
        "Se definió la tabla de hechos (`Fact_PresionFiscal`) y se implementó la lógica para extraer los datos transaccionales de la tabla de origen en BigQuery, realizar lookups con las tablas de dimensiones (también en BigQuery) para obtener las claves subrogadas, y cargar los datos transformados en la tabla de hechos en BigQuery.\n",
        "\n",
        "**Tabla de Hechos y Esquema:**\n",
        "\n",
        "*   `Fact_PresionFiscal`: `id_geografia` (INT64, REQUIRED), `tiempo_id` (INT64, REQUIRED), `tipo_presion_id` (INT64, REQUIRED), `presion_fiscal` (FLOAT64, NULLABLE)\n",
        "\n",
        "Se aseguró que la tabla de hechos existiera o fuera creada con el esquema correcto en el dataset `sri_presion_fiscal` del proyecto `etl-sistemas-sri`.\n",
        "\n",
        "**Tarea ETL de Hechos:**\n",
        "\n",
        "*   `process_fact_presionfiscal`: `PythonOperator` que ejecuta la función `process_fact_presionfiscal`.\n",
        "\n",
        "**Código:** Referirse a la definición del esquema de la tabla de hechos, la función Python `process_fact_presionfiscal` y la instanciación del `PythonOperator` correspondiente en el archivo `presionfiscal_etl_dag.py`.\n",
        "\n",
        "### Fase 4: Definición de Tareas y Dependencias del DAG\n",
        "\n",
        "Se definieron todas las tareas y se establecieron las dependencias para asegurar el orden de ejecución correcto:\n",
        "\n",
        "`start` >> `download_source_csv` >> `[process_dim_geografia, process_dim_tiempo, process_dim_tipopresion]` >> `process_fact_presionfiscal` >> `end`\n",
        "\n",
        "Esto garantiza que la descarga se complete antes de iniciar el ETL de dimensiones, que todo el ETL de dimensiones se complete antes de iniciar el ETL de hechos (ya que la tabla de hechos depende de las claves subrogadas de las dimensiones), y que la tarea final se ejecute solo después de que el ETL de hechos haya terminado.\n",
        "\n",
        "**Código:** Referirse a la sección donde se definen las dependencias entre las tareas utilizando el operador `>>` en el archivo `presionfiscal_etl_dag.py`.\n",
        "\n",
        "### Archivo Completo del DAG\n",
        "\n",
        "El código completo del DAG, combinando todas las importaciones, variables de configuración, definiciones de funciones Python y la estructura del DAG con tareas y dependencias, se encuentra en el archivo `presionfiscal_etl_dag.py`.\n",
        "\n",
        "*(El contenido de `presionfiscal_etl_dag.py` fue generado y mostrado en la celda `948eccef`)*\n",
        "\n",
        "### Instrucciones para Usar el DAG en Airflow (Google Cloud Composer)\n",
        "\n",
        "1.  **Crear/Verificar Entorno de Composer:** Asegúrate de que tu entorno de Google Cloud Composer (`etl-composer` u otro nombre) se haya creado y esté en estado \"Running\" en la consola de Google Cloud.\n",
        "2.  **Acceder a la UI de Airflow:** Utiliza el enlace proporcionado en la página de detalles de tu entorno de Composer en la consola de Google Cloud para acceder a la interfaz de usuario web de Airflow.\n",
        "3.  **Configurar Credenciales (Opcional pero Recomendado):** Aunque el DAG actual usa la variable de entorno `GOOGLE_APPLICATION_CREDENTIALS`, la práctica recomendada en producción es configurar una conexión de Google Cloud en Airflow (Admin -> Connections) o usar Variables de Airflow para el ID del proyecto y la ruta del archivo de credenciales, y modificar las funciones Python para usar estas configuraciones. Si mantienes el enfoque de variable de entorno, asegúrate de que la clave JSON esté accesible en el sistema de archivos de los workers de Airflow en Composer y que la variable de entorno `GOOGLE_APPLICATION_CREDENTIALS` esté configurada para apuntar a esa ruta en el entorno de ejecución de Airflow.\n",
        "4.  **Cargar el Archivo del DAG:** Sube el archivo `presionfiscal_etl_dag.py` a la carpeta `dags` del bucket de Cloud Storage asociado a tu entorno de Composer. Puedes encontrar el nombre del bucket en la página de detalles del entorno de Composer. Utiliza Cloud Storage Browser en la consola de GCP o gsutil para subir el archivo.\n",
        "5.  **Verificar en la UI de Airflow:** Refresca la página de la UI de Airflow. El DAG `presionfiscal_etl` debería aparecer en la lista (puede tardar unos minutos en ser detectado). Si hay errores de sintaxis, el DAG no aparecerá y deberás revisar los logs del scheduler de Airflow en Cloud Logging.\n",
        "6.  **Activar el DAG:** Asegúrate de que el interruptor del DAG esté activado en la UI de Airflow.\n",
        "7.  **Ejecutar el DAG:** Haz clic en el nombre del DAG y luego en el botón \"Trigger DAG\" para iniciar una ejecución manual.\n",
        "8.  **Monitorizar la Ejecución:** Observa el progreso de las tareas en la vista \"Graph View\" o \"Tree View\". Los logs de cada tarea (`Log` en el menú desplegable de la instancia de tarea) son cruciales para depurar problemas.\n",
        "9.  **Verificar Resultados en BigQuery:** Una vez que el DAG complete su ejecución exitosamente, dirígete a la consola de Google Cloud, navega a BigQuery y verifica que las tablas `Dim_Geografia`, `Dim_Tiempo`, `Dim_TipoPresion` y `Fact_PresionFiscal` existan dentro del dataset `sri_presion_fiscal` en el proyecto `etl-sistemas-sri` y que contengan los datos esperados.\n",
        "\n",
        "\"\"\"\n",
        "print(documentation_content)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## Documentación del Proceso ETL y Definición del DAG de Airflow\n",
            "\n",
            "Este documento detalla los pasos exitosos seguidos para construir un pipeline ETL que descarga datos de presión fiscal desde una URL, los transforma y los carga en un esquema estrella en Google BigQuery utilizando Apache Airflow.\n",
            "\n",
            "El código completo del DAG se encuentra en el archivo `presionfiscal_etl_dag.py`, generado previamente en la celda `948eccef`.\n",
            "\n",
            "### Fase 1: Definición Básica del DAG y Tarea de Descarga\n",
            "\n",
            "Se definió la estructura básica del DAG de Airflow `presionfiscal_etl` y una tarea para descargar el archivo CSV de origen.\n",
            "\n",
            "**Configuración del DAG:**\n",
            "\n",
            "*   **dag_id:** `presionfiscal_etl`\n",
            "*   **default_args:** (Ver código completo del DAG)\n",
            "*   **description:** 'ETL pipeline to download, transform, and load SRI tax pressure data to BigQuery'\n",
            "*   **schedule:** `None` (para ejecución manual)\n",
            "*   **catchup:** `False`\n",
            "*   **max_active_runs:** `1`\n",
            "*   **tags:** `['etl', 'bigquery', 'sri', 'presion_fiscal']`\n",
            "\n",
            "**Tareas:**\n",
            "\n",
            "*   `start`: `EmptyOperator` para marcar el inicio.\n",
            "*   `download_source_csv`: `PythonOperator` que ejecuta la función `download_csv_from_url` para descargar el CSV.\n",
            "*   `end`: `EmptyOperator` para marcar el fin (conectado al final del pipeline).\n",
            "\n",
            "**Código:** Referirse a las secciones de importaciones, `default_args`, definición del DAG (`with DAG(...)`), función `download_csv_from_url`, y definición de las tareas `start`, `download_source_csv` y `end` en el archivo `presionfiscal_etl_dag.py`.\n",
            "\n",
            "### Fase 2: ETL de Dimensiones\n",
            "\n",
            "Se definieron las tablas de dimensiones (`Dim_Geografia`, `Dim_Tiempo`, `Dim_TipoPresion`) y se implementó la lógica para extraer datos únicos de la tabla de origen en BigQuery, generar claves subrogadas y cargar los datos en las respectivas tablas de dimensión en BigQuery.\n",
            "\n",
            "**Tablas de Dimensiones y Esquemas:**\n",
            "\n",
            "*   `Dim_Geografia`: `id_geografia` (INT64, REQUIRED), `pais` (STRING, REQUIRED), `region` (STRING, REQUIRED)\n",
            "*   `Dim_Tiempo`: `tiempo_id` (INT64, REQUIRED), `año` (INT64, REQUIRED)\n",
            "*   `Dim_TipoPresion`: `tipo_presion_id` (INT64, REQUIRED), `tipo_presion_nombre` (STRING, REQUIRED)\n",
            "\n",
            "Se utilizó el cliente de BigQuery autenticado con un archivo de credenciales de servicio y se aseguró que el dataset y las tablas de dimensiones existieran o fueran creadas en el proyecto `etl-sistemas-sri`.\n",
            "\n",
            "**Tareas ETL de Dimensiones:**\n",
            "\n",
            "*   `process_dim_geografia`: `PythonOperator` que ejecuta la función `process_dim_geografia`.\n",
            "*   `process_dim_tiempo`: `PythonOperator` que ejecuta la función `process_dim_tiempo`.\n",
            "*   `process_dim_tipopresion`: `PythonOperator` que ejecuta la función `process_dim_tipopresion`.\n",
            "\n",
            "**Código:** Referirse a la definición de los esquemas de las tablas de dimensiones, la función `create_or_get_table`, las funciones Python `process_dim_geografia`, `process_dim_tiempo`, `process_dim_tipopresion` y la instanciación de los `PythonOperator` correspondientes en el archivo `presionfiscal_etl_dag.py`.\n",
            "\n",
            "### Fase 3: ETL de Hechos\n",
            "\n",
            "Se definió la tabla de hechos (`Fact_PresionFiscal`) y se implementó la lógica para extraer los datos transaccionales de la tabla de origen en BigQuery, realizar lookups con las tablas de dimensiones (también en BigQuery) para obtener las claves subrogadas, y cargar los datos transformados en la tabla de hechos en BigQuery.\n",
            "\n",
            "**Tabla de Hechos y Esquema:**\n",
            "\n",
            "*   `Fact_PresionFiscal`: `id_geografia` (INT64, REQUIRED), `tiempo_id` (INT64, REQUIRED), `tipo_presion_id` (INT64, REQUIRED), `presion_fiscal` (FLOAT64, NULLABLE)\n",
            "\n",
            "Se aseguró que la tabla de hechos existiera o fuera creada con el esquema correcto en el dataset `sri_presion_fiscal` del proyecto `etl-sistemas-sri`.\n",
            "\n",
            "**Tarea ETL de Hechos:**\n",
            "\n",
            "*   `process_fact_presionfiscal`: `PythonOperator` que ejecuta la función `process_fact_presionfiscal`.\n",
            "\n",
            "**Código:** Referirse a la definición del esquema de la tabla de hechos, la función Python `process_fact_presionfiscal` y la instanciación del `PythonOperator` correspondiente en el archivo `presionfiscal_etl_dag.py`.\n",
            "\n",
            "### Fase 4: Definición de Tareas y Dependencias del DAG\n",
            "\n",
            "Se definieron todas las tareas y se establecieron las dependencias para asegurar el orden de ejecución correcto:\n",
            "\n",
            "`start` >> `download_source_csv` >> `[process_dim_geografia, process_dim_tiempo, process_dim_tipopresion]` >> `process_fact_presionfiscal` >> `end`\n",
            "\n",
            "Esto garantiza que la descarga se complete antes de iniciar el ETL de dimensiones, que todo el ETL de dimensiones se complete antes de iniciar el ETL de hechos (ya que la tabla de hechos depende de las claves subrogadas de las dimensiones), y que la tarea final se ejecute solo después de que el ETL de hechos haya terminado.\n",
            "\n",
            "**Código:** Referirse a la sección donde se definen las dependencias entre las tareas utilizando el operador `>>` en el archivo `presionfiscal_etl_dag.py`.\n",
            "\n",
            "### Archivo Completo del DAG\n",
            "\n",
            "El código completo del DAG, combinando todas las importaciones, variables de configuración, definiciones de funciones Python y la estructura del DAG con tareas y dependencias, se encuentra en el archivo `presionfiscal_etl_dag.py`.\n",
            "\n",
            "*(El contenido de `presionfiscal_etl_dag.py` fue generado y mostrado en la celda `948eccef`)*\n",
            "\n",
            "### Instrucciones para Usar el DAG en Airflow (Google Cloud Composer)\n",
            "\n",
            "1.  **Crear/Verificar Entorno de Composer:** Asegúrate de que tu entorno de Google Cloud Composer (`etl-composer` u otro nombre) se haya creado y esté en estado \"Running\" en la consola de Google Cloud.\n",
            "2.  **Acceder a la UI de Airflow:** Utiliza el enlace proporcionado en la página de detalles de tu entorno de Composer en la consola de Google Cloud para acceder a la interfaz de usuario web de Airflow.\n",
            "3.  **Configurar Credenciales (Opcional pero Recomendado):** Aunque el DAG actual usa la variable de entorno `GOOGLE_APPLICATION_CREDENTIALS`, la práctica recomendada en producción es configurar una conexión de Google Cloud en Airflow (Admin -> Connections) o usar Variables de Airflow para el ID del proyecto y la ruta del archivo de credenciales, y modificar las funciones Python para usar estas configuraciones. Si mantienes el enfoque de variable de entorno, asegúrate de que la clave JSON esté accesible en el sistema de archivos de los workers de Airflow en Composer y que la variable de entorno `GOOGLE_APPLICATION_CREDENTIALS` esté configurada para apuntar a esa ruta en el entorno de ejecución de Airflow.\n",
            "4.  **Cargar el Archivo del DAG:** Sube el archivo `presionfiscal_etl_dag.py` a la carpeta `dags` del bucket de Cloud Storage asociado a tu entorno de Composer. Puedes encontrar el nombre del bucket en la página de detalles del entorno de Composer. Utiliza Cloud Storage Browser en la consola de GCP o gsutil para subir el archivo.\n",
            "5.  **Verificar en la UI de Airflow:** Refresca la página de la UI de Airflow. El DAG `presionfiscal_etl` debería aparecer en la lista (puede tardar unos minutos en ser detectado). Si hay errores de sintaxis, el DAG no aparecerá y deberás revisar los logs del scheduler de Airflow en Cloud Logging.\n",
            "6.  **Activar el DAG:** Asegúrate de que el interruptor del DAG esté activado en la UI de Airflow.\n",
            "7.  **Ejecutar el DAG:** Haz clic en el nombre del DAG y luego en el botón \"Trigger DAG\" para iniciar una ejecución manual.\n",
            "8.  **Monitorizar la Ejecución:** Observa el progreso de las tareas en la vista \"Graph View\" o \"Tree View\". Los logs de cada tarea (`Log` en el menú desplegable de la instancia de tarea) son cruciales para depurar problemas.\n",
            "9.  **Verificar Resultados en BigQuery:** Una vez que el DAG complete su ejecución exitosamente, dirígete a la consola de Google Cloud, navega a BigQuery y verifica que las tablas `Dim_Geografia`, `Dim_Tiempo`, `Dim_TipoPresion` y `Fact_PresionFiscal` existan dentro del dataset `sri_presion_fiscal` en el proyecto `etl-sistemas-sri` y que contengan los datos esperados.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f232dbe"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "## Documentación del Proceso ETL y Definición del DAG de Airflow\n",
        "\n",
        "Este documento detalla los pasos exitosos seguidos para construir un pipeline ETL que descarga datos de presión fiscal desde una URL, los transforma y los carga en un esquema estrella en Google BigQuery utilizando Apache Airflow.\n",
        "\n",
        "El código completo del DAG se encuentra en el archivo `presionfiscal_etl_dag.py`, generado previamente en la celda `948eccef`.\n",
        "\n",
        "### Fase 1: Definición Básica del DAG y Tarea de Descarga\n",
        "\n",
        "Se definió la estructura básica del DAG de Airflow `presionfiscal_etl` y una tarea para descargar el archivo CSV de origen.\n",
        "\n",
        "**Configuración del DAG:**\n",
        "\n",
        "*   **dag_id:** `presionfiscal_etl`\n",
        "*   **default_args:** (Ver código completo del DAG)\n",
        "*   **description:** 'ETL pipeline to download, transform, and load SRI tax pressure data to BigQuery'\n",
        "*   **schedule:** `None` (para ejecución manual)\n",
        "*   **catchup:** `False`\n",
        "*   **max_active_runs:** `1`\n",
        "*   **tags:** `['etl', 'bigquery', 'sri', 'presion_fiscal']`\n",
        "\n",
        "**Tareas:**\n",
        "\n",
        "*   `start`: `EmptyOperator` para marcar el inicio.\n",
        "*   `download_source_csv`: `PythonOperator` que ejecuta la función `download_csv_from_url` para descargar el CSV.\n",
        "*   `end`: `EmptyOperator` para marcar el fin (conectado al final del pipeline).\n",
        "\n",
        "**Código:** Referirse a las secciones de importaciones, `default_args`, definición del DAG (`with DAG(...)`), función `download_csv_from_url`, y definición de las tareas `start`, `download_source_csv` y `end` en el archivo `presionfiscal_etl_dag.py`.\n",
        "\n",
        "### Fase 2: ETL de Dimensiones\n",
        "\n",
        "Se definieron las tablas de dimensiones (`Dim_Geografia`, `Dim_Tiempo`, `Dim_TipoPresion`) y se implementó la lógica para extraer datos únicos de la tabla de origen en BigQuery, generar claves subrogadas y cargar los datos en las respectivas tablas de dimensión en BigQuery.\n",
        "\n",
        "**Tablas de Dimensiones y Esquemas:**\n",
        "\n",
        "*   `Dim_Geografia`: `id_geografia` (INT64, REQUIRED), `pais` (STRING, REQUIRED), `region` (STRING, REQUIRED)\n",
        "*   `Dim_Tiempo`: `tiempo_id` (INT64, REQUIRED), `año` (INT64, REQUIRED)\n",
        "*   `Dim_TipoPresion`: `tipo_presion_id` (INT64, REQUIRED), `tipo_presion_nombre` (STRING, REQUIRED)\n",
        "\n",
        "Se utilizó el cliente de BigQuery autenticado con un archivo de credenciales de servicio y se aseguró que el dataset y las tablas de dimensiones existieran o fueran creadas en el proyecto `etl-sistemas-sri`.\n",
        "\n",
        "**Tareas ETL de Dimensiones:**\n",
        "\n",
        "*   `process_dim_geografia`: `PythonOperator` que ejecuta la función `process_dim_geografia`.\n",
        "*   `process_dim_tiempo`: `PythonOperator` que ejecuta la función `process_dim_tiempo`.\n",
        "*   `process_dim_tipopresion`: `PythonOperator` que ejecuta la función `process_dim_tipopresion`.\n",
        "\n",
        "**Código:** Referirse a la definición de los esquemas de las tablas de dimensiones, la función `create_or_get_table`, las funciones Python `process_dim_geografia`, `process_dim_tiempo`, `process_dim_tipopresion` y la instanciación de los `PythonOperator` correspondientes en el archivo `presionfiscal_etl_dag.py`.\n",
        "\n",
        "### Fase 3: ETL de Hechos\n",
        "\n",
        "Se definió la tabla de hechos (`Fact_PresionFiscal`) y se implementó la lógica para extraer los datos transaccionales de la tabla de origen en BigQuery, realizar lookups con las tablas de dimensiones (también en BigQuery) para obtener las claves subrogadas, y cargar los datos transformados en la tabla de hechos en BigQuery.\n",
        "\n",
        "**Tabla de Hechos y Esquema:**\n",
        "\n",
        "*   `Fact_PresionFiscal`: `id_geografia` (INT64, REQUIRED), `tiempo_id` (INT64, REQUIRED), `tipo_presion_id` (INT64, REQUIRED), `presion_fiscal` (FLOAT64, NULLABLE)\n",
        "\n",
        "Se aseguró que la tabla de hechos existiera o fuera creada con el esquema correcto en el dataset `sri_presion_fiscal` del proyecto `etl-sistemas-sri`.\n",
        "\n",
        "**Tarea ETL de Hechos:**\n",
        "\n",
        "*   `process_fact_presionfiscal`: `PythonOperator` que ejecuta la función `process_fact_presionfiscal`.\n",
        "\n",
        "**Código:** Referirse a la definición del esquema de la tabla de hechos, la función Python `process_fact_presionfiscal` y la instanciación del `PythonOperator` correspondiente en el archivo `presionfiscal_etl_dag.py`.\n",
        "\n",
        "### Fase 4: Definición de Tareas y Dependencias del DAG\n",
        "\n",
        "Se definieron todas las tareas y se establecieron las dependencias para asegurar el orden de ejecución correcto:\n",
        "\n",
        "`start` >> `download_source_csv` >> `[process_dim_geografia, process_dim_tiempo, process_dim_tipopresion]` >> `process_fact_presionfiscal` >> `end`\n",
        "\n",
        "Esto garantiza que la descarga se complete antes de iniciar el ETL de dimensiones, que todo el ETL de dimensiones se complete antes de iniciar el ETL de hechos (ya que la tabla de hechos depende de las claves subrogadas de las dimensiones), y que la tarea final se ejecute solo después de que el ETL de hechos haya terminado.\n",
        "\n",
        "**Código:** Referirse a la sección donde se definen las dependencias entre las tareas utilizando el operador `>>` en el archivo `presionfiscal_etl_dag.py`.\n",
        "\n",
        "### Archivo Completo del DAG\n",
        "\n",
        "El código completo del DAG, combinando todas las importaciones, variables de configuración, definiciones de funciones Python y la estructura del DAG con tareas y dependencias, se encuentra en el archivo `presionfiscal_etl_dag.py`.\n",
        "\n",
        "*(El contenido de `presionfiscal_etl_dag.py` fue generado y mostrado en la celda `948eccef`)*\n",
        "\n",
        "### Instrucciones para Usar el DAG en Airflow (Google Cloud Composer)\n",
        "\n",
        "1.  **Crear/Verificar Entorno de Composer:** Asegúrate de que tu entorno de Google Cloud Composer (`etl-composer` u otro nombre) se haya creado y esté en estado \"Running\" en la consola de Google Cloud.\n",
        "2.  **Acceder a la UI de Airflow:** Utiliza el enlace proporcionado en la página de detalles de tu entorno de Composer en la consola de Google Cloud para acceder a la interfaz de usuario web de Airflow.\n",
        "3.  **Configurar Credenciales (Opcional pero Recomendado):** Aunque el DAG actual usa la variable de entorno `GOOGLE_APPLICATION_CREDENTIALS`, la práctica recomendada en producción es configurar una conexión de Google Cloud en Airflow (Admin -> Connections) o usar Variables de Airflow para el ID del proyecto y la ruta del archivo de credenciales, y modificar las funciones Python para usar estas configuraciones. Si mantienes el enfoque de variable de entorno, asegúrate de que la clave JSON esté accesible en el sistema de archivos de los workers de Airflow en Composer y que la variable de entorno `GOOGLE_APPLICATION_CREDENTIALS` esté configurada para apuntar a esa ruta en el entorno de ejecución de Airflow.\n",
        "4.  **Cargar el Archivo del DAG:** Sube el archivo `presionfiscal_etl_dag.py` a la carpeta `dags` del bucket de Cloud Storage asociado a tu entorno de Composer. Puedes encontrar el nombre del bucket en la página de detalles del entorno de Composer. Utiliza Cloud Storage Browser en la consola de GCP o gsutil para subir el archivo.\n",
        "5.  **Verificar en la UI de Airflow:** Refresca la página de la UI de Airflow. El DAG `presionfiscal_etl` debería aparecer en la lista (puede tardar unos minutos en ser detectado). Si hay errores de sintaxis, el DAG no aparecerá y deberás revisar los logs del scheduler de Airflow en Cloud Logging.\n",
        "6.  **Activar el DAG:** Asegúrate de que el interruptor del DAG esté activado en la UI de Airflow.\n",
        "7.  **Ejecutar el DAG:** Haz clic en el nombre del DAG y luego en el botón \"Trigger DAG\" para iniciar una ejecución manual.\n",
        "8.  **Monitorizar la Ejecución:** Observa el progreso de las tareas en la vista \"Graph View\" o \"Tree View\". Los logs de cada tarea (`Log` en el menú desplegable de la instancia de tarea) son cruciales para depurar problemas.\n",
        "9.  **Verificar Resultados en BigQuery:** Una vez que el DAG complete su ejecución exitosamente, dirígete a la consola de Google Cloud, navega a BigQuery y verifica que las tablas `Dim_Geografia`, `Dim_Tiempo`, `Dim_TipoPresion` y `Fact_PresionFiscal` existan dentro del dataset `sri_presion_fiscal` en el proyecto `etl-sistemas-sri` y que contengan los datos esperados.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   Successful code blocks for the ETL process and Airflow DAG definition were identified across multiple notebook cells, including those defining the DAG structure, BigQuery schemas, ETL logic for dimensions and facts, and task dependencies.\n",
        "*   The successful generation of the complete DAG file (`presionfiscal_etl_dag.py`) facilitated the documentation process by providing a single point of reference for the code.\n",
        "*   The documentation outlines the ETL process in four key phases: basic DAG definition and download task, dimension ETL, fact ETL, and task/dependency definition.\n",
        "*   Instructions for deploying and running the DAG on Google Cloud Composer were included in the documentation.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Enhance the documentation by adding more detailed explanations of the Python functions used for ETL within the DAG file.\n",
        "*   Refine the instructions for credential management in Airflow, emphasizing best practices for production environments like using Airflow Connections or Variables instead of relying solely on environment variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8040873f"
      },
      "source": [
        "# Documentación Consolidada del Proceso ETL y Definición del DAG de Airflow\n",
        "\n",
        "Este documento presenta el código final y la documentación detallada del pipeline ETL para cargar datos de presión fiscal del SRI en Google BigQuery, organizado por fases. Se han incluido explicaciones y comentarios en español para facilitar su comprensión y mantenimiento.\n",
        "\n",
        "El pipeline realiza las siguientes operaciones:\n",
        "1.  Descarga el archivo CSV de origen desde una URL.\n",
        "2.  Carga los datos brutos a una tabla temporal en BigQuery (esto se realizó en pasos previos y no se incluye como tarea separada en el DAG final, ya que las funciones ETL leerán directamente de la tabla de origen previamente cargada).\n",
        "3.  Realiza el proceso de Extracción, Transformación y Carga (ETL) para las tablas de dimensiones (`Dim_Geografia`, `Dim_Tiempo`, `Dim_TipoPresion`).\n",
        "4.  Realiza el proceso de ETL para la tabla de hechos (`Fact_PresionFiscal`), combinando los datos de origen con las claves subrogadas de las dimensiones.\n",
        "5.  Define un DAG de Apache Airflow para orquestar estas tareas.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Importaciones y Configuración Global\n",
        "\n",
        "Este bloque contiene todas las importaciones de bibliotecas necesarias y las variables de configuración que se utilizan en las distintas partes del código ETL y la definición del DAG."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "id": "9f1d26db",
        "outputId": "5faa3f22-ecc1-4cd9-8054-ac8132f382f9"
      },
      "source": [
        "# Import necessary modules from Apache Airflow and other libraries\n",
        "from airflow import DAG # Objeto DAG para definir el flujo de trabajo\n",
        "from datetime import datetime, timedelta # Para especificar la fecha de inicio y el retraso de reintento\n",
        "import pandas as pd # Para la transformación de datos usando DataFrames\n",
        "from airflow.operators.python import PythonOperator # Operador para ejecutar funciones Python\n",
        "from airflow.operators.empty import EmptyOperator # Tareas de inicio y fin (reemplaza a DummyOperator)\n",
        "from google.cloud import bigquery # Para interactuar con la API de Google BigQuery\n",
        "import os # Para acceder a variables de entorno (aunque se usa ruta directa para credenciales)\n",
        "from google.oauth2 import service_account # Para cargar credenciales de cuenta de servicio de Google Cloud\n",
        "import requests # Para descargar el archivo CSV\n",
        "import logging # Para logging\n",
        "\n",
        "# Configurar logging para el DAG\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- Variables de Configuración Global ---\n",
        "# Define los parámetros de BigQuery (usando el proyecto y dataset de pasos exitosos previos)\n",
        "PROJECT_ID = \"etl-sistemas-sri\" # ID de tu proyecto de Google Cloud\n",
        "DATASET_ID = \"sri_presion_fiscal\" # ID de tu Dataset en BigQuery\n",
        "# ID de la tabla de origen en BigQuery donde se cargaron los datos brutos del CSV\n",
        "SOURCE_TABLE_ID = f\"{PROJECT_ID}.{DATASET_ID}.sri_presion_fiscal\"\n",
        "\n",
        "# IDs de las tablas de dimensión y hechos en BigQuery\n",
        "table_id_dim_geografia = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Geografia\"\n",
        "table_id_dim_tiempo = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Tiempo\"\n",
        "table_id_dim_tipopresion = f\"{PROJECT_ID}.{DATASET_ID}.Dim_TipoPresion\"\n",
        "table_id_fact = f\"{PROJECT_ID}.{DATASET_ID}.Fact_PresionFiscal\"\n",
        "\n",
        "# Nombre local para el archivo CSV descargado (usado por la tarea de descarga)\n",
        "local_filename = \"SRI_Presion_Fiscal.csv\"\n",
        "\n",
        "# URL para el archivo CSV de origen\n",
        "csv_url = \"https://www.sri.gob.ec/o/sri-portlet-biblioteca-alfresco-internet/descargar/7e45627e-1f7e-4e21-ae59-d520634fc63f/SRI_Presion_Fiscal.csv\"\n",
        "\n",
        "# Ruta al archivo de credenciales de la cuenta de servicio (proporcionada por el usuario)\n",
        "# IMPORTANTE: Para entornos de producción de Airflow, considera usar Airflow Connections\n",
        "# para la autenticación de Google Cloud en lugar de depender de una ruta de archivo en el archivo DAG.\n",
        "# Asegúrate de que este archivo sea accesible para tus workers de Airflow.\n",
        "credential_filepath = \"/etl-sistemas-sri-cc64af72be76.json\" # Usa la ruta completa a tu archivo JSON de credenciales"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-07-01T02:28:52.676+0000] {utils.py:164} INFO - NumExpr defaulting to 2 threads.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m/tmp/\u001b[0m\u001b[1;33mipython-input-1-1090128645.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m5\u001b[0m\u001b[1;33m DeprecationWarning\u001b[0m\u001b[33m: The `airflow.operators.python.PythonOperator` class is deprecated. Please use `\u001b[0m\u001b[33m'airflow.providers.standard.operators.python.PythonOperator'\u001b[0m\u001b[33m`.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/tmp/ipython-input-1-1090128645.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">5</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> DeprecationWarning</span><span style=\"color: #808000; text-decoration-color: #808000\">: The `airflow.operators.python.PythonOperator` class is deprecated. Please use `</span><span style=\"color: #808000; text-decoration-color: #808000\">'airflow.providers.standard.operators.python.PythonOperator'</span><span style=\"color: #808000; text-decoration-color: #808000\">`.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m/tmp/\u001b[0m\u001b[1;33mipython-input-1-1090128645.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m6\u001b[0m\u001b[1;33m DeprecationWarning\u001b[0m\u001b[33m: The `airflow.operators.empty.EmptyOperator` class is deprecated. Please use `\u001b[0m\u001b[33m'airflow.providers.standard.operators.empty.EmptyOperator'\u001b[0m\u001b[33m`.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/tmp/ipython-input-1-1090128645.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">6</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> DeprecationWarning</span><span style=\"color: #808000; text-decoration-color: #808000\">: The `airflow.operators.empty.EmptyOperator` class is deprecated. Please use `</span><span style=\"color: #808000; text-decoration-color: #808000\">'airflow.providers.standard.operators.empty.EmptyOperator'</span><span style=\"color: #808000; text-decoration-color: #808000\">`.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cf243561",
        "outputId": "a078a539-575b-4d1c-af8d-3855459ed98b"
      },
      "source": [
        "!pip install apache-airflow google-cloud-bigquery pandas requests google-auth google-auth-oauthlib google-auth-httplib2"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting apache-airflow\n",
            "  Downloading apache_airflow-3.0.2-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: google-cloud-bigquery in /usr/local/lib/python3.11/dist-packages (3.34.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.11/dist-packages (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Collecting apache-airflow-core==3.0.2 (from apache-airflow)\n",
            "  Downloading apache_airflow_core-3.0.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting apache-airflow-task-sdk<1.1.0,>=1.0.0 (from apache-airflow)\n",
            "  Downloading apache_airflow_task_sdk-1.0.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting a2wsgi>=1.10.8 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading a2wsgi-1.10.10-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting aiosqlite>=0.20.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting alembic<2.0,>=1.13.1 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting apache-airflow-providers-common-compat>=1.6.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading apache_airflow_providers_common_compat-1.7.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting apache-airflow-providers-common-io>=1.5.3 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading apache_airflow_providers_common_io-1.6.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting apache-airflow-providers-common-sql>=1.26.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading apache_airflow_providers_common_sql-1.27.2-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting apache-airflow-providers-smtp>=2.0.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading apache_airflow_providers_smtp-2.1.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting apache-airflow-providers-standard>=0.4.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading apache_airflow_providers_standard-1.3.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting argcomplete>=1.10 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading argcomplete-3.6.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting asgiref>=2.3.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: attrs!=25.2.0,>=22.1.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (25.3.0)\n",
            "Collecting cadwyn>=5.2.1 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading cadwyn-5.4.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting colorlog>=6.8.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting cron-descriptor>=1.2.24 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading cron_descriptor-1.4.5-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting croniter>=2.0.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading croniter-6.0.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: cryptography>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (43.0.3)\n",
            "Collecting deprecated>=1.2.13 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: dill>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (0.3.7)\n",
            "Requirement already satisfied: fastapi!=0.115.10,>=0.115.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow) (0.115.13)\n",
            "Requirement already satisfied: flask>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (3.1.1)\n",
            "Collecting gunicorn>=20.1.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.5 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (8.7.0)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.5 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=4.19.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (4.24.0)\n",
            "Collecting lazy-object-proxy>=1.2.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading lazy_object_proxy-1.11.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting libcst>=1.1.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading libcst-1.8.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: linkify-it-py>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.0.3)\n",
            "Collecting lockfile>=0.12.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading lockfile-0.12.2-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting methodtools>=0.4.7 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading methodtools-0.4.7-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting opentelemetry-api>=1.26.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp>=1.26.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_exporter_otlp-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (24.2)\n",
            "Collecting pathspec>=0.9.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pendulum<4.0,>=2.1.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading pendulum-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: pluggy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (1.6.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (5.9.5)\n",
            "Requirement already satisfied: pydantic>=2.11.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.11.7)\n",
            "Requirement already satisfied: pygments!=2.19.0,>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.19.2)\n",
            "Requirement already satisfied: pyjwt>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.10.1)\n",
            "Collecting python-daemon>=3.0.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading python_daemon-3.1.2-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify>=5.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (8.0.4)\n",
            "Collecting rich-argparse>=1.0.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading rich_argparse-1.7.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: rich>=13.6.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (13.9.4)\n",
            "Requirement already satisfied: setproctitle>=1.3.3 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (1.3.6)\n",
            "Collecting sqlalchemy-jsonfield>=1.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading SQLAlchemy_JSONField-1.0.2-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting sqlalchemy-utils>=0.41.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading SQLAlchemy_Utils-0.41.2-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting sqlalchemy<2.0,>=1.4.49 (from sqlalchemy[asyncio]<2.0,>=1.4.49->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading SQLAlchemy-1.4.54-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting svcs>=25.1.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading svcs-25.1.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (0.9.0)\n",
            "Requirement already satisfied: tenacity!=8.2.0,>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (8.5.0)\n",
            "Requirement already satisfied: termcolor>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-core==3.0.2->apache-airflow) (3.1.0)\n",
            "Collecting typing-extensions!=4.14.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting universal-pathlib!=0.2.4,>=0.2.2 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading universal_pathlib-0.2.6-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting uuid6>=2024.7.10 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading uuid6-2025.0.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=2.11.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (2.25.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery) (2.7.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib) (2.0.0)\n",
            "Requirement already satisfied: httplib2>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-httplib2) (0.22.0)\n",
            "Collecting aiologic>=0.14.0 (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow)\n",
            "  Downloading aiologic-0.14.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow) (2025.3.2)\n",
            "Collecting msgspec>=0.19.0 (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow)\n",
            "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting psutil>=5.8.0 (from apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting retryhttp!=1.3.0,>=1.2.0 (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow)\n",
            "  Downloading retryhttp-1.3.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting structlog>=25.4.0 (from apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow)\n",
            "  Downloading structlog-25.4.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.26.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.73.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.71.0)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-resumable-media<3.0.0,>=2.0.0->google-cloud-bigquery) (1.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2>=0.19.0->google-auth-httplib2) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7.0->apache-airflow-core==3.0.2->apache-airflow) (1.17.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.3.1)\n",
            "Requirement already satisfied: wrapt>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from aiologic>=0.14.0->apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow) (1.17.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic<2.0,>=1.13.1->apache-airflow-core==3.0.2->apache-airflow) (1.1.3)\n",
            "Requirement already satisfied: sqlparse>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-common-sql>=1.26.0->apache-airflow-core==3.0.2->apache-airflow) (0.5.3)\n",
            "Requirement already satisfied: more-itertools>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-common-sql>=1.26.0->apache-airflow-core==3.0.2->apache-airflow) (10.7.0)\n",
            "Requirement already satisfied: starlette>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from cadwyn>=5.2.1->apache-airflow-core==3.0.2->apache-airflow) (0.46.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from cadwyn>=5.2.1->apache-airflow-core==3.0.2->apache-airflow) (0.4.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=41.0.0->apache-airflow-core==3.0.2->apache-airflow) (1.17.1)\n",
            "Collecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow) (0.0.20)\n",
            "Collecting email-validator>=2.0.0 (from fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow) (0.34.3)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask>=2.1.1->apache-airflow-core==3.0.2->apache-airflow) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=2.1.1->apache-airflow-core==3.0.2->apache-airflow) (8.2.1)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask>=2.1.1->apache-airflow-core==3.0.2->apache-airflow) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from flask>=2.1.1->apache-airflow-core==3.0.2->apache-airflow) (3.1.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->apache-airflow-core==3.0.2->apache-airflow) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->apache-airflow-core==3.0.2->apache-airflow) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->apache-airflow-core==3.0.2->apache-airflow) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.5->apache-airflow-core==3.0.2->apache-airflow) (3.23.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.1->apache-airflow-core==3.0.2->apache-airflow) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.1->apache-airflow-core==3.0.2->apache-airflow) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.1->apache-airflow-core==3.0.2->apache-airflow) (0.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.2 in /usr/local/lib/python3.11/dist-packages (from libcst>=1.1.0->apache-airflow-core==3.0.2->apache-airflow) (6.0.2)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.11/dist-packages (from linkify-it-py>=2.0.0->apache-airflow-core==3.0.2->apache-airflow) (1.0.3)\n",
            "Collecting wirerope>=0.4.7 (from methodtools>=0.4.7->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading wirerope-1.0.0-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc==1.34.1 (from opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http==1.34.1 (from opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.34.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk~=1.34.1 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.0->apache-airflow-core==3.0.2->apache-airflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.0->apache-airflow-core==3.0.2->apache-airflow) (2.33.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify>=5.0->apache-airflow-core==3.0.2->apache-airflow) (1.3)\n",
            "Collecting types-requests (from retryhttp!=1.3.0,>=1.2.0->apache-airflow-task-sdk<1.1.0,>=1.0.0->apache-airflow)\n",
            "  Downloading types_requests-2.32.4.20250611-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.6.0->apache-airflow-core==3.0.2->apache-airflow) (3.0.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<2.0,>=1.4.49->sqlalchemy[asyncio]<2.0,>=1.4.49->apache-airflow-core==3.0.2->apache-airflow) (3.2.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=41.0.0->apache-airflow-core==3.0.2->apache-airflow) (2.22)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow) (0.16.0)\n",
            "Collecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading rich_toolkit-0.14.8-py3-none-any.whl.metadata (999 bytes)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.6.0->apache-airflow-core==3.0.2->apache-airflow) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->apache-airflow-core==3.0.2->apache-airflow) (1.3.1)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow) (15.0.1)\n",
            "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk~=1.34.1->opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->apache-airflow-core==3.0.2->apache-airflow)\n",
            "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.115.10,>=0.115.0->apache-airflow-core==3.0.2->apache-airflow) (1.5.4)\n",
            "Downloading apache_airflow-3.0.2-py3-none-any.whl (12 kB)\n",
            "Downloading apache_airflow_core-3.0.2-py3-none-any.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading apache_airflow_task_sdk-1.0.2-py3-none-any.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.5/241.5 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading a2wsgi-1.10.10-py3-none-any.whl (17 kB)\n",
            "Downloading aiologic-0.14.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
            "Downloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading apache_airflow_providers_common_compat-1.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading apache_airflow_providers_common_io-1.6.0-py3-none-any.whl (19 kB)\n",
            "Downloading apache_airflow_providers_common_sql-1.27.2-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading apache_airflow_providers_smtp-2.1.0-py3-none-any.whl (21 kB)\n",
            "Downloading apache_airflow_providers_standard-1.3.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading argcomplete-3.6.2-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading cadwyn-5.4.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading cron_descriptor-1.4.5-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading croniter-6.0.0-py2.py3-none-any.whl (25 kB)\n",
            "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lazy_object_proxy-1.11.0-py3-none-any.whl (16 kB)\n",
            "Downloading libcst-1.8.2-cp311-cp311-manylinux_2_28_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lockfile-0.12.2-py2.py3-none-any.whl (13 kB)\n",
            "Downloading methodtools-0.4.7-py2.py3-none-any.whl (4.0 kB)\n",
            "Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp-1.34.1-py3-none-any.whl (7.0 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_http-1.34.1-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.34.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading pendulum-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (353 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m353.7/353.7 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_daemon-3.1.2-py3-none-any.whl (30 kB)\n",
            "Downloading retryhttp-1.3.3-py3-none-any.whl (17 kB)\n",
            "Downloading rich_argparse-1.7.1-py3-none-any.whl (25 kB)\n",
            "Downloading SQLAlchemy-1.4.54-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy_JSONField-1.0.2-py3-none-any.whl (10 kB)\n",
            "Downloading SQLAlchemy_Utils-0.41.2-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading structlog-25.4.0-py3-none-any.whl (68 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.7/68.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading svcs-25.1.0-py3-none-any.whl (19 kB)\n",
            "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading universal_pathlib-0.2.6-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uuid6-2025.0.0-py3-none-any.whl (7.0 kB)\n",
            "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\n",
            "Downloading wirerope-1.0.0-py2.py3-none-any.whl (9.2 kB)\n",
            "Downloading types_requests-2.32.4.20250611-py3-none-any.whl (20 kB)\n",
            "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading rich_toolkit-0.14.8-py3-none-any.whl (24 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lockfile, cron-descriptor, wirerope, uvloop, uuid6, universal-pathlib, typing-extensions, types-requests, svcs, structlog, sqlalchemy, python-dotenv, python-daemon, psutil, pathspec, opentelemetry-proto, msgspec, libcst, lazy-object-proxy, httptools, gunicorn, dnspython, deprecated, colorlog, asgiref, argcomplete, aiologic, a2wsgi, sqlalchemy-utils, sqlalchemy-jsonfield, pendulum, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, methodtools, email-validator, croniter, alembic, aiosqlite, watchfiles, rich-toolkit, rich-argparse, opentelemetry-semantic-conventions, retryhttp, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, fastapi-cli, cadwyn, opentelemetry-exporter-otlp, apache-airflow-providers-common-compat, apache-airflow-providers-standard, apache-airflow-providers-smtp, apache-airflow-providers-common-sql, apache-airflow-providers-common-io, apache-airflow-task-sdk, apache-airflow-core, apache-airflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.14.0\n",
            "    Uninstalling typing_extensions-4.14.0:\n",
            "      Successfully uninstalled typing_extensions-4.14.0\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.41\n",
            "    Uninstalling SQLAlchemy-2.0.41:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.41\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\n",
            "typeguard 4.4.4 requires typing_extensions>=4.14.0, but you have typing-extensions 4.13.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed a2wsgi-1.10.10 aiologic-0.14.0 aiosqlite-0.21.0 alembic-1.16.2 apache-airflow-3.0.2 apache-airflow-core-3.0.2 apache-airflow-providers-common-compat-1.7.1 apache-airflow-providers-common-io-1.6.0 apache-airflow-providers-common-sql-1.27.2 apache-airflow-providers-smtp-2.1.0 apache-airflow-providers-standard-1.3.0 apache-airflow-task-sdk-1.0.2 argcomplete-3.6.2 asgiref-3.8.1 cadwyn-5.4.2 colorlog-6.9.0 cron-descriptor-1.4.5 croniter-6.0.0 deprecated-1.2.18 dnspython-2.7.0 email-validator-2.2.0 fastapi-cli-0.0.7 gunicorn-23.0.0 httptools-0.6.4 lazy-object-proxy-1.11.0 libcst-1.8.2 lockfile-0.12.2 methodtools-0.4.7 msgspec-0.19.0 opentelemetry-api-1.34.1 opentelemetry-exporter-otlp-1.34.1 opentelemetry-exporter-otlp-proto-common-1.34.1 opentelemetry-exporter-otlp-proto-grpc-1.34.1 opentelemetry-exporter-otlp-proto-http-1.34.1 opentelemetry-proto-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 pathspec-0.12.1 pendulum-3.1.0 psutil-7.0.0 python-daemon-3.1.2 python-dotenv-1.1.1 retryhttp-1.3.3 rich-argparse-1.7.1 rich-toolkit-0.14.8 sqlalchemy-1.4.54 sqlalchemy-jsonfield-1.0.2 sqlalchemy-utils-0.41.2 structlog-25.4.0 svcs-25.1.0 types-requests-2.32.4.20250611 typing-extensions-4.13.2 universal-pathlib-0.2.6 uuid6-2025.0.0 uvloop-0.21.0 watchfiles-1.1.0 wirerope-1.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              },
              "id": "54a7ce56484546098b65a4c4adc64ccc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b0a61f6"
      },
      "source": [
        "## 2. Funciones Python para las Tareas ETL\n",
        "\n",
        "Esta sección contiene las definiciones de las funciones Python que se ejecutarán como tareas individuales dentro del DAG de Airflow. Cada función encapsula la lógica necesaria para una etapa específica del proceso ETL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf892ac9"
      },
      "source": [
        "# --- Define Python Functions for ETL Tasks ---\n",
        "\n",
        "def download_csv_from_url(csv_url, local_filename):\n",
        "    \"\"\"\n",
        "    Descarga un archivo CSV desde una URL dada y lo guarda localmente.\n",
        "\n",
        "    Args:\n",
        "        csv_url (str): La URL del archivo CSV a descargar.\n",
        "        local_filename (str): El nombre del archivo local donde se guardará el CSV.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Intentando descargar archivo desde: {csv_url}\")\n",
        "    try:\n",
        "        response = requests.get(csv_url)\n",
        "        # Lanza un HTTPError para respuestas de error (4xx o 5xx)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Abre el archivo local en modo binario de escritura y guarda el contenido\n",
        "        with open(local_filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        logger.info(f\"Archivo '{local_filename}' descargado exitosamente.\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error al descargar el archivo: {e}\")\n",
        "        # Lanzar una excepción para que la tarea falle en Airflow\n",
        "        raise ValueError(f\"Fallo la tarea de descarga: {e}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Ocurrió un error inesperado durante la descarga: {e}\")\n",
        "        raise ValueError(f\"Fallo la tarea de descarga inesperadamente: {e}\")\n",
        "\n",
        "\n",
        "def process_dim_geografia(credential_filepath=credential_filepath):\n",
        "    \"\"\"\n",
        "    Extrae combinaciones únicas de País y Región de la tabla de origen en BigQuery,\n",
        "    genera claves subrogadas y carga los datos en la tabla Dim_Geografia en BigQuery.\n",
        "\n",
        "    Args:\n",
        "        credential_filepath (str): Ruta al archivo de credenciales de la cuenta de servicio de GCP.\n",
        "    \"\"\"\n",
        "    logger.info(\"Iniciando tarea process_dim_geografia.\")\n",
        "\n",
        "    # Inicializar cliente de BigQuery dentro de la función usando las credenciales proporcionadas\n",
        "    try:\n",
        "        credentials = service_account.Credentials.from_service_account_file(credential_filepath)\n",
        "        client = bigquery.Client(project=PROJECT_ID, credentials=credentials)\n",
        "        logger.info(\"Cliente de BigQuery inicializado para ETL de Dim_Geografia.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error al inicializar cliente de BigQuery para ETL de Dim_Geografia: {e}\")\n",
        "        raise # Relanzar la excepción\n",
        "\n",
        "    # Definir esquema para Dim_Geografia (debe coincidir con la tabla en BigQuery)\n",
        "    schema_dim_geografia = [\n",
        "        bigquery.SchemaField(\"id_geografia\", \"INT64\", mode=\"REQUIRED\"), # Clave Primaria\n",
        "        bigquery.SchemaField(\"pais\", \"STRING\", mode=\"REQUIRED\"),      # País\n",
        "        bigquery.SchemaField(\"region\", \"STRING\", mode=\"REQUIRED\"),    # Región\n",
        "    ]\n",
        "\n",
        "    # Consulta SQL para seleccionar combinaciones distintas de País y Región\n",
        "    # Asumiendo que las filas de encabezado se manejaron durante la carga inicial a la tabla de origen\n",
        "    query_geografia = f\"\"\"\n",
        "    SELECT DISTINCT `País`, Region\n",
        "    FROM `{SOURCE_TABLE_ID}`\n",
        "    WHERE `País` IS NOT NULL\n",
        "      AND Region IS NOT NULL\n",
        "    \"\"\"\n",
        "\n",
        "    # Ejecutar la consulta y cargar resultados en un DataFrame de Pandas\n",
        "    try:\n",
        "        df_geografia = client.query(query_geografia).to_dataframe()\n",
        "        logger.info(f\"Extraídas {len(df_geografia)} combinaciones únicas País-Región para Dim_Geografia.\")\n",
        "\n",
        "        # Añadir columna id_geografia (clave subrogada)\n",
        "        # Ordenar por País y Región antes de generar el ID para consistencia\n",
        "        df_geografia = df_geografia.sort_values(by=['País', 'Region']).reset_index(drop=True)\n",
        "        # ID incremental simple basado en el índice (desde 1)\n",
        "        df_geografia['id_geografia'] = df_geografia.index + 1\n",
        "\n",
        "        # Renombrar columnas para que coincidan con el esquema de BigQuery\n",
        "        df_geografia.rename(columns={'País': 'pais', 'Region': 'region'}, inplace=True)\n",
        "\n",
        "        # Seleccionar solo las columnas requeridas y asegurar el orden\n",
        "        df_geografia = df_geografia[['id_geografia', 'pais', 'region']]\n",
        "\n",
        "        logger.info(\"Encabezado del DataFrame transformado de Dim_Geografia:\")\n",
        "        # display(df_geografia.head()) # Deshabilitado para logs limpios en Airflow\n",
        "        logger.info(\"Información del DataFrame transformado de Dim_Geografia:\")\n",
        "        # display(df_geografia.info()) # Deshabilitado para logs limpios en Airflow\n",
        "\n",
        "\n",
        "        # Configuración del trabajo de carga para BigQuery\n",
        "        job_config_geografia = bigquery.LoadJobConfig(\n",
        "            schema=schema_dim_geografia,\n",
        "            write_disposition=\"WRITE_TRUNCATE\", # Sobrescribir la tabla existente\n",
        "        )\n",
        "\n",
        "        # Cargar el DataFrame en la tabla de BigQuery\n",
        "        job_geografia = client.load_table_from_dataframe(\n",
        "            df_geografia, table_id_dim_geografia, job_config=job_config_geografia\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Iniciando trabajo de carga en BigQuery para Dim_Geografia: {job_geografia.job_id}\")\n",
        "        job_geografia.result() # Esperar a que el trabajo se complete\n",
        "        logger.info(f\"DataFrame cargado exitosamente en {table_id_dim_geografia}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error al procesar Dim_Geografia: {e}\")\n",
        "        raise # Relanzar la excepción\n",
        "    logger.info(\"Tarea process_dim_geografia finalizada.\")\n",
        "\n",
        "\n",
        "def process_dim_tiempo(credential_filepath=credential_filepath):\n",
        "    \"\"\"\n",
        "    Extrae años únicos de la tabla de origen en BigQuery,\n",
        "    genera claves subrogadas y carga los datos en la tabla Dim_Tiempo en BigQuery.\n",
        "\n",
        "    Args:\n",
        "        credential_filepath (str): Ruta al archivo de credenciales de la cuenta de servicio de GCP.\n",
        "    \"\"\"\n",
        "    logger.info(\"Iniciando tarea process_dim_tiempo.\")\n",
        "\n",
        "    # Inicializar cliente de BigQuery dentro de la función\n",
        "    try:\n",
        "        credentials = service_account.Credentials.from_service_account_file(credential_filepath)\n",
        "        client = bigquery.Client(project=PROJECT_ID, credentials=credentials)\n",
        "        logger.info(\"Cliente de BigQuery inicializado para ETL de Dim_Tiempo.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error al inicializar cliente de BigQuery para ETL de Dim_Tiempo: {e}\")\n",
        "        raise # Relanzar la excepción\n",
        "\n",
        "    # Definir esquema para Dim_Tiempo\n",
        "    schema_dim_tiempo = [\n",
        "        bigquery.SchemaField(\"tiempo_id\", \"INT64\", mode=\"REQUIRED\"), # Clave Primaria\n",
        "        bigquery.SchemaField(\"año\", \"INT64\", mode=\"REQUIRED\"),      # Año\n",
        "    ]\n",
        "\n",
        "    # Consulta SQL para seleccionar años distintos\n",
        "    query_tiempo = f\"\"\"\n",
        "    SELECT DISTINCT `Año`\n",
        "    FROM `{SOURCE_TABLE_ID}`\n",
        "    WHERE `Año` IS NOT NULL\n",
        "    \"\"\"\n",
        "\n",
        "    # Ejecutar la consulta y cargar resultados en un DataFrame\n",
        "    try:\n",
        "        df_tiempo = client.query(query_tiempo).to_dataframe()\n",
        "        logger.info(f\"Extraídos {len(df_tiempo)} años únicos para Dim_Tiempo.\")\n",
        "\n",
        "        # Añadir columna tiempo_id (clave subrogada)\n",
        "        # ID incremental simple basado en el rango de los años (ID denso)\n",
        "        df_tiempo['tiempo_id'] = df_tiempo['Año'].rank(method='dense').astype('Int64')\n",
        "\n",
        "        # Renombrar la columna de año para que coincida con el esquema\n",
        "        df_tiempo.rename(columns={'Año': 'año'}, inplace=True)\n",
        "\n",
        "        # Seleccionar solo las columnas requeridas y asegurar el orden\n",
        "        df_tiempo = df_tiempo[['tiempo_id', 'año']]\n",
        "\n",
        "        logger.info(\"Encabezado del DataFrame transformado de Dim_Tiempo:\")\n",
        "        # display(df_tiempo.head()) # Deshabilitado para logs limpios en Airflow\n",
        "        logger.info(\"Información del DataFrame transformado de Dim_Tiempo:\")\n",
        "        # display(df_tiempo.info()) # Deshabilitado para logs limpios en Airflow\n",
        "\n",
        "        # Configuración del trabajo de carga para BigQuery\n",
        "        job_config_tiempo = bigquery.LoadJobConfig(\n",
        "            schema=schema_dim_tiempo,\n",
        "            write_disposition=\"WRITE_TRUNCATE\",\n",
        "        )\n",
        "\n",
        "        # Cargar el DataFrame en la tabla de BigQuery\n",
        "        job_tiempo = client.load_table_from_dataframe(\n",
        "            df_tiempo, table_id_dim_tiempo, job_config=job_config_tiempo\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Iniciando trabajo de carga en BigQuery para Dim_Tiempo: {job_tiempo.job_id}\")\n",
        "        job_tiempo.result()\n",
        "        logger.info(f\"DataFrame cargado exitosamente en {table_id_dim_tiempo}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error al procesar Dim_Tiempo: {e}\")\n",
        "        raise # Relanzar la excepción\n",
        "    logger.info(\"Tarea process_dim_tiempo finalizada.\")\n",
        "\n",
        "\n",
        "def process_dim_tipopresion(credential_filepath=credential_filepath):\n",
        "    \"\"\"\n",
        "    Extrae valores únicos de Tipo_Presion de la tabla de origen en BigQuery,\n",
        "    genera claves subrogadas y carga los datos en la tabla Dim_TipoPresion en BigQuery.\n",
        "\n",
        "    Args:\n",
        "        credential_filepath (str): Ruta al archivo de credenciales de la cuenta de servicio de GCP.\n",
        "    \"\"\"\n",
        "    logger.info(\"Iniciando tarea process_dim_tipopresion.\")\n",
        "\n",
        "    # Inicializar cliente de BigQuery dentro de la función\n",
        "    try:\n",
        "        credentials = service_account.Credentials.from_service_account_file(credential_filepath)\n",
        "        client = bigquery.Client(project=PROJECT_ID, credentials=credentials)\n",
        "        logger.info(\"Cliente de BigQuery inicializado para ETL de Dim_TipoPresion.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error al inicializar cliente de BigQuery para ETL de Dim_TipoPresion: {e}\")\n",
        "        raise # Relanzar la excepción\n",
        "\n",
        "    # Definir esquema para Dim_TipoPresion\n",
        "    schema_dim_tipopresion = [\n",
        "        bigquery.SchemaField(\"tipo_presion_id\", \"INT64\", mode=\"REQUIRED\"), # Clave Primaria\n",
        "        bigquery.SchemaField(\"tipo_presion_nombre\", \"STRING\", mode=\"REQUIRED\"), # Nombre del Tipo de Presión\n",
        "    ]\n",
        "\n",
        "    # Consulta SQL para seleccionar valores distintos de Tipo_Presion\n",
        "    # Asumiendo que las filas de encabezado se manejaron durante la carga inicial a la tabla de origen\n",
        "    query_tipopresion = f\"\"\"\n",
        "    SELECT DISTINCT Tipo_Presion\n",
        "    FROM `{SOURCE_TABLE_ID}`\n",
        "    WHERE Tipo_Presion IS NOT NULL\n",
        "    \"\"\"\n",
        "\n",
        "    # Ejecutar la consulta y cargar resultados en un DataFrame\n",
        "    try:\n",
        "        df_tipopresion = client.query(query_tipopresion).to_dataframe()\n",
        "        logger.info(f\"Extraídos {len(df_tipopresion)} valores únicos de Tipo_Presion para Dim_TipoPresion.\")\n",
        "\n",
        "        # Añadir columna tipo_presion_id (clave subrogada)\n",
        "        df_tipopresion['tipo_presion_id'] = df_tipopresion.reset_index().index + 1 # ID incremental simple\n",
        "\n",
        "        # Renombrar la columna Tipo_Presion para que coincida con el esquema\n",
        "        df_tipopresion.rename(columns={'Tipo_Presion': 'tipo_presion_nombre'}, inplace=True)\n",
        "\n",
        "        # Seleccionar solo las columnas requeridas y asegurar el orden\n",
        "        df_tipopresion = df_tipopresion[['tipo_presion_id', 'tipo_presion_nombre']]\n",
        "\n",
        "        logger.info(\"Encabezado del DataFrame transformado de Dim_TipoPresion:\")\n",
        "        # display(df_tipopresion.head()) # Deshabilitado para logs limpios en Airflow\n",
        "        logger.info(\"Información del DataFrame transformado de Dim_TipoPresion:\")\n",
        "        # display(df_tipopresion.info()) # Deshabilitado para logs limpios en Airflow\n",
        "\n",
        "\n",
        "        # Configuración del trabajo de carga para BigQuery\n",
        "        job_config_tipopresion = bigquery.LoadJobConfig(\n",
        "            schema=schema_dim_tipopresion,\n",
        "            write_disposition=\"WRITE_TRUNCATE\",\n",
        "        )\n",
        "\n",
        "        # Cargar el DataFrame en la tabla de BigQuery\n",
        "        job_tipopresion = client.load_table_from_dataframe(\n",
        "            df_tipopresion, table_id_dim_tipopresion, job_config=job_config_tipopresion\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Iniciando trabajo de carga en BigQuery para Dim_TipoPresion: {job_tipopresion.job_id}\")\n",
        "        job_tipopresion.result()\n",
        "        logger.info(f\"DataFrame cargado exitosamente en {table_id_dim_tipopresion}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error al procesar Dim_TipoPresion: {e}\")\n",
        "        raise # Relanzar la excepción\n",
        "    logger.info(\"Tarea process_dim_tipopresion finalizada.\")\n",
        "\n",
        "\n",
        "def process_fact_presionfiscal(credential_filepath=credential_filepath):\n",
        "    \"\"\"\n",
        "    Extrae datos de la tabla de origen en BigQuery, realiza lookups con las tablas\n",
        "    de dimensiones (también en BigQuery) para obtener las claves subrogadas, y carga\n",
        "    los datos transformados en la tabla Fact_PresionFiscal en BigQuery.\n",
        "    Asume que las tablas de dimensiones ya están pobladas.\n",
        "\n",
        "    Args:\n",
        "        credential_filepath (str): Ruta al archivo de credenciales de la cuenta de servicio de GCP.\n",
        "    \"\"\"\n",
        "    logger.info(\"Iniciando tarea process_fact_presionfiscal.\")\n",
        "\n",
        "    # Inicializar cliente de BigQuery dentro de la función\n",
        "    try:\n",
        "        credentials = service_account.Credentials.from_service_account_file(credential_filepath)\n",
        "        client = bigquery.Client(project=PROJECT_ID, credentials=credentials)\n",
        "        logger.info(\"Cliente de BigQuery inicializado para ETL de Hechos.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error al inicializar cliente de BigQuery para ETL de Hechos: {e}\")\n",
        "        raise # Relanzar la excepción\n",
        "\n",
        "    # Definir esquema para Fact_PresionFiscal\n",
        "    schema_fact = [\n",
        "        bigquery.SchemaField(\"id_geografia\", \"INT64\", mode=\"REQUIRED\"), # Clave foránea a Dim_Geografia\n",
        "        bigquery.SchemaField(\"tiempo_id\", \"INT64\", mode=\"REQUIRED\"),   # Clave foránea a Dim_Tiempo\n",
        "        bigquery.SchemaField(\"tipo_presion_id\", \"INT64\", mode=\"REQUIRED\"), # Clave foránea a Dim_TipoPresion\n",
        "        bigquery.SchemaField(\"presion_fiscal\", \"FLOAT64\", mode=\"NULLABLE\"), # La métrica (porcentaje de presión fiscal)\n",
        "    ]\n",
        "\n",
        "    # Definir IDs de las tablas de dimensión (necesarias para los lookups)\n",
        "    table_id_dim_geografia_fact = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Geografia\"\n",
        "    table_id_dim_tiempo_fact = f\"{PROJECT_ID}.{DATASET_ID}.Dim_Tiempo\"\n",
        "    table_id_dim_tipopresion_fact = f\"{PROJECT_ID}.{DATASET_ID}.Dim_TipoPresion\"\n",
        "\n",
        "    # --- Extraer Datos de Origen de BigQuery ---\n",
        "    # Asumiendo que las filas de encabezado se manejaron durante la carga inicial a la tabla de origen\n",
        "    query_source_data = f\"\"\"\n",
        "    SELECT\n",
        "        Region,\n",
        "        `País`,\n",
        "        `Año`,\n",
        "        `%_Presion`,\n",
        "        Tipo_Presion\n",
        "    FROM `{SOURCE_TABLE_ID}`\n",
        "    WHERE Region IS NOT NULL\n",
        "      AND `País` IS NOT NULL\n",
        "      AND `Año` IS NOT NULL\n",
        "      AND `%_Presion` IS NOT NULL\n",
        "      AND Tipo_Presion IS NOT NULL\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        df_source_data = client.query(query_source_data).to_dataframe()\n",
        "        logger.info(f\"Extraídas {len(df_source_data)} filas de la tabla de origen para procesamiento de hechos.\")\n",
        "\n",
        "        # Asegurar que los tipos de datos sean correctos antes de combinar\n",
        "        # Convertir 'Año' a entero (Int64 para manejar posibles valores nulos)\n",
        "        df_source_data['Año'] = pd.to_numeric(df_source_data['Año'], errors='coerce').astype('Int64')\n",
        "        # Convertir '%_Presion' a flotante\n",
        "        df_source_data['%_Presion'] = pd.to_numeric(df_source_data['%_Presion'], errors='coerce')\n",
        "        logger.info(\"Tipos de datos de origen limpiados para combinación.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error al extraer datos de origen para ETL de Hechos: {e}\")\n",
        "        raise # Relanzar la excepción\n",
        "\n",
        "    # --- Extraer Datos de Dimensiones de BigQuery para Lookups ---\n",
        "    # Estas consultas asumen que las tablas de dimensiones existen y están pobladas.\n",
        "\n",
        "    # Consulta Dim_Geografia\n",
        "    query_dim_geografia = f\"\"\"\n",
        "    SELECT id_geografia, pais, region\n",
        "    FROM `{table_id_dim_geografia_fact}`\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df_dim_geografia = client.query(query_dim_geografia).to_dataframe()\n",
        "        logger.info(f\"Extraídas {len(df_dim_geografia)} filas de Dim_Geografia para lookup.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error al extraer datos de Dim_Geografia para ETL de Hechos: {e}\")\n",
        "        raise # Relanzar la excepción\n",
        "\n",
        "    # Consulta Dim_Tiempo\n",
        "    query_dim_tiempo = f\"\"\"\n",
        "    SELECT tiempo_id, año\n",
        "    FROM `{table_id_dim_tiempo_fact}`\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df_dim_tiempo = client.query(query_dim_tiempo).to_dataframe()\n",
        "        logger.info(f\"Extraídas {len(df_dim_tiempo)} filas de Dim_Tiempo para lookup.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error al extraer datos de Dim_Tiempo para ETL de Hechos: {e}\")\n",
        "        raise # Relanzar la excepción\n",
        "\n",
        "    # Consulta Dim_TipoPresion\n",
        "    query_dim_tipopresion = f\"\"\"\n",
        "    SELECT tipo_presion_id, tipo_presion_nombre\n",
        "    FROM `{table_id_dim_tipopresion_fact}`\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df_dim_tipopresion = client.query(query_dim_tipopresion).to_dataframe()\n",
        "        logger.info(f\"Extraídas {len(df_dim_tipopresion)} filas de Dim_TipoPresion para lookup.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error al extraer datos de Dim_TipoPresion para ETL de Hechos: {e}\")\n",
        "        raise # Relanzar la excepción\n",
        "    # --- Fin de Extracción de Datos de Dimensiones ---\n",
        "\n",
        "\n",
        "    # --- Realizar Lookups y Preparar Datos de Hechos ---\n",
        "    df_fact_staging = df_source_data.copy()\n",
        "\n",
        "    # Combinar con Dim_Geografia para obtener id_geografia\n",
        "    # Asegurar que los nombres de columna coincidan para la combinación\n",
        "    df_fact_staging = pd.merge(df_fact_staging, df_dim_geografia,\n",
        "                               left_on=['País', 'Region'], right_on=['pais', 'region'], how='left')\n",
        "\n",
        "    # Combinar con Dim_Tiempo para obtener tiempo_id\n",
        "    df_fact_staging = pd.merge(df_fact_staging, df_dim_tiempo,\n",
        "                               left_on='Año', right_on='año', how='left')\n",
        "\n",
        "    # Combinar con Dim_TipoPresion para obtener tipo_presion_id\n",
        "    df_fact_staging = pd.merge(df_fact_staging, df_dim_tipopresion,\n",
        "                               left_on='Tipo_Presion', right_on='tipo_presion_nombre', how='left')\n",
        "\n",
        "    # Verificar filas no coincidentes (donde los lookups resultaron en IDs nulos) - Opcional para depuración\n",
        "    unmatched_rows_geografia = df_fact_staging[df_fact_staging['id_geografia'].isnull()]\n",
        "    if not unmatched_rows_geografia.empty:\n",
        "        logger.warning(f\"Advertencia: {len(unmatched_rows_geografia)} filas no coincidieron en Dim_Geografia durante el lookup.\")\n",
        "\n",
        "    unmatched_rows_tiempo = df_fact_staging[df_fact_staging['tiempo_id'].isnull()]\n",
        "    if not unmatched_rows_tiempo.empty:\n",
        "        logger.warning(f\"Advertencia: {len(unmatched_rows_tiempo)} filas no coincidieron en Dim_Tiempo durante el lookup.\")\n",
        "\n",
        "    unmatched_rows_tipopresion = df_fact_staging[df_fact_staging['tipo_presion_id'].isnull()]\n",
        "    if not unmatched_rows_tipopresion.empty:\n",
        "        logger.warning(f\"Advertencia: {len(unmatched_rows_tipopresion)} filas no coincidieron en Dim_TipoPresion durante el lookup.\")\n",
        "\n",
        "\n",
        "    # Seleccionar y renombrar columnas para que coincidan con el esquema de la tabla de hechos\n",
        "    # Asegurar que todas las columnas de clave subrogada requeridas estén presentes después de las combinaciones\n",
        "    df_fact = df_fact_staging[['id_geografia', 'tiempo_id', 'tipo_presion_id', '%_Presion']].copy()\n",
        "    df_fact.rename(columns={'%_Presion': 'presion_fiscal'}, inplace=True)\n",
        "\n",
        "    # Asegurar que las columnas de clave subrogada sean de tipo Int64 (entero nullable)\n",
        "    # Esto es importante ya que las combinaciones left pueden introducir flotantes para columnas enteras con valores nulos\n",
        "    for col_id in ['id_geografia', 'tiempo_id', 'tipo_presion_id']:\n",
        "        if col_id in df_fact.columns:\n",
        "            # Convertir a Int64 para permitir posibles valores NaN de la combinación left\n",
        "            # Usar errors='coerce' por si acaso, aunque merge debería manejar esto si los tipos coinciden\n",
        "            df_fact[col_id] = pd.to_numeric(df_fact[col_id], errors='coerce').astype('Int64')\n",
        "\n",
        "    logger.info(\"Encabezado del DataFrame transformado de Hechos (con claves subrogadas):\")\n",
        "    # display(df_fact.head()) # Deshabilitado para logs limpios en Airflow\n",
        "    logger.info(\"Información del DataFrame transformado de Hechos:\")\n",
        "    # display(df_fact.info()) # Deshabilitado para logs limpios en Airflow\n",
        "\n",
        "    # --- Cargar a BigQuery ---\n",
        "    # Configuración del trabajo de carga para BigQuery\n",
        "    job_config_fact = bigquery.LoadJobConfig(\n",
        "        schema=schema_fact,\n",
        "        write_disposition=\"WRITE_TRUNCATE\", # Sobrescribir la tabla\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        job_fact = client.load_table_from_dataframe(\n",
        "            df_fact, table_id_fact, job_config=job_config_fact\n",
        "        )\n",
        "        logger.info(f\"Iniciando trabajo de carga en BigQuery para Fact_PresionFiscal: {job_fact.job_id}\")\n",
        "        job_fact.result() # Esperar a que el trabajo se complete\n",
        "        logger.info(f\"DataFrame cargado exitosamente en {table_id_fact}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error al cargar Fact_PresionFiscal en BigQuery: {e}\")\n",
        "        raise # Relanzar la excepción\n",
        "\n",
        "    logger.info(\"Tarea process_fact_presionfiscal finalizada.\")\n",
        "\n",
        "# Note: La función download_csv_from_url se define en la sección 2.\n",
        "# La definición del DAG y las tareas con dependencias se definen en la sección 3."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "142ff98e"
      },
      "source": [
        "## 3. Definición del DAG de Airflow, Tareas y Dependencias\n",
        "\n",
        "Esta sección define el objeto DAG de Airflow, instancia las tareas que ejecutan las funciones ETL definidas anteriormente y establece el flujo de trabajo (las dependencias) entre ellas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c4ebcb6"
      },
      "source": [
        "# Assuming necessary imports and the Python functions for ETL tasks are defined earlier in the DAG file.\n",
        "\n",
        "# Define default arguments for the DAG (Fase 1)\n",
        "# These arguments are passed to all tasks unless overridden\n",
        "default_args = {\n",
        "    'owner': 'airflow', # The owner of the DAG\n",
        "    'depends_on_past': False, # Set to true if you want tasks to wait for the previous run to succeed\n",
        "    'start_date': datetime(2024, 6, 1), # The date when the DAG starts being scheduled (Fase 1)\n",
        "    'email_on_failure': False, # Set to true to receive emails on task failure (Fase 1)\n",
        "    'email_on_retry': False, # Set to true to receive emails on task retry (Fase 1)\n",
        "    'retries': 1, # Number of times to retry a task on failure (Fase 1)\n",
        "    'retry_delay': timedelta(minutes=5), # Time to wait between retries (Fase 1)\n",
        "}\n",
        "\n",
        "# Define the DAG object (Fase 1)\n",
        "# 'presionfiscal_etl' is the unique DAG ID as specified\n",
        "with DAG(\n",
        "    'presionfiscal_etl', # dag_id as specified\n",
        "    default_args=default_args, # Apply the default arguments\n",
        "    description='ETL pipeline to download, transform, and load SRI tax pressure data to BigQuery', # A brief description of the DAG\n",
        "    schedule=None, # Set the schedule interval to None for manual runs (Fase 1)\n",
        "    catchup=False, # Set to False as specified (Fase 1)\n",
        "    max_active_runs=1, # Set max_active_runs as specified (Fase 1)\n",
        "    tags=['etl', 'bigquery', 'sri', 'presion_fiscal'], # Tags to help categorize and filter DAGs\n",
        ") as dag:\n",
        "\n",
        "    # Define the start task using EmptyOperator (Fase 4)\n",
        "    start = EmptyOperator(\n",
        "        task_id='start',\n",
        "    )\n",
        "\n",
        "    # Instantiate the PythonOperator for the download task\n",
        "    # Executes the 'download_csv_from_url' function defined earlier\n",
        "    download_csv_task = PythonOperator(\n",
        "        task_id='download_source_csv', # Unique identifier for the task\n",
        "        python_callable=download_csv_from_url, # The Python function to execute\n",
        "        op_kwargs={'csv_url': csv_url, 'local_filename': local_filename}, # Keyword arguments to pass to the function\n",
        "    )\n",
        "\n",
        "    # Instantiate the PythonOperator tasks for Dimension ETL\n",
        "    # These execute the corresponding functions defined earlier\n",
        "    process_dim_geografia_task = PythonOperator(\n",
        "        task_id='process_dim_geografia',\n",
        "        python_callable=process_dim_geografia,\n",
        "    )\n",
        "\n",
        "    process_dim_tiempo_task = PythonOperator(\n",
        "        task_id='process_dim_tiempo',\n",
        "        python_callable=process_dim_tiempo,\n",
        "    )\n",
        "\n",
        "    process_dim_tipopresion_task = PythonOperator(\n",
        "        task_id='process_dim_tipopresion',\n",
        "        python_callable=process_dim_tipopresion,\n",
        "    )\n",
        "\n",
        "    # Instantiate the PythonOperator task for Fact ETL\n",
        "    # Executes the corresponding function defined earlier\n",
        "    process_fact_presionfiscal_task = PythonOperator(\n",
        "        task_id='process_fact_presionfiscal',\n",
        "        python_callable=process_fact_presionfiscal,\n",
        "    )\n",
        "\n",
        "    # Define the end task using EmptyOperator (Fase 4)\n",
        "    end = EmptyOperator(\n",
        "        task_id='end',\n",
        "    )\n",
        "\n",
        "    # --- Define Dependencies (Fase 4) ---\n",
        "    # Flow: start >> download >> [dimensions parallel] >> fact >> end\n",
        "\n",
        "    start >> download_csv_task\n",
        "\n",
        "    # Dimension ETL tasks run in parallel after the download task completes\n",
        "    download_csv_task >> [process_dim_geografia_task, process_dim_tiempo_task, process_dim_tipopresion_task]\n",
        "\n",
        "    # The Fact ETL task runs only after ALL dimension ETL tasks are complete\n",
        "    [process_dim_geografia_task, process_dim_tiempo_task, process_dim_tipopresion_task] >> process_fact_presionfiscal_task\n",
        "\n",
        "    # The end task runs after the Fact ETL task completes\n",
        "    process_fact_presionfiscal_task >> end"
      ],
      "execution_count": 3,
      "outputs": []
    }
  ]
}