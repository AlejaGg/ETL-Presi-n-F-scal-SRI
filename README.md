# PresionFiscalETL

Pipeline ETL dise√±ado para automatizar la extracci√≥n, transformaci√≥n y carga de datos de presi√≥n fiscal en pa√≠ses de Am√©rica Latina y Europa. El sistema est√° implementado en Google Cloud Platform (GCP) utilizando **Apache Airflow (Cloud Composer)**, **BigQuery**, y **Cloud Storage**, lo que permite una integraci√≥n escalable, reproducible y trazable de datos abiertos publicados por el Servicio de Rentas Internas del Ecuador (SRI).

---

## Objetivo General

Automatizar la ingesta y transformaci√≥n de datos sobre presi√≥n fiscal en un esquema de Data Warehouse basado en modelo dimensional (estrella), permitiendo consultas anal√≠ticas eficientes y la generaci√≥n de reportes OLAP sobre la carga tributaria de los pa√≠ses registrados.

---
## Herramientas y Tecnolog√≠as Utilizadas

Este proyecto fue desarrollado utilizando un conjunto moderno de herramientas para la automatizaci√≥n del proceso ETL en la nube:

- **Google Cloud Platform (GCP)**: Plataforma principal de infraestructura en la nube.
- **Google BigQuery**: Motor de an√°lisis de datos a gran escala, usado como Data Warehouse.
- **Google Cloud Storage (GCS)**: Almacenamiento de objetos, usado para guardar el archivo CSV fuente.
- **Apache Airflow (Cloud Composer)**: Orquestador de flujos de trabajo ETL, gestionando tareas dependientes.
- **Google Colab**: Entorno de desarrollo interactivo utilizado para pruebas y desarrollo inicial de scripts.
- **Python 3**: Lenguaje principal para el desarrollo del pipeline.
- **Pandas**: Librer√≠a de an√°lisis y transformaci√≥n de datos.
- **PyArrow**: Backend utilizado por pandas para exportar datos a BigQuery.
- **google-cloud-bigquery**: Cliente oficial de Python para trabajar con BigQuery.
- **google-cloud-storage**: Cliente oficial de Python para interactuar con GCS.
- **google-auth**: Autenticaci√≥n mediante credenciales de cuenta de servicio para conexi√≥n segura con GCP.


## Arquitectura del Proyecto

```
+---------------------+       +---------------------------+       +---------------------------+
|   üì• Descarga CSV   | ----> |  ‚òÅÔ∏è Google Cloud Storage  | ----> | üõ†Ô∏è Apache Airflow (DAG ETL) |
|     (SRI Ecuador)   |       |     (Archivo Fuente)      |       |     (Transformaciones)     |
+---------------------+       +---------------------------+       +---------------------------+
                                                                          |
                                                                          v
                                                                +---------------------+
                                                                |   üìä BigQuery DW     |
                                                                |  (Modelo Estrella)   |
                                                                +---------------------+
                                                              
```

### Componentes Principales

| Componente        | Descripci√≥n                                                                 |
|-------------------|-----------------------------------------------------------------------------|
| Google Cloud Storage | Almacena el archivo CSV descargado del SRI.                              |
| Apache Airflow (Cloud Composer) | Orquesta y automatiza el pipeline ETL con tareas programadas. |
| BigQuery          | Almacena las tablas del modelo dimensional para an√°lisis OLAP.              |
| Python + Pandas   | Realiza transformaciones y validaciones sobre los datos.                    |

---

## Modelo Dimensional - Esquema Estrella

| Tabla                 | Descripci√≥n                                   |
|-----------------------|-----------------------------------------------|
| `Fact_PresionFiscal`  | M√©tricas de presi√≥n fiscal.                   |
| `Dim_Geografia`       | Dimensi√≥n geogr√°fica (Pa√≠s, Regi√≥n).          |
| `Dim_Tiempo`          | Dimensi√≥n temporal (A√±o).                     |
| `Dim_TipoPresion`     | Tipo de presi√≥n fiscal reportada.             |

---

## üõ†Ô∏è Requisitos del Entorno

### 1. Infraestructura en GCP

- Proyecto en Google Cloud creado y habilitado.
- Bucket en Cloud Storage creado (ej: `us-central1-etl-composer-XXXX-bucket`).
- Dataset en BigQuery: `sri_presion_fiscal`.

### 2. IAM (Roles y Permisos)

Debe crearse una **Cuenta de Servicio** con los siguientes roles:

- `roles/bigquery.dataViewer`: Visualizador de datos en BigQuery.
- `roles/bigquery.jobUser`: Permite ejecutar jobs de carga y consultas.
- `roles/storage.admin`: Acceso total al bucket donde se almacenan archivos.
- (Opcional) `roles/composer.worker`: Si deseas integrarlo completamente con Cloud Composer.

> üí° Guarda la clave de esta cuenta de servicio en formato `.json`.

### 3. Librer√≠as Python requeridas

```
pip install pandas pyarrow requests
pip install google-cloud-bigquery google-cloud-storage google-auth google-auth-oauthlib
```

### 4. Archivo de Credenciales

Sube el archivo de clave JSON al siguiente path en tu entorno Airflow:

```
/home/airflow/gcs/data/credentials/etl-sistemas-sri-xxxx.json
```

---

## Estructura del DAG en Airflow

```
presionfiscal_etl
‚îú‚îÄ‚îÄ start
‚îú‚îÄ‚îÄ download_source_csv
‚îú‚îÄ‚îÄ process_dim_geografia
‚îú‚îÄ‚îÄ process_dim_tiempo
‚îú‚îÄ‚îÄ process_dim_tipopresion
‚îú‚îÄ‚îÄ process_fact_presionfiscal
‚îî‚îÄ‚îÄ end
```

### Funciones de cada tarea:

| Tarea                          | Funci√≥n principal                                                           |
|--------------------------------|------------------------------------------------------------------------------|
| `download_source_csv`         | Descarga el archivo CSV desde el portal del SRI.                            |
| `process_dim_geografia`       | Extrae datos √∫nicos de pa√≠s y regi√≥n, genera claves y carga a BigQuery.     |
| `process_dim_tiempo`          | Extrae a√±os √∫nicos y crea la dimensi√≥n temporal.                            |
| `process_dim_tipopresion`     | Genera dimensi√≥n para los tipos de presi√≥n fiscal.                          |
| `process_fact_presionfiscal`  | Realiza joins con dimensiones, transforma m√©tricas y carga los hechos.      |

---

## Ruta de Subida del DAG

Aseg√∫rate de subir tu archivo `.py` del DAG a la siguiente ruta de Cloud Storage:

```
gsutil cp presionfiscal_etl_dag.py gs://us-central1-etl-composer-XXXX-bucket/dags/
```

Y las credenciales a:

```
gsutil cp etl-sistemas-sri-xxxx.json gs://us-central1-etl-composer-XXXX-bucket/data/credentials/
```

---

## üîç Resultados Esperados

- Tablas `Dim_*` y `Fact_*` correctamente pobladas en BigQuery.
- Datos limpios, validados y sin valores nulos.
- Historial de ejecuciones exitosas del DAG visible en Airflow.
- Posibilidad de realizar consultas OLAP para an√°lisis fiscal.

---

## Evidencias de funcionamiento

- Captura del DAG ejecut√°ndose con √©xito en Airflow.
  
  ![Captura del DAG ejecut√°ndose con √©xito en Airflow](./E1.png)
  
- Conteos de registros por tabla en BigQuery.
  
   ![Captura del DAG ejecut√°ndose con √©xito en Airflow](./E2.png)
  
- Ejemplo de consulta SQL para validaci√≥n de relaciones entre dimensiones y hechos.
  
  ![Captura del DAG ejecut√°ndose con √©xito en Airflow](./E3.png)

- Ejemplo de gr√°fico generado (si aplica) a partir de los datos del DW.
  
  ![Captura del DAG ejecut√°ndose con √©xito en Airflow](./E4.png)
  

---

## üìö Recursos Adicionales

- [Documentaci√≥n oficial de Airflow](https://airflow.apache.org/docs/)
- [BigQuery Python Client](https://cloud.google.com/bigquery/docs/reference/libraries#client-libraries-install-python)
- [Servicio de Rentas Internas (SRI)](https://www.sri.gob.ec)

---

## üë©‚Äçüíª Autora

**Miryam Alexandra Guerrero Gaibor**  
Estudiante de Ingenier√≠a de Software - Proyecto Acad√©mico  
---

---

